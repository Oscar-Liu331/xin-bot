import json
import re
from pathlib import Path
from typing import List, Dict, Any

import requests
from math import radians, sin, cos, asin, sqrt

from fastapi import FastAPI
from pydantic import BaseModel
from fastapi.middleware.cors import CORSMiddleware

from fastapi.staticfiles import StaticFiles
from fastapi.responses import FileResponse

from typing import Optional


CITY_PATTERN = (
    r"(å°åŒ—å¸‚|è‡ºåŒ—å¸‚|æ–°åŒ—å¸‚|æ¡ƒåœ’å¸‚|è‡ºä¸­å¸‚|å°ä¸­å¸‚|è‡ºå—å¸‚|å°å—å¸‚|é«˜é›„å¸‚|"
    r"åŸºéš†å¸‚|æ–°ç«¹å¸‚|å˜‰ç¾©å¸‚|æ–°ç«¹ç¸£|è‹—æ —ç¸£|å½°åŒ–ç¸£|å—æŠ•ç¸£|é›²æ—ç¸£|å˜‰ç¾©ç¸£|"
    r"å±æ±ç¸£|å®œè˜­ç¸£|èŠ±è“®ç¸£|è‡ºæ±ç¸£|å°æ±ç¸£|æ¾æ¹–ç¸£|é‡‘é–€ç¸£|é€£æ±Ÿç¸£)"
)

ADDR_HEAD_RE = re.compile(rf"^{CITY_PATTERN}(.*?(å€|é„‰|é®|å¸‚))")

SECTION = "pro"
DATASET_PATTERN = f"elearn_{SECTION}_*_*_dataset.json"
TOP_K = 5  

XIN_POINTS_FILE = Path("xin_points.json")
UNITS_FILE = Path("wellbeing_elearn_pro_all_with_articles.json")

MENTAL_KEYWORDS = [
    "æ†‚é¬±", "æƒ…ç·’ä½è½", "å¿ƒæƒ…ä¸å¥½", "å¿ƒæƒ…ä½è½",
    "å¿ƒæƒ…",      
    "ä½è½",      
    "é›£é",     
    "æ²®å–ª", "æ²’å‹•åŠ›",
    "ç„¦æ…®", "ç·Šå¼µ", "ææ…Œ", "å£“åŠ›", "è·å ´å£“åŠ›", "ç¡ä¸è‘—", "å¤±çœ ",
    "å­¤å–®", "å¯‚å¯",
    "å©†åª³", "å©†å©†", "å…¬å©†", "å®¶åº­è¡çª", "å®¶åº­é—œä¿‚",
    "å¤«å¦»", "å©šå§»", "è¦ªå­è¡çª", "è¦ªå­é—œä¿‚",

    "å°å­©", "å­©å­", "å¹¼å…’", "é’å°‘å¹´",
    "æ•™é¤Š", "è¦ªå­", "è¦ªå­è¡çª", "è¦ªå­é—œä¿‚",
    "åµæ¶", "é ‚å˜´", "å“­é¬§", "æƒ…ç·’å¤±æ§", "è„¾æ°£",
    
    "ç¹ªæœ¬","æ•…äº‹",
    "é•·è¼©éä¸–",
]

STOP_WORDS = [
    "æˆ‘", "ä½ ", "ä»–", "å¥¹", "å®ƒ", "æˆ‘å€‘", "ä½ å€‘", "ä»–å€‘",
    "æœ€è¿‘", "ä¸€ç›´", "è¦ºå¾—", "æœ‰é»", "æœ‰ä¸€é»",
    "å¦‚æœ", "å¥½åƒ", "æ˜¯ä¸æ˜¯",
    "è©²æ€éº¼è¾¦", "æ€éº¼è¾¦", "æ€éº¼åš", "è©²æ€éº¼åš",
    "å¯ä»¥", "è¦ºå¾—", "è‡ªå·±",
    "çš„", "äº†", "å‘¢", "å—", "å§"
]

def detect_pagination_intent(q: str) -> bool:
    return any(w in q for w in ["çµ¦æˆ‘å¾Œäº”å€‹","çµ¦æˆ‘ä¸‹äº”å€‹","å¾Œäº”å€‹","ä¸‹äº”å€‹","ä¸‹ä¸€é ","æ›´å¤šæ¨è–¦"])

def detect_special_intent(q: str) -> Optional[str]:
    """
    åµæ¸¬æ˜¯ä¸æ˜¯å±¬æ–¼ã€Œè¦ç›´æ¥çµ¦å»ºè­°ã€çš„å¹¾ç¨®å¸¸è¦‹å•é¡Œï¼š
      - æ†‚é¬±è¦ä¸è¦çœ‹é†«å¸«
      - æ“”å¿ƒå®¶äººå¤±æ™ºï¼Œçœ‹å“ªä¸€ç§‘
      - å°å­¸ç”Ÿæ‰‹æ©Ÿç©å¤ªå¤š
      - å©†å©†å¸¶å°å­©æ•™é¤Šè¡çª
    å›å‚³ intent åç¨±ï¼Œæ²’å‘½ä¸­å°±å›å‚³ Noneã€‚
    """
    # å»æ‰ç©ºç™½ç¬¦è™Ÿ
    text = re.sub(r"\s+", "", q)
    print(f"[intent-debug] raw='{q}' text='{text}'")

    # ---------- 1) æˆ‘è¦ºå¾—æœ‰é»æ†‚é¬±ï¼Œè¦ä¸è¦çœ‹é†«å¸«ï¼Ÿ ----------
    has_depression_word = any(w in text for w in ["æ†‚é¬±", "å¿ƒæƒ…ä½è½", "å¿ƒæƒ…ä¸å¥½", "æƒ…ç·’ä½è½"])
    # åªè¦æœ‰ã€Œçœ‹ã€+ï¼ˆé†«å¸«/é†«ç”Ÿ/å¿ƒç†/èº«å¿ƒç§‘/ç²¾ç¥ç§‘ï¼‰å°±è¦–ç‚ºå•å°±é†«
    has_see_doctor = (
        "çœ‹" in text and (
            "é†«å¸«" in text or "é†«ç”Ÿ" in text or "å¿ƒç†å¸«" in text or
            "å¿ƒç†é†«å¸«" in text or "èº«å¿ƒç§‘" in text or "ç²¾ç¥ç§‘" in text
        )
    )
    # é¡å¤–åŠ ä¸€äº›å¸¸è¦‹èªæ°£
    has_doubt_phrase = any(w in text for w in ["è¦ä¸è¦", "è©²ä¸è©²", "éœ€ä¸éœ€è¦", "è¦ä¸è¦å»"])

    if has_depression_word and (has_see_doctor or has_doubt_phrase):
        return "depression_go_doctor"

    # ---------- 2) æ“”å¿ƒçˆ¸çˆ¸ / é•·è¼©å¤±æ™ºï¼Œçœ‹å“ªä¸€ç§‘ï¼Ÿ ----------
    if "å¤±æ™º" in text and any(w in text for w in ["çˆ¸çˆ¸", "åª½åª½", "çˆ¶è¦ª", "æ¯è¦ª", "é•·è¼©", "å®¶äºº", "é˜¿å…¬", "é˜¿å¬¤"]):
        return "dementia_parent"

    # ---------- 3) åœ‹å°å°å­©æ‰‹æ©Ÿç©å¤ªå…‡ï¼Œæ€éº¼è¾¦ï¼Ÿ ----------
    has_device = any(w in text for w in ["æ‰‹æ©Ÿ", "å¹³æ¿", "éŠæˆ²", "é›»å‹•", "ç·šä¸ŠéŠæˆ²"])
    has_child = any(w in text for w in ["åœ‹å°", "å°å­¸", "å°å­©", "å­©å­", "å…’å­", "å¥³å…’", "åœ‹ä¸‰", "åœ‹äºŒ", "åœ‹ä¸€"])
    if has_device and has_child:
        return "child_phone"

    # ---------- 4) å©†å©†ç…§é¡§å°å­©æ–¹å¼å’Œæˆ‘å¾ˆä¸ä¸€æ¨£ ----------
    has_inlaw = any(w in text for w in ["å©†å©†", "å…¬å©†", "å©†å®¶"])
    has_childcare = any(w in text for w in ["å°å­©", "å­©å­", "é¡§å°å­©", "å¸¶å°å­©", "ç…§é¡§", "æ•™é¤Š"])
    if has_inlaw and has_childcare:
        return "mother_in_law_childcare"

    return None


def build_special_intent_response(intent: str, q: str) -> Dict[str, Any]:
    """
    æŠŠå››ç¨®æƒ…å¢ƒçš„å»ºè­°ï¼ŒåŒ…æˆçµæ§‹åŒ– JSONï¼Œçµ¦å‰ç«¯æ¸²æŸ“ã€‚
    """
    if intent == "depression_go_doctor":
        title = "è¦ºå¾—è‡ªå·±æœ‰é»æ†‚é¬±ï¼Œè©²ä¸è©²å»çœ‹å¿ƒç†é†«å¸«ï¼Ÿ"
        sections = [
            {
                "title": "1ï¸âƒ£ ä»€éº¼æƒ…æ³æ¯”è¼ƒå»ºè­°æ‰¾å°ˆæ¥­é†«å¸«æˆ–å¿ƒç†å¸«ï¼Ÿ",
                "items": [
                    "æƒ…ç·’ä½è½ã€æ²’å‹•åŠ›ã€å®¹æ˜“æƒ³å“­ï¼ŒæŒçºŒè¶…é 2 é€±ä»¥ä¸Šã€‚",
                    "å½±éŸ¿åˆ°å·¥ä½œã€èª²æ¥­ã€ç¡çœ ã€é£Ÿæ…¾æˆ–äººéš›é—œä¿‚ã€‚",
                    "å‡ºç¾ã€Œä¸å¦‚æ¶ˆå¤±ç®—äº†ã€ã€ã€Œæ´»è‘—å¥½ç´¯ã€é€™é¡è² é¢æˆ–è‡ªå‚·çš„æƒ³æ³•ã€‚"
                ]
            },
            {
                "title": "2ï¸âƒ£ å¯ä»¥çœ‹çš„ç§‘åˆ¥ / å°ˆæ¥­",
                "items": [
                    "é†«é™¢çš„ã€Œèº«å¿ƒç§‘ / ç²¾ç¥ç§‘ã€é–€è¨ºï¼Œå¯ä»¥è©•ä¼°æ˜¯å¦éœ€è¦ç”¨è—¥æˆ–é€²ä¸€æ­¥æª¢æŸ¥ã€‚",
                    "é†«é™¢æˆ–ç¤¾å€çš„ã€Œè‡¨åºŠå¿ƒç†å¸«ã€è«®å•†å¿ƒç†å¸«ã€åšå¿ƒç†è«®å•†ã€‚",
                    "å¦‚æœé‚„ä¸ç¢ºå®šï¼Œä¹Ÿå¯ä»¥å…ˆå¾ã€Œå®¶é†«ç§‘ã€æˆ–ç¤¾å€å¿ƒç†è¡›ç”Ÿä¸­å¿ƒè«®è©¢é–‹å§‹ã€‚"
                ]
            },
            {
                "title": "3ï¸âƒ£ å¦‚æœç¾åœ¨é‚„å‹‰å¼·æ’å¾—ä½ï¼Œå¯ä»¥å…ˆå˜—è©¦çš„èª¿æ•´",
                "items": [
                    "å›ºå®šç¡è¦ºã€èµ·åºŠæ™‚é–“ï¼Œç›¡é‡å°‘ç†¬å¤œã€‚",
                    "æ‰¾ä¸€å€‹ä¿¡ä»»çš„äººèŠèŠï¼ŒæŠŠå£“åŠ›èªªå‡ºä¾†ã€‚",
                    "å…ˆå¾çŸ­æ™‚é–“æ•£æ­¥æˆ–ç°¡å–®é‹å‹•é–‹å§‹ï¼Œè®“èº«é«”å‹•èµ·ä¾†ã€‚"
                ]
            },
            {
                "title": "âš ï¸ ä»€éº¼æƒ…æ³è¦ç«‹åˆ»å°‹æ±‚å”åŠ©ï¼Ÿ",
                "items": [
                    "æœ‰æ˜é¡¯çš„è‡ªæ®ºå¿µé ­ã€è¡å‹•ï¼Œæˆ–å·²ç¶“æƒ³å¥½æ–¹æ³•ã€‚",
                    "æ­¤æ™‚è«‹å„˜å¿«åˆ°é†«é™¢æ€¥è¨ºï¼Œæˆ–è«‹å®¶äººæœ‹å‹é™ªåŒå°±é†«ï¼Œä¸¦å¯è¯çµ¡è‡ªæ®ºé˜²æ²» / å¿ƒç†æ”¯æŒå°ˆç·šã€‚"
                ]
            }
        ]

    elif intent == "dementia_parent":
        title = "æ“”å¿ƒçˆ¸çˆ¸ / å®¶äººå¯èƒ½æœ‰å¤±æ™ºï¼Œæ€éº¼ç¢ºå®šï¼Ÿçœ‹å“ªä¸€ç§‘ï¼Ÿ"
        sections = [
            {
                "title": "1ï¸âƒ£ å¸¸è¦‹çš„å¤±æ™ºè­¦è¨Šï¼ˆèˆ‰ä¾‹ï¼‰",
                "items": [
                    "è¨˜æ†¶åŠ›æ˜é¡¯è®Šå·®ï¼šåŒä¸€ä»¶äº‹å•å¾ˆå¤šæ¬¡ï¼Œå¿˜è¨˜å‰›ç™¼ç”Ÿçš„äº‹æƒ…ã€‚",
                    "å®¹æ˜“è¿·è·¯ï¼šåœ¨ç†Ÿæ‚‰çš„ç’°å¢ƒåè€Œæœƒèµ°éŒ¯ã€æ‰¾ä¸åˆ°è·¯ã€‚",
                    "åˆ¤æ–·åŠ›è®Šå·®ï¼šä¾‹å¦‚å®¹æ˜“è¢«è©é¨™ã€åšä¸€äº›ä»¥å‰ä¸æœƒåšçš„æ€ªæ±ºå®šã€‚",
                    "æ€§æ ¼æˆ–è¡Œç‚ºæ”¹è®Šï¼šè®Šå¾—æ˜é¡¯æš´èºã€é€€ç¸®ï¼Œæˆ–è·Ÿä»¥å‰å€‹æ€§å·®å¾ˆå¤šã€‚"
                ]
            },
            {
                "title": "2ï¸âƒ£ è¦æ€éº¼æ¯”è¼ƒç¢ºå®šæ˜¯ä¸æ˜¯å¤±æ™ºï¼Ÿ",
                "items": [
                    "éœ€è¦ç”±é†«å¸«åšå®Œæ•´è©•ä¼°ï¼Œå¯èƒ½åŒ…æ‹¬å•è¨ºã€ç¥ç¶“å¿ƒç†é‡è¡¨ã€è¡€æ¶²æª¢æŸ¥ã€å½±åƒæª¢æŸ¥ç­‰ã€‚",
                    "å®¶å±¬å¯ä»¥å…ˆæ•´ç†æœ€è¿‘è§€å¯Ÿåˆ°çš„æ”¹è®Šï¼ˆå¾ä»€éº¼æ™‚å€™é–‹å§‹ã€ç™¼ç”Ÿåœ¨ä»€éº¼æƒ…å¢ƒï¼‰ã€‚"
                ]
            },
            {
                "title": "3ï¸âƒ£ å»ºè­°å…ˆçœ‹å“ªä¸€ç§‘ï¼Ÿ",
                "items": [
                    "å¤§å‹é†«é™¢å¸¸è¦‹ç§‘åˆ¥ï¼šç¥ç¶“å…§ç§‘ã€å®¶é†«ç§‘ã€è€å¹´é†«å­¸ç§‘ï¼Œéƒ¨åˆ†é†«é™¢æœ‰ã€Œå¤±æ™ºå…±åŒç…§è­·é–€è¨ºã€ã€‚",
                    "è‹¥é•·è¼©æœ‰æ˜é¡¯æƒ…ç·’æˆ–è¡Œç‚ºæ”¹è®Šï¼ˆå¦‚å¦„æƒ³ã€å¹»è¦ºã€åš´é‡ç„¦æ…®ï¼‰ï¼Œèº«å¿ƒç§‘ / ç²¾ç¥ç§‘ä¹Ÿèƒ½å”åŠ©è©•ä¼°ã€‚"
                ]
            },
            {
                "title": "4ï¸âƒ£ å¸¶é•·è¼©çœ‹è¨ºçš„å°æŠ€å·§",
                "items": [
                    "å¯ä»¥ç”¨ã€Œåšå¥åº·æª¢æŸ¥ã€çš„æ–¹å¼é‚€è«‹ï¼Œè€Œä¸æ˜¯ç›´æ¥èªªã€Œæ‡·ç–‘ä½ å¤±æ™ºã€ã€‚",
                    "çœ‹è¨ºæ™‚ï¼Œå®¶å±¬å¯æŠŠåœ¨å®¶è§€å¯Ÿåˆ°çš„ç‹€æ³æ•´ç†åœ¨ç´™ä¸Šçµ¦é†«å¸«çœ‹ï¼Œæ¯”è¼ƒä¸æœƒæ¼è¬›ã€‚"
                ]
            }
        ]

    elif intent == "child_phone":
        title = "åœ‹å°å­©å­æ‰‹æ©Ÿè¶Šç©è¶Šå…‡ï¼Œæˆ‘è©²æ€éº¼è¾¦ï¼Ÿ"
        sections = [
            {
                "title": "1ï¸âƒ£ å…ˆäº†è§£ã€Œæ€éº¼ç©ã€è€Œä¸åªæ˜¯ã€Œç©å¤šä¹…ã€",
                "items": [
                    "ä¸»è¦æ˜¯åœ¨ç©éŠæˆ²ã€çœ‹å½±ç‰‡ï¼Œé‚„æ˜¯è·ŸåŒå­¸èŠå¤©ï¼Ÿ",
                    "é€šå¸¸åœ¨ä»€éº¼æ™‚é–“é»ç©ï¼šå¯«åŠŸèª²å‰ï¼Ÿç¡å‰ï¼Ÿå‡æ—¥æ•´å¤©ï¼Ÿ"
                ]
            },
            {
                "title": "2ï¸âƒ£ è·Ÿå­©å­ä¸€èµ·ã€Œè«‡è¦å‰‡ã€ï¼Œä¸æ˜¯åªä¸‹å‘½ä»¤",
                "items": [
                    "ä¾‹å¦‚ï¼šå¹³æ—¥æ¯å¤©å¯ä»¥ç© 30â€“60 åˆ†é˜ï¼Œå…ˆå¯«å®Œä½œæ¥­ã€æ´—æ¾¡å†é–‹æ©Ÿã€‚",
                    "é¿å…ç¡å‰ 1 å°æ™‚ç”¨æ‰‹æ©Ÿï¼Œè®“å¤§è…¦æœ‰æ™‚é–“ã€Œé™é€Ÿã€æ¯”è¼ƒå¥½ç¡ã€‚",
                    "æŠŠè¦å‰‡å¯«åœ¨ç´™ä¸Šè²¼å‡ºä¾†ï¼Œæ¸›å°‘è‡¨æ™‚åµæ¶ã€‚"
                ]
            },
            {
                "title": "3ï¸âƒ£ æä¾›æ›¿ä»£æ´»å‹•ï¼Œä¸æ˜¯åªæœ‰ã€Œä¸å‡†ç©ã€",
                "items": [
                    "å®‰æ’å¯ä»¥ä¸€èµ·åšçš„äº‹ï¼šæ¡ŒéŠã€é‹å‹•ã€æ•£æ­¥ã€ç•«ç•«ã€åšæ–™ç†ã€‚",
                    "å‡æ—¥ç´„å®šã€Œç„¡æ‰‹æ©Ÿæ™‚æ®µã€ï¼Œå…¨å®¶ä¸€èµ·åšåˆ¥çš„æ´»å‹•ã€‚"
                ]
            },
            {
                "title": "4ï¸âƒ£ å¤§äººä¹Ÿè¦ç•¶æ¦œæ¨£",
                "items": [
                    "å¦‚æœçˆ¶æ¯ä¸€ç›´æ»‘æ‰‹æ©Ÿï¼Œå°å­©å¾ˆé›£æ¥å—ã€Œä½ ä¸å¯ä»¥æ»‘ã€ã€‚",
                    "å¯ä»¥ä¸€èµ·ç´„å®šï¼šåƒé£¯ã€ç¡å‰åŠå°æ™‚ï¼Œå…¨å®¶éƒ½ä¸çœ‹æ‰‹æ©Ÿã€‚"
                ]
            },
            {
                "title": "5ï¸âƒ£ ä»€éº¼æ™‚å€™éœ€è¦å°ˆæ¥­å”åŠ©ï¼Ÿ",
                "items": [
                    "å·²ç¶“å½±éŸ¿åˆ°å­¸æ¥­ã€ç¡çœ ï¼Œæˆ–ç‚ºäº†æ‰‹æ©Ÿæœƒå¤§åµå¤§é¬§ã€æ‘”æ±è¥¿ã€‚",
                    "å¯ä»¥è€ƒæ…®è«®è©¢ï¼šå­¸æ ¡è¼”å°è€å¸«ã€å…’ç«¥é’å°‘å¹´èº«å¿ƒç§‘ã€å…’ç«¥è‡¨åºŠ / è«®å•†å¿ƒç†å¸«ã€‚"
                ]
            }
        ]

    elif intent == "mother_in_law_childcare":
        title = "å©†å©†ç…§é¡§å°å­©æ–¹å¼è·Ÿæˆ‘å¾ˆä¸ä¸€æ¨£ï¼Œæˆ‘è©²æ€éº¼è¾¦ï¼Ÿ"
        sections = [
            {
                "title": "1ï¸âƒ£ å…ˆåˆ†è¾¨ï¼šæ˜¯ã€åšæ³•ä¸åŒã€é‚„æ˜¯ã€å®‰å…¨æœ‰ç–‘æ…®ã€",
                "items": [
                    "åšæ³•ä¸åŒä½†å®‰å…¨ï¼šä¾‹å¦‚é›¶é£Ÿå¤šä¸€é»ã€çœ‹é›»è¦–ä¹…ä¸€é»ï¼Œå¯ä»¥å…ˆç•¶æˆã€Œé¢¨æ ¼å·®ç•°ã€ã€‚",
                    "æ¶‰åŠå®‰å…¨ï¼šä¾‹å¦‚è®“å°å­©å–®ç¨åœ¨é™½å°ã€åƒå®¹æ˜“å™åˆ°çš„é£Ÿç‰©ï¼Œå°±éœ€è¦æ¯”è¼ƒå …å®šåœ°æºé€šã€‚"
                ]
            },
            {
                "title": "2ï¸âƒ£ æºé€šæ™‚ï¼Œå…ˆæ„Ÿè¬å†è¡¨é”æ“”å¿ƒï¼ˆç”¨ã€Œæˆ‘ã€è¨Šæ¯ï¼‰",
                "items": [
                    "å¦‚ï¼šã€Œæˆ‘çœŸçš„å¾ˆæ„Ÿè¬å¦³å¹«å¿™é¡§å°å­©ï¼Œæˆ‘æœƒæ¯”è¼ƒæ”¾å¿ƒã€‚ã€",
                    "å†æ¥ï¼šã€Œåªæ˜¯æˆ‘æœ‰é»æ“”å¿ƒï¼Œä»–åƒå¤ªå¤šç³–å°ç‰™é½’ä¸å¥½ï¼Œæˆ‘æƒ³æˆ‘å€‘å¯ä¸å¯ä»¥ä¸€èµ·å¹«ä»–å°‘ä¸€é»ï¼Ÿã€",
                    "é¿å…ç”¨ã€Œå¦³é€™æ¨£ä¸å°ã€ã€Œå¦³æŠŠå°å­©å¸¶å£äº†ã€ï¼Œæ¯”è¼ƒä¸æœƒç«‹åˆ»è®Šæˆåµæ¶ã€‚"
                ]
            },
            {
                "title": "3ï¸âƒ£ å„˜é‡è®“å¦ä¸€åŠç•¶æ©‹æ¨‘",
                "items": [
                    "è‡ªå·±çš„çˆ¸åª½é€šå¸¸æ¯”è¼ƒè½è‡ªå·±å°å­©çš„è©±ã€‚",
                    "å¯ä»¥å…ˆè·Ÿå¦ä¸€åŠç§ä¸‹æºé€šå¥½ç«‹å ´èˆ‡åº•ç·šï¼Œå†ç”±ä»– / å¥¹è·Ÿå©†å©†èªªã€‚"
                ]
            },
            {
                "title": "4ï¸âƒ£ å»ºç«‹å¹¾æ¢ã€Œå…¨å®¶å…±åŒçš„åŸå‰‡ã€",
                "items": [
                    "ä¾‹å¦‚ï¼šç”¨è—¥ä¸€å®šè¦å•çˆ¸åª½ã€ä¸èƒ½æ‰“å°å­©ã€å¤§ç´„å¹¾é»ç¡è¦ºã€‚",
                    "åŸå‰‡è¶Šæ¸…æ¥šï¼Œè¶Šä¸æœƒæ¯ä»¶å°äº‹éƒ½åµæˆä¸€åœ˜ã€‚"
                ]
            },
            {
                "title": "5ï¸âƒ£ å¦‚æœè¡çªå½±éŸ¿åˆ°ä½ è‡ªå·±çš„æƒ…ç·’",
                "items": [
                    "å¯ä»¥è€ƒæ…®å’Œè«®å•†å¿ƒç†å¸«è«‡è«‡ï¼Œæ•´ç†è‡ªå·±çš„å§”å±ˆèˆ‡è§’è‰²å£“åŠ›ï¼ˆåª³å©¦ / åª½åª½é›™é‡èº«åˆ†ï¼‰ã€‚",
                    "æœ‰æ™‚å€™å•é¡Œä¸åªæ˜¯æ•™é¤ŠæŠ€å·§ï¼Œä¹Ÿæ˜¯ä½ å’Œå…ˆç”Ÿã€ä½ å’Œå©†å©†ä¹‹é–“çš„ç•Œç·šã€‚"
                ]
            }
        ]

    else:
        title = "ä¸€èˆ¬å»ºè­°"
        sections = [
            {
                "title": "",
                "items": ["é€™å€‹å•é¡Œç›®å‰é‚„æ²’æœ‰å°ˆé–€å¯«å¥½çš„å»ºè­°å›ç­”ï¼Œå…ˆç”¨èª²ç¨‹èˆ‡æ–‡ç« æ¨è–¦æ¨¡å¼å¹«ä½ æ‰¾è³‡æ–™ã€‚"]
            }
        ]

    return {
        "type": "advice",
        "intent": intent,
        "query": q,
        "title": title,
        "sections": sections
    }


def build_nearby_points_response(address: str, results):
    """
    æŠŠ find_nearby_points çš„çµæœè½‰æˆ JSON-friendly çµæ§‹
    """
    if not results:
        return {
            "type": "xin_points",
            "address": address,
            "points": [],
            "message": f"åœ¨ã€Œ{address}ã€5 å…¬é‡Œå…§æ²’æœ‰æ‰¾åˆ°å¿ƒæ“šé»"
        }

    points = []
    for p, d in results:
        points.append({
            "title": p.get("title"),
            "address": p.get("address"),
            "tel": p.get("tel"),
            "distance_km": round(d, 2),
        })

    return {
        "type": "xin_points",
        "address": address,
        "points": points
    }


def build_recommendations_response(
    query: str,
    results: List[Dict[str, Any]],
    offset: int = 0,
    limit: int = TOP_K
):
    # æ²’æœ‰çµæœ
    if not results:
        return {
            "type": "course_recommendation",
            "query": query,
            "total": 0,
            "video_count": 0,
            "article_count": 0,
            "offset": offset,
            "limit": limit,
            "has_more": False,
            "results": [],
            "message": "ç›®å‰æ‰¾ä¸åˆ°å¾ˆç¬¦åˆçš„èª²ç¨‹ï¼Œå¯ä»¥è©¦è‘—ç”¨ï¼šå©†åª³ã€å£“åŠ›ã€æ†‚é¬±ã€å¤±çœ â€¦ ç­‰è©å†è©¦è©¦çœ‹ã€‚"
        }

    # âœ… å…ˆé‡æ’ï¼ˆåˆ†é å‰ï¼‰
    results = reorder_episode_pairs(results)

    total = len(results)
    # âœ… è¨ˆç®—ç¸½æ•¸ï¼ˆç”¨é‡æ’å¾Œçš„ results çµ±è¨ˆå³å¯ï¼‰
    video_count = sum(1 for r in results if not r.get("is_article"))
    article_count = sum(1 for r in results if r.get("is_article"))

    # âœ… æ­£ç¢ºåˆ‡åˆ†é 
    page_results = results[offset: offset + limit]

    items = []
    for r in page_results:  # âœ… ä¸€å®šè¦ç”¨ page_results
        title = r.get("title") or "(ç„¡æ¨™é¡Œ)"
        section_title = r.get("section_title") or "(æœªåˆ†é¡å°ç¯€)"
        score = r.get("_score", 0.0)

        is_article = bool(r.get("is_article"))
        youtube_url = r.get("youtube_url")

        entry: Dict[str, Any] = {
            "section_title": section_title,
            "title": title,
            "score": score,
            "is_article": is_article,  # âœ… çµ¦å‰ç«¯ç›´æ¥ç”¨
            "type": "article" if is_article else "video",
        }

        if is_article:
            article_url = r.get("article_url") or r.get("url")
            content_text = (r.get("content_text") or "").replace("\n", " ")
            snippet = content_text[:100] + ("..." if len(content_text) > 100 else "")
            entry["article_url"] = article_url
            entry["snippet"] = snippet
        else:
            seg = r.get("_best_segment")
            if seg:
                start_sec = seg.get("start_sec", 0.0)
                start_str = format_time(start_sec)
                seg_text = seg.get("text", "") or ""
                hint = f"è©²å–®å…ƒåœ¨ {start_str} æœ‰æåˆ°ï¼šã€Œ{seg_text[:30]}...ã€"
            else:
                hint = "å­—å¹•è£¡æ²’æœ‰ç‰¹åˆ¥å‘½ä¸­é—œéµå¥ï¼Œå¯ä»¥å¾é ­é–‹å§‹çœ‹ã€‚"
            entry["youtube_url"] = youtube_url
            entry["hint"] = hint

        items.append(entry)

    return {
        "type": "course_recommendation",
        "query": query,
        "total": total,
        "video_count": video_count,
        "article_count": article_count,
        "offset": offset,
        "limit": limit,
        "has_more": offset + limit < total,
        "results": items
    }



def load_xin_points() -> List[Dict[str, Any]]:
    try:
        data = json.loads(XIN_POINTS_FILE.read_text("utf-8"))
        return data.get("data", [])
    except Exception as e:
        print(f"[xin] âš ï¸ å¿ƒæ“šé»è¼‰å…¥å¤±æ•—ï¼š{e}")
        return []
    
def haversine_km(lon1, lat1, lon2, lat2) -> float:
    lon1, lat1, lon2, lat2 = map(radians, [lon1, lat1, lon2, lat2])
    dlon = lon2 - lon1
    dlat = lat2 - lat1
    a = sin(dlat/2)**2 + cos(lat1)*cos(lat2)*sin(dlon/2)**2
    c = 2 * asin(sqrt(a))
    return 6371 * c  # km

def geocode_address(address: str):
    if not address:
        return None

    def try_geocode(addr: str):
        url = "https://nominatim.openstreetmap.org/search"
        params = {"q": addr, "format": "json", "limit": 1}
        headers = {"User-Agent": "xin-bot/1.0"}
        try:
            r = requests.get(url, params=params, headers=headers, timeout=5)
            r.raise_for_status()
            data = r.json()
            if data:
                lat = float(data[0]["lat"])
                lon = float(data[0]["lon"])
                print(f"[geocode] å‘½ä¸­ï¼š'{addr}' -> lat={lat}, lon={lon}")
                return lat, lon
        except Exception as e:
            print(f"[geocode] éŒ¯èª¤ï¼š{e}")
        return None

    print(f"[geocode] å˜—è©¦ï¼š{address}")
    result = try_geocode(address)
    if result:
        return result

    if "è‡º" in address:
        addr2 = address.replace("è‡º", "å°")
        print(f"[geocode] å˜—è©¦ï¼š{addr2}")
        result = try_geocode(addr2)
        if result:
            return result

    addr3 = re.sub(r"\d+è™Ÿ.*", "", address)
    if addr3 != address:
        print(f"[geocode] å˜—è©¦ï¼ˆå»è™Ÿï¼‰ï¼š{addr3}")
        result = try_geocode(addr3)
        if result:
            return result

    addr4 = re.sub(r"\d+å¼„.*", "", address)
    if addr4 != address:
        print(f"[geocode] å˜—è©¦ï¼ˆå»å¼„ï¼‰ï¼š{addr4}")
        result = try_geocode(addr4)
        if result:
            return result

    addr5 = re.sub(r"\d+å··.*", "", address)
    if addr5 != address:
        print(f"[geocode] å˜—è©¦ï¼ˆå»å··ï¼‰ï¼š{addr5}")
        result = try_geocode(addr5)
        if result:
            return result

    m = re.match(
        r"(å°åŒ—å¸‚|è‡ºåŒ—å¸‚|æ–°åŒ—å¸‚|æ¡ƒåœ’å¸‚|è‡ºä¸­å¸‚|å°ä¸­å¸‚|è‡ºå—å¸‚|å°å—å¸‚|é«˜é›„å¸‚|"
        r"åŸºéš†å¸‚|æ–°ç«¹å¸‚|å˜‰ç¾©å¸‚|æ–°ç«¹ç¸£|è‹—æ —ç¸£|å½°åŒ–ç¸£|å—æŠ•ç¸£|é›²æ—ç¸£|å˜‰ç¾©ç¸£|"
        r"å±æ±ç¸£|å®œè˜­ç¸£|èŠ±è“®ç¸£|è‡ºæ±ç¸£|å°æ±ç¸£|æ¾æ¹–ç¸£|é‡‘é–€ç¸£|é€£æ±Ÿç¸£)"
        r"(.+?(å€|å¸‚|é®|é„‰))",
        address
    )
    if m:
        addr6 = m.group(1) + m.group(2)
        print(f"[geocode] å˜—è©¦ï¼ˆå¸‚+å€/é„‰/é®/å¸‚ï¼‰ï¼š{addr6}")
        result = try_geocode(addr6)
        if result:
            return result

    print(f"[geocode] å®Œå…¨æŸ¥ä¸åˆ°ï¼š{address}")
    return None
    
def find_nearby_points(lat, lon, max_km=5, top_k=5):
    points = load_xin_points()
    results = []

    for p in points:
        if p.get("lat") and p.get("lon"):
            d = haversine_km(lon, lat, p["lon"], p["lat"])
            if d <= max_km:
                results.append((p, d))

    results.sort(key=lambda x: x[1])
    return results[:top_k]

def load_all_units() -> List[Dict[str, Any]]:
    data = json.loads(UNITS_FILE.read_text("utf-8"))
    raw_units = data.get("units", [])

    units: List[Dict[str, Any]] = []

    for u in raw_units:
        u = dict(u)  

        section_title = u.get("section_title") or ""

        subtitle_texts = " ".join(
            seg.get("text", "") for seg in u.get("subtitles", []) or []
        )

        content_text = u.get("content_text", "") or ""

        search_text = " ".join(
            s for s in [
                section_title,
                u.get("title") or "",
                content_text,
                subtitle_texts,
            ] if s
        )

        u["_search_text"] = search_text

        units.append(u)

    print(f"[load] âœ… å…±è¼‰å…¥ {len(units)} å€‹å–®å…ƒï¼ˆå«å½±ç‰‡ + æ–‡ç« ï¼‰")
    return units

def extract_address_from_query(q: str) -> str:
    original = q

    if "é™„è¿‘" in q:
        q = q.split("é™„è¿‘")[0]

    for kw in ["å¿ƒæ“šé»", "é–€è¨º", "çœ‹è¨º"]:
        if kw in q:
            q = q.split(kw)[0]

    prefixes = ["æˆ‘ä½åœ¨", "æˆ‘ä½", "å®¶åœ¨", "å®¶ä½", "ä½åœ¨", "ä½", "åœ¨"]
    q = q.strip()
    for p in prefixes:
        if q.startswith(p):
            q = q[len(p):].strip()
            break

    tail_words = ["æœ‰æ²’æœ‰", "æœ‰å—", "å—", "å‘¢", "å•Š", "å•¦"]
    for t in tail_words:
        if q.endswith(t):
            q = q[: -len(t)].strip()

    q = q.strip(" ?ï¼Ÿ!")

    if len(q) < 4:
        return ""

    print(f"[debug] extract_address_from_query: '{original}' -> '{q}'")
    return q

def normalize_query(q: str) -> List[str]:
    q = q.strip()
    if not q:
        return []

    terms: List[str] = []

    for kw in MENTAL_KEYWORDS:
        if kw in q and kw not in terms:
            terms.append(kw)

    parts = re.split(r"[ï¼Œã€‚ï¼!ï¼Ÿ?\sã€ï¼›;:ï¼š]+", q)
    for part in parts:
        part = part.strip()
        if not part or part in STOP_WORDS:
            continue

        if re.fullmatch(r"[\u4e00-\u9fff]+", part):
            n = len(part)
            for size in range(2, min(4, n) + 1):
                for i in range(0, n - size + 1):
                    gram = part[i:i+size]
                    if gram not in STOP_WORDS and gram not in terms:
                        terms.append(gram)
        else:
            if len(part) >= 2 and part not in terms and part not in STOP_WORDS:
                terms.append(part)

    if not terms:
        terms = [q]

    return terms

def score_unit(unit, query_terms, core_terms):
    text = unit.get("_search_text", "") or ""
    if not text:
        return 0.0, None

    title = (unit.get("section_title") or "") + (unit.get("title") or "")
    subtitles = unit.get("subtitles", [])

    title_core = any(c in title for c in core_terms)

    subtitle_core = False
    for seg in subtitles:
        seg_text = seg.get("text", "") or ""
        if any(c in seg_text for c in core_terms):
            subtitle_core = True
            break

    if not (title_core or subtitle_core):
        return 0.0, None

    total_hits = sum(text.count(term) for term in query_terms)
    if total_hits < 3:
        return 0.0, None

    total_core_hits = sum(text.count(c) for c in core_terms)

    if not title_core and total_core_hits < 3:
        return 0.0, None

    best_seg = None
    best_seg_score = 0
    for seg in subtitles:
        seg_text = seg.get("text", "") or ""
        seg_score = sum(seg_text.count(t) for t in query_terms)
        if seg_score > best_seg_score:
            best_seg_score = seg_score
            best_seg = seg

    title_core_hits = sum(1 for c in core_terms if c in title)

    final_score = (
        total_core_hits * 4 +      # æ ¸å¿ƒè©åœ¨å…¨æ–‡å‡ºç¾è¶Šå¤šï¼Œè¶Šç›¸é—œ
        title_core_hits * 6 +      # æ¨™é¡Œæœ‰æ ¸å¿ƒè©ï¼ŒåŠ å¤§æ¬Šé‡
        total_hits * 1 +           # å…¶ä»–è©ä¹Ÿç•¥åŠ åˆ†
        best_seg_score * 2         # æœ‰ä¸€æ®µå­—å¹•ç‰¹åˆ¥é›†ä¸­ï¼Œä¹ŸåŠ åˆ†
    )

    return final_score, best_seg

# æŠŠã€Œ(ä¸Š)/(ä¸‹)/(ï¼ˆä¸Šï¼‰)/(ï¼ˆä¸‹ï¼‰)/ä¸Šç¯‡/ä¸‹ç¯‡/ä¸Šé›†/ä¸‹é›†ã€è¦–ç‚ºé›†æ•¸æ¨™è¨˜ï¼ˆå¯å‡ºç¾åœ¨ä»»ä½•ä½ç½®ï¼‰
EP_TAG_RE = re.compile(r"(ï¼ˆä¸Šï¼‰|ï¼ˆä¸‹ï¼‰|\(ä¸Š\)|\(ä¸‹\)|ä¸Šç¯‡|ä¸‹ç¯‡|ä¸Šé›†|ä¸‹é›†)")

def get_episode_tag(title: str) -> Optional[str]:
    """å›å‚³ 'ä¸Š' / 'ä¸‹' / Noneï¼ˆä¸é™å‡ºç¾åœ¨çµå°¾ï¼‰"""
    if not title:
        return None
    t = title.strip()
    if re.search(r"(ï¼ˆä¸Šï¼‰|\(ä¸Š\)|ä¸Šç¯‡|ä¸Šé›†)", t):
        return "ä¸Š"
    if re.search(r"(ï¼ˆä¸‹ï¼‰|\(ä¸‹\)|ä¸‹ç¯‡|ä¸‹é›†)", t):
        return "ä¸‹"
    return None

def get_base_key(section_title: str, title: str) -> str:
    """
    ç”¨ä¾†æŠŠã€Œä¸Š/ä¸‹ã€è¦–ç‚ºåŒä¸€çµ„çš„ key
    - ç§»é™¤æ¨™é¡Œä¸­çš„ä¸Š/ä¸‹æ¨™è¨˜ï¼ˆä¸é™ä½ç½®ï¼‰
    - å†ç”¨ section_title + æ¸…ç†å¾Œ title ç•¶ key
    """
    s = (section_title or "").strip()
    t = (title or "").strip()
    t2 = EP_TAG_RE.sub("", t)  # âœ… ä¸é™çµå°¾ï¼Œç›´æ¥æŠŠ(ä¸Š)/(ä¸‹)ç§»é™¤
    t2 = re.sub(r"\s+", "", t2)  # å¯é¸ï¼šå»ç©ºç™½ï¼Œè®“ key æ›´ç©©
    s2 = re.sub(r"\s+", "", s)
    return f"{s2}||{t2}"


def reorder_episode_pairs(results: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    """
    ç›®æ¨™ï¼š
    - åŒä¸€ç³»åˆ—ï¼ˆåŒ base_keyï¼‰çš„ä¸Š/ä¸‹è¦é»åœ¨ä¸€èµ·ï¼šä¸Š â†’ ä¸‹
    - ç³»åˆ—èˆ‡ç³»åˆ—ä¹‹é–“ï¼Œç”¨è©²ç³»åˆ—çš„ä»£è¡¨åˆ†æ•¸æ’åºï¼ˆé è¨­å–ç³»åˆ—å…§æœ€é«˜ _scoreï¼‰
    - æ²’æœ‰ä¸Š/ä¸‹æ¨™è¨˜çš„å–®ç¯‡ï¼Œè¦–ç‚ºä¸€å€‹ç¨ç«‹ç³»åˆ—ï¼Œä¿ç•™åŸæœ¬é …ç›®æœ¬èº«
    """

    # 1) å»ºç«‹ç¾¤çµ„ï¼škey -> {"items": [...], "best_score": float, "first_idx": int}
    groups: Dict[str, Dict[str, Any]] = {}

    for idx, r in enumerate(results):
        key = get_base_key(r.get("section_title"), r.get("title"))
        score = float(r.get("_score", 0.0))

        g = groups.get(key)
        if g is None:
            groups[key] = {
                "items": [],
                "best_score": score,
                "first_idx": idx,  # è‹¥åˆ†æ•¸ä¸€æ¨£ï¼Œç”¨æœ€æ—©å‡ºç¾é †åºç•¶ tie-break
            }
            g = groups[key]

        g["items"].append(r)
        if score > g["best_score"]:
            g["best_score"] = score

    # 2) æ¯çµ„å…§ï¼šä¸Šâ†’ä¸‹â†’å…¶ä»–ï¼ˆå¦‚æœæœ‰å¥‡æ€ªçš„æ²’æ¨™è¨˜ï¼‰
    def item_rank(r: Dict[str, Any]) -> int:
        tag = get_episode_tag(r.get("title") or "")
        if tag == "ä¸Š":
            return 0
        if tag == "ä¸‹":
            return 1
        return 2

    for g in groups.values():
        # åŒä¸€çµ„å¯èƒ½æœ‰å¤šå€‹ä¸Šæˆ–å¤šå€‹ä¸‹ï¼šåŒ rank å…§å†ç”¨åˆ†æ•¸é«˜çš„æ’å‰
        g["items"].sort(key=lambda r: (item_rank(r), -float(r.get("_score", 0.0))))

    # 3) çµ„èˆ‡çµ„ä¹‹é–“æ’åºï¼šåˆ†æ•¸é«˜çš„çµ„æ’å‰ï¼›åˆ†æ•¸åŒå‰‡ç”¨ first_idx ä¿æŒç©©å®š
    ordered_groups = sorted(
        groups.values(),
        key=lambda g: (-g["best_score"], g["first_idx"])
    )

    # 4) æ”¤å¹³æˆä¸€æ¢ list
    out: List[Dict[str, Any]] = []
    for g in ordered_groups:
        out.extend(g["items"])

    return out

def format_time(seconds: float) -> str:
    s = int(seconds)
    h = s // 3600
    m = (s % 3600) // 60
    sec = s % 60
    if h > 0:
        return f"{h:02d}:{m:02d}:{sec:02d}"
    return f"{m:02d}:{sec:02d}"

def search_units(units: List[Dict[str, Any]], query: str, top_k: int = TOP_K):
    terms = normalize_query(query)
    if not terms:
        return []
    
    core_terms: List[str] = [t for t in terms if t in MENTAL_KEYWORDS]

    if not core_terms:
        long_terms = sorted([t for t in terms if len(t) >= 2],
                            key=lambda x: len(x),
                            reverse=True)
        core_terms = long_terms[:2]

    print(f"[debug] query={query} â†’ terms={terms} | core_terms={core_terms}")

    results = []
    for u in units:
        score, best_seg = score_unit(u, terms, core_terms)
        if score > 0:
            r = dict(u)
            r["_score"] = score
            r["_best_segment"] = best_seg
            results.append(r)

    results.sort(key=lambda x: x["_score"], reverse=True)
    return results

# ---------- äº’å‹•ä¸»è¿´åœˆ ----------
app = FastAPI(title="å¿ƒå¿«æ´»èª²ç¨‹æ¨è–¦ API")

# è‹¥å‰ç«¯ç¶²é æœƒè·¨ç¶²åŸŸå‘¼å«ï¼Œå¯é–‹ CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # ä¸Šç·šæ™‚å»ºè­°æ”¹æˆä½ çš„ç¶²åŸŸ
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# â–¼â–¼ æ–°å¢ï¼šè®“ /static åº•ä¸‹å¯ä»¥ç›´æ¥æŠ“æª”æ¡ˆ â–¼â–¼
app.mount("/static", StaticFiles(directory="static"), name="static")
@app.get("/", include_in_schema=False)
def serve_index():
    return FileResponse("static/index.html")

# å•Ÿå‹•æ™‚å°±å…ˆè¼‰å…¥æ‰€æœ‰å–®å…ƒ
UNITS_CACHE: List[Dict[str, Any]] = load_all_units()

# ç°¡å–®æ”¾åœ¨è¨˜æ†¶é«”çš„èŠå¤©ç´€éŒ„ï¼ˆserver é‡å•Ÿæœƒæ¸…ç©ºï¼‰
HISTORY: Dict[str, List[Dict[str, Any]] ] = {}


class ChatRequest(BaseModel):
    query: str
    session_id: Optional[str] = None


class NearbyRequest(BaseModel):
    address: str


class RecommendRequest(BaseModel):
    query: str


@app.get("/ping")
def ping():
    return {"status": "ok"}


@app.post("/chat")
def chat(req: ChatRequest):
    q = req.query.strip()
    session_id = req.session_id or "anonymous"  # æ²’å‚³å°±å…ˆæ­¸åˆ° anonymousï¼ˆç†è«–ä¸Šå‰ç«¯éƒ½æœƒå‚³ï¼‰
    resp: Dict[str, Any]
    print(">>> /chat session_id =", session_id)
    # 1) åˆ¤æ–·æ˜¯å¦ç‚ºã€Œå¿ƒæ“šé»ã€/ã€Œçœ‹è¨ºã€è©¢å•
    if ("é™„è¿‘" in q) and ("å¿ƒæ“šé»" in q or "çœ‹è¨º" in q or "é–€è¨º" in q):
        addr = extract_address_from_query(q)
        if not addr:
            resp = {
                "type": "xin_points",
                "address": None,
                "points": [],
                "message": "æˆ‘æœ‰é»æŠ“ä¸åˆ°åœ°å€ï¼Œè«‹å˜—è©¦è¼¸å…¥å®Œæ•´åœ°å€ï¼Œä¾‹å¦‚ï¼šå°å—å¸‚æ±å€å¤§å­¸è·¯1è™Ÿ"
            }
        else:
            geo = geocode_address(addr)
            if not geo:
                resp = {
                    "type": "xin_points",
                    "address": addr,
                    "points": [],
                    "message": f"æŸ¥ä¸åˆ°ã€Œ{addr}ã€é€™å€‹åœ°å€ï¼Œè«‹æ”¹æˆæ›´æ­£å¼çš„å¯«æ³•è©¦è©¦çœ‹"
                }
            else:
                lat, lon = geo
                results = find_nearby_points(lat, lon, max_km=5, top_k=TOP_K)
                resp = build_nearby_points_response(addr, results)

    # 2) ç›´æ¥è¼¸å…¥å®Œæ•´åœ°å€ï¼ˆé–‹é ­å°±æ˜¯ã€Œå°å—å¸‚xxxã€ä¹‹é¡ï¼‰
    elif ADDR_HEAD_RE.match(q):
        addr = q
        geo = geocode_address(addr)
        if not geo:
            resp = {
                "type": "xin_points",
                "address": addr,
                "points": [],
                "message": f"æŸ¥ä¸åˆ°ã€Œ{addr}ã€é€™å€‹åœ°å€ï¼Œè«‹æ”¹æˆæ›´æ­£å¼çš„å¯«æ³•è©¦è©¦çœ‹"
            }
        else:
            lat, lon = geo
            results = find_nearby_points(lat, lon, max_km=5, top_k=TOP_K)
            resp = build_nearby_points_response(addr, results)
    elif detect_pagination_intent(q):
        history = HISTORY.get(session_id, [])
        # æ‰¾æœ€è¿‘ä¸€ç­†ã€Œèª²ç¨‹æ¨è–¦ã€
        last = next(
            (h for h in reversed(history)
            if h["response"].get("type") == "course_recommendation"),
            None
        )

        if not last:
            resp = {
                "type": "text",
                "message": "ç›®å‰æ²’æœ‰ä¸Šä¸€ç­†æ¨è–¦çµæœï¼Œå¯ä»¥å…ˆå•ä¸€å€‹å•é¡Œ ğŸ˜Š"
            }
        else:
            prev = last["response"]
            new_offset = prev["offset"] + prev["limit"]

            full_results = search_units(UNITS_CACHE, prev["query"], top_k=9999)
            resp = build_recommendations_response(
                prev["query"],
                full_results,
                offset=new_offset,
                limit=TOP_K
            )
    # 3) ç‰¹å®šæƒ…å¢ƒï¼šç›´æ¥çµ¦å»ºè­°ï¼Œä¸èµ°èª²ç¨‹æ¨è–¦
    else:
        special_intent = detect_special_intent(q)
        print(f"[chat-debug] special_intent={special_intent}")
        if special_intent:
            resp = build_special_intent_response(special_intent, q)
        else:
            # 4) å…¶ä»–æƒ…æ³ï¼šç•¶ä½œèª²ç¨‹ / æ–‡ç« æ¨è–¦æŸ¥è©¢
            full_results = search_units(UNITS_CACHE, q, top_k=9999)
            resp = build_recommendations_response(q, full_results, offset=0, limit=TOP_K)

    # --- è¨˜éŒ„æ­·å²ï¼šä¾ session_id åˆ†é–‹ ---
    history_list = HISTORY.setdefault(session_id, [])
    history_list.append({
        "query": q,
        "response": resp,
    })
    if len(history_list) > 50:
        history_list.pop(0)

    return resp


@app.get("/history")
def get_history(session_id: str):
    """
    ä¾ session_id å›å‚³å°ˆå±¬èŠå¤©ç´€éŒ„ã€‚
    /history?session_id=xxxx
    """
    print(">>> /history called session_id =", session_id)
    items = HISTORY.get(session_id, [])
    print(">>> HISTORY keys =", list(HISTORY.keys()))
    return {
        "items": items
    }

@app.post("/nearby")
def nearby(req: NearbyRequest):
    addr = req.address.strip()
    if not addr:
        return {
            "type": "xin_points",
            "address": None,
            "points": [],
            "message": "è«‹æä¾›å®Œæ•´åœ°å€ï¼Œä¾‹å¦‚ï¼šå°å—å¸‚æ±å€å¤§å­¸è·¯1è™Ÿ"
        }

    geo = geocode_address(addr)
    if not geo:
        return {
            "type": "xin_points",
            "address": addr,
            "points": [],
            "message": f"æŸ¥ä¸åˆ°ã€Œ{addr}ã€é€™å€‹åœ°å€ï¼Œè«‹æ”¹æˆæ›´æ­£å¼çš„å¯«æ³•è©¦è©¦çœ‹"
        }

    lat, lon = geo
    results = find_nearby_points(lat, lon, max_km=5, top_k=TOP_K)
    return build_nearby_points_response(addr, results)


@app.post("/recommend")
def recommend(req: RecommendRequest):
    q = req.query.strip()
    full_results = search_units(UNITS_CACHE, q, top_k=9999)
    resp = build_recommendations_response(q, full_results, offset=0, limit=TOP_K)
    return resp

if __name__ == "__main__":
    import uvicorn
    uvicorn.run("xin_api:app", host="0.0.0.0", port=8000, reload=True)