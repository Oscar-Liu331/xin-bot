import json
import re
from pathlib import Path
from typing import List, Dict, Any

import requests
from math import radians, sin, cos, asin, sqrt

from fastapi import FastAPI
from pydantic import BaseModel
from fastapi.middleware.cors import CORSMiddleware

from fastapi.staticfiles import StaticFiles
from fastapi.responses import FileResponse

from typing import Optional


CITY_PATTERN = (
    r"(å°åŒ—å¸‚|è‡ºåŒ—å¸‚|æ–°åŒ—å¸‚|æ¡ƒåœ’å¸‚|è‡ºä¸­å¸‚|å°ä¸­å¸‚|è‡ºå—å¸‚|å°å—å¸‚|é«˜é›„å¸‚|"
    r"åŸºéš†å¸‚|æ–°ç«¹å¸‚|å˜‰ç¾©å¸‚|æ–°ç«¹ç¸£|è‹—æ —ç¸£|å½°åŒ–ç¸£|å—æŠ•ç¸£|é›²æ—ç¸£|å˜‰ç¾©ç¸£|"
    r"å±æ±ç¸£|å®œè˜­ç¸£|èŠ±è“®ç¸£|è‡ºæ±ç¸£|å°æ±ç¸£|æ¾æ¹–ç¸£|é‡‘é–€ç¸£|é€£æ±Ÿç¸£)"
)

ADDR_HEAD_RE = re.compile(rf"^{CITY_PATTERN}(.*?(å€|é„‰|é®|å¸‚))")

SECTION = "pro"
DATASET_PATTERN = f"elearn_{SECTION}_*_*_dataset.json"
TOP_K = 5  

XIN_POINTS_FILE = Path("xin_points.json")
UNITS_FILE = Path("wellbeing_elearn_pro_all_with_articles.json")

# --- é—œéµå­—è¼‰å…¥é‚è¼¯ ---
KEYWORDS_FILE = Path("keywords.json")
# --- å…¨åŸŸè®Šæ•¸ ---
KEYWORDS_DATA = {} # å­˜æ”¾åŸå§‹åˆ†é¡çµæ§‹
MENTAL_KEYWORDS = [] # æ‰å¹³åŒ–å¾Œçš„æ¸…å–®ï¼Œä¾›åŸæœå°‹é‚è¼¯ä½¿ç”¨
STOP_WORDS = []

def load_keywords_from_json():
    global KEYWORDS_DATA, MENTAL_KEYWORDS, STOP_WORDS
    try:
        if KEYWORDS_FILE.exists():
            with open(KEYWORDS_FILE, "r", encoding="utf-8") as f:
                data = json.load(f)
                KEYWORDS_DATA = data.get("mental_keywords", {})
                
                # å°‡æ‰€æœ‰åˆ†é¡çš„é—œéµå­—æ”¤å¹³æˆä¸€å€‹æ¸…å–®ï¼Œç›¸å®¹èˆŠæœ‰çš„æœå°‹é‚è¼¯
                all_kws = []
                for category_list in KEYWORDS_DATA.values():
                    all_kws.extend(category_list)
                
                # å»é™¤é‡è¤‡é …
                MENTAL_KEYWORDS = list(set(all_kws))
                STOP_WORDS = data.get("stop_words", [])
                
            print(f"[load] âœ… åˆ†é¡è¼‰å…¥æˆåŠŸã€‚å…± {len(KEYWORDS_DATA)} å€‹é¡åˆ¥ï¼Œ{len(MENTAL_KEYWORDS)} å€‹é—œéµå­—ã€‚")
    except Exception as e:
        print(f"[load] âŒ åˆ†é¡è¼‰å…¥å¤±æ•—: {e}")

# åˆå§‹è¼‰å…¥
load_keywords_from_json()

def detect_pagination_intent(q: str) -> bool:
    return any(w in q for w in ["çµ¦æˆ‘å¾Œäº”å€‹","çµ¦æˆ‘ä¸‹äº”å€‹","å¾Œäº”å€‹","ä¸‹äº”å€‹","ä¸‹ä¸€é ","æ›´å¤šæ¨è–¦"])

def check_category_intent(text: str, category_name: str) -> bool:
    """å·¥å…·å‡½å¼ï¼šæª¢æŸ¥æ–‡å­—ä¸­æ˜¯å¦åŒ…å«ç‰¹å®šåˆ†é¡çš„é—œéµå­—"""
    keywords = KEYWORDS_DATA.get(category_name, [])
    return any(kw in text for kw in keywords)

def detect_special_intent(q: str) -> Optional[str]:
    """
    ç²¾ç¢ºåµæ¸¬å››å¤§ç‰¹å®šå•é¡Œï¼Œä½¿ç”¨ç´°åˆ†å¾Œçš„ JSON é¡åˆ¥
    """
    text = re.sub(r"\s+", "", q).lower()
    
    def has_cat(category_name):
        return any(kw in text for kw in KEYWORDS_DATA.get(category_name, []))

    # ---------- 1. æ†‚é¬±å°±é†«å»ºè­° ----------
    # é‚è¼¯ï¼š(æ†‚é¬±é¡ æˆ– å‹•åŠ›é¡) + (å°±é†«é—œéµå­— æˆ– ç–‘å•å¥)
    if has_cat("depressive_mood") or has_cat("low_motivation"):
        if any(w in text for w in ["é†«å¸«", "é†«ç”Ÿ", "å¿ƒç†å¸«", "èº«å¿ƒç§‘", "è©²ä¸è©²", "è¦ä¸è¦", "çœ‹è¨º"]):
            return "depression_go_doctor"

    # ---------- 2. é•·è¼©å¤±æ™ºç¢ºèªèˆ‡çœ‹è¨º ----------
    # é‚è¼¯ï¼šç›´æ¥å‘½ä¸­ã€Œå¤±æ™ºã€ + (é•·è¼©é¡ æˆ– è©¢å•ç§‘åˆ¥/ç¢ºèª)
    if "å¤±æ™º" in text:
        if has_cat("family_elder") or any(w in text for w in ["å“ªä¸€ç§‘", "ç¢ºå®š", "æª¢æŸ¥", "è¨ºæ–·"]):
            return "dementia_parent"

    # ---------- 3. å­©å­æ‰‹æ©Ÿå•é¡Œ ----------
    # é‚è¼¯ï¼š(å­©å­é¡) + (æ•¸ä½æˆç™®é¡)
    if has_cat("child_teen") and has_cat("digital_addiction"):
        return "child_phone"

    # ---------- 4. å©†å©†æ•™é¤Šè§€å¿µè¡çª ----------
    # é‚è¼¯ï¼š(å©†åª³é¡) + (æ•™é¤Šè¡çªé¡ æˆ– ç…§é¡§è¡Œç‚º)
    if has_cat("in_laws"):
        if has_cat("parenting_conflict") or any(w in text for w in ["ç…§é¡§", "å¾ˆä¸ä¸€æ¨£", "å·®ç•°"]):
            return "mother_in_law_childcare"

    return None

def build_special_intent_response(intent: str, q: str) -> Dict[str, Any]:
    """
    æŠŠå››ç¨®æƒ…å¢ƒçš„å»ºè­°ï¼ŒåŒ…æˆçµæ§‹åŒ– JSONï¼Œçµ¦å‰ç«¯æ¸²æŸ“ã€‚
    """
    if intent == "depression_go_doctor":
        title = "è¦ºå¾—è‡ªå·±æœ‰é»æ†‚é¬±ï¼Œè©²ä¸è©²å»çœ‹å¿ƒç†é†«å¸«ï¼Ÿ"
        sections = [
            {
                "title": "1ï¸âƒ£ ä»€éº¼æƒ…æ³æ¯”è¼ƒå»ºè­°æ‰¾å°ˆæ¥­é†«å¸«æˆ–å¿ƒç†å¸«ï¼Ÿ",
                "items": [
                    "æƒ…ç·’ä½è½ã€æ²’å‹•åŠ›ã€å®¹æ˜“æƒ³å“­ï¼ŒæŒçºŒè¶…é 2 é€±ä»¥ä¸Šã€‚",
                    "å½±éŸ¿åˆ°å·¥ä½œã€èª²æ¥­ã€ç¡çœ ã€é£Ÿæ…¾æˆ–äººéš›é—œä¿‚ã€‚",
                    "å‡ºç¾ã€Œä¸å¦‚æ¶ˆå¤±ç®—äº†ã€ã€ã€Œæ´»è‘—å¥½ç´¯ã€é€™é¡è² é¢æˆ–è‡ªå‚·çš„æƒ³æ³•ã€‚"
                ]
            },
            {
                "title": "2ï¸âƒ£ å¯ä»¥çœ‹çš„ç§‘åˆ¥ / å°ˆæ¥­",
                "items": [
                    "é†«é™¢çš„ã€Œèº«å¿ƒç§‘ / ç²¾ç¥ç§‘ã€é–€è¨ºï¼Œå¯ä»¥è©•ä¼°æ˜¯å¦éœ€è¦ç”¨è—¥æˆ–é€²ä¸€æ­¥æª¢æŸ¥ã€‚",
                    "é†«é™¢æˆ–ç¤¾å€çš„ã€Œè‡¨åºŠå¿ƒç†å¸«ã€è«®å•†å¿ƒç†å¸«ã€åšå¿ƒç†è«®å•†ã€‚",
                    "å¦‚æœé‚„ä¸ç¢ºå®šï¼Œä¹Ÿå¯ä»¥å…ˆå¾ã€Œå®¶é†«ç§‘ã€æˆ–ç¤¾å€å¿ƒç†è¡›ç”Ÿä¸­å¿ƒè«®è©¢é–‹å§‹ã€‚"
                ]
            },
            {
                "title": "3ï¸âƒ£ å¦‚æœç¾åœ¨é‚„å‹‰å¼·æ’å¾—ä½ï¼Œå¯ä»¥å…ˆå˜—è©¦çš„èª¿æ•´",
                "items": [
                    "å›ºå®šç¡è¦ºã€èµ·åºŠæ™‚é–“ï¼Œç›¡é‡å°‘ç†¬å¤œã€‚",
                    "æ‰¾ä¸€å€‹ä¿¡ä»»çš„äººèŠèŠï¼ŒæŠŠå£“åŠ›èªªå‡ºä¾†ã€‚",
                    "å…ˆå¾çŸ­æ™‚é–“æ•£æ­¥æˆ–ç°¡å–®é‹å‹•é–‹å§‹ï¼Œè®“èº«é«”å‹•èµ·ä¾†ã€‚"
                ]
            },
            {
                "title": "âš ï¸ ä»€éº¼æƒ…æ³è¦ç«‹åˆ»å°‹æ±‚å”åŠ©ï¼Ÿ",
                "items": [
                    "æœ‰æ˜é¡¯çš„è‡ªæ®ºå¿µé ­ã€è¡å‹•ï¼Œæˆ–å·²ç¶“æƒ³å¥½æ–¹æ³•ã€‚",
                    "æ­¤æ™‚è«‹å„˜å¿«åˆ°é†«é™¢æ€¥è¨ºï¼Œæˆ–è«‹å®¶äººæœ‹å‹é™ªåŒå°±é†«ï¼Œä¸¦å¯è¯çµ¡è‡ªæ®ºé˜²æ²» / å¿ƒç†æ”¯æŒå°ˆç·šã€‚"
                ]
            }
        ]

    elif intent == "dementia_parent":
        title = "æ“”å¿ƒçˆ¸çˆ¸ / å®¶äººå¯èƒ½æœ‰å¤±æ™ºï¼Œæ€éº¼ç¢ºå®šï¼Ÿçœ‹å“ªä¸€ç§‘ï¼Ÿ"
        sections = [
            {
                "title": "1ï¸âƒ£ å¸¸è¦‹çš„å¤±æ™ºè­¦è¨Šï¼ˆèˆ‰ä¾‹ï¼‰",
                "items": [
                    "è¨˜æ†¶åŠ›æ˜é¡¯è®Šå·®ï¼šåŒä¸€ä»¶äº‹å•å¾ˆå¤šæ¬¡ï¼Œå¿˜è¨˜å‰›ç™¼ç”Ÿçš„äº‹æƒ…ã€‚",
                    "å®¹æ˜“è¿·è·¯ï¼šåœ¨ç†Ÿæ‚‰çš„ç’°å¢ƒåè€Œæœƒèµ°éŒ¯ã€æ‰¾ä¸åˆ°è·¯ã€‚",
                    "åˆ¤æ–·åŠ›è®Šå·®ï¼šä¾‹å¦‚å®¹æ˜“è¢«è©é¨™ã€åšä¸€äº›ä»¥å‰ä¸æœƒåšçš„æ€ªæ±ºå®šã€‚",
                    "æ€§æ ¼æˆ–è¡Œç‚ºæ”¹è®Šï¼šè®Šå¾—æ˜é¡¯æš´èºã€é€€ç¸®ï¼Œæˆ–è·Ÿä»¥å‰å€‹æ€§å·®å¾ˆå¤šã€‚"
                ]
            },
            {
                "title": "2ï¸âƒ£ è¦æ€éº¼æ¯”è¼ƒç¢ºå®šæ˜¯ä¸æ˜¯å¤±æ™ºï¼Ÿ",
                "items": [
                    "éœ€è¦ç”±é†«å¸«åšå®Œæ•´è©•ä¼°ï¼Œå¯èƒ½åŒ…æ‹¬å•è¨ºã€ç¥ç¶“å¿ƒç†é‡è¡¨ã€è¡€æ¶²æª¢æŸ¥ã€å½±åƒæª¢æŸ¥ç­‰ã€‚",
                    "å®¶å±¬å¯ä»¥å…ˆæ•´ç†æœ€è¿‘è§€å¯Ÿåˆ°çš„æ”¹è®Šï¼ˆå¾ä»€éº¼æ™‚å€™é–‹å§‹ã€ç™¼ç”Ÿåœ¨ä»€éº¼æƒ…å¢ƒï¼‰ã€‚"
                ]
            },
            {
                "title": "3ï¸âƒ£ å»ºè­°å…ˆçœ‹å“ªä¸€ç§‘ï¼Ÿ",
                "items": [
                    "å¤§å‹é†«é™¢å¸¸è¦‹ç§‘åˆ¥ï¼šç¥ç¶“å…§ç§‘ã€å®¶é†«ç§‘ã€è€å¹´é†«å­¸ç§‘ï¼Œéƒ¨åˆ†é†«é™¢æœ‰ã€Œå¤±æ™ºå…±åŒç…§è­·é–€è¨ºã€ã€‚",
                    "è‹¥é•·è¼©æœ‰æ˜é¡¯æƒ…ç·’æˆ–è¡Œç‚ºæ”¹è®Šï¼ˆå¦‚å¦„æƒ³ã€å¹»è¦ºã€åš´é‡ç„¦æ…®ï¼‰ï¼Œèº«å¿ƒç§‘ / ç²¾ç¥ç§‘ä¹Ÿèƒ½å”åŠ©è©•ä¼°ã€‚"
                ]
            },
            {
                "title": "4ï¸âƒ£ å¸¶é•·è¼©çœ‹è¨ºçš„å°æŠ€å·§",
                "items": [
                    "å¯ä»¥ç”¨ã€Œåšå¥åº·æª¢æŸ¥ã€çš„æ–¹å¼é‚€è«‹ï¼Œè€Œä¸æ˜¯ç›´æ¥èªªã€Œæ‡·ç–‘ä½ å¤±æ™ºã€ã€‚",
                    "çœ‹è¨ºæ™‚ï¼Œå®¶å±¬å¯æŠŠåœ¨å®¶è§€å¯Ÿåˆ°çš„ç‹€æ³æ•´ç†åœ¨ç´™ä¸Šçµ¦é†«å¸«çœ‹ï¼Œæ¯”è¼ƒä¸æœƒæ¼è¬›ã€‚"
                ]
            }
        ]

    elif intent == "child_phone":
        title = "åœ‹å°å­©å­æ‰‹æ©Ÿè¶Šç©è¶Šå…‡ï¼Œæˆ‘è©²æ€éº¼è¾¦ï¼Ÿ"
        sections = [
            {
                "title": "1ï¸âƒ£ å…ˆäº†è§£ã€Œæ€éº¼ç©ã€è€Œä¸åªæ˜¯ã€Œç©å¤šä¹…ã€",
                "items": [
                    "ä¸»è¦æ˜¯åœ¨ç©éŠæˆ²ã€çœ‹å½±ç‰‡ï¼Œé‚„æ˜¯è·ŸåŒå­¸èŠå¤©ï¼Ÿ",
                    "é€šå¸¸åœ¨ä»€éº¼æ™‚é–“é»ç©ï¼šå¯«åŠŸèª²å‰ï¼Ÿç¡å‰ï¼Ÿå‡æ—¥æ•´å¤©ï¼Ÿ"
                ]
            },
            {
                "title": "2ï¸âƒ£ è·Ÿå­©å­ä¸€èµ·ã€Œè«‡è¦å‰‡ã€ï¼Œä¸æ˜¯åªä¸‹å‘½ä»¤",
                "items": [
                    "ä¾‹å¦‚ï¼šå¹³æ—¥æ¯å¤©å¯ä»¥ç© 30â€“60 åˆ†é˜ï¼Œå…ˆå¯«å®Œä½œæ¥­ã€æ´—æ¾¡å†é–‹æ©Ÿã€‚",
                    "é¿å…ç¡å‰ 1 å°æ™‚ç”¨æ‰‹æ©Ÿï¼Œè®“å¤§è…¦æœ‰æ™‚é–“ã€Œé™é€Ÿã€æ¯”è¼ƒå¥½ç¡ã€‚",
                    "æŠŠè¦å‰‡å¯«åœ¨ç´™ä¸Šè²¼å‡ºä¾†ï¼Œæ¸›å°‘è‡¨æ™‚åµæ¶ã€‚"
                ]
            },
            {
                "title": "3ï¸âƒ£ æä¾›æ›¿ä»£æ´»å‹•ï¼Œä¸æ˜¯åªæœ‰ã€Œä¸å‡†ç©ã€",
                "items": [
                    "å®‰æ’å¯ä»¥ä¸€èµ·åšçš„äº‹ï¼šæ¡ŒéŠã€é‹å‹•ã€æ•£æ­¥ã€ç•«ç•«ã€åšæ–™ç†ã€‚",
                    "å‡æ—¥ç´„å®šã€Œç„¡æ‰‹æ©Ÿæ™‚æ®µã€ï¼Œå…¨å®¶ä¸€èµ·åšåˆ¥çš„æ´»å‹•ã€‚"
                ]
            },
            {
                "title": "4ï¸âƒ£ å¤§äººä¹Ÿè¦ç•¶æ¦œæ¨£",
                "items": [
                    "å¦‚æœçˆ¶æ¯ä¸€ç›´æ»‘æ‰‹æ©Ÿï¼Œå°å­©å¾ˆé›£æ¥å—ã€Œä½ ä¸å¯ä»¥æ»‘ã€ã€‚",
                    "å¯ä»¥ä¸€èµ·ç´„å®šï¼šåƒé£¯ã€ç¡å‰åŠå°æ™‚ï¼Œå…¨å®¶éƒ½ä¸çœ‹æ‰‹æ©Ÿã€‚"
                ]
            },
            {
                "title": "5ï¸âƒ£ ä»€éº¼æ™‚å€™éœ€è¦å°ˆæ¥­å”åŠ©ï¼Ÿ",
                "items": [
                    "å·²ç¶“å½±éŸ¿åˆ°å­¸æ¥­ã€ç¡çœ ï¼Œæˆ–ç‚ºäº†æ‰‹æ©Ÿæœƒå¤§åµå¤§é¬§ã€æ‘”æ±è¥¿ã€‚",
                    "å¯ä»¥è€ƒæ…®è«®è©¢ï¼šå­¸æ ¡è¼”å°è€å¸«ã€å…’ç«¥é’å°‘å¹´èº«å¿ƒç§‘ã€å…’ç«¥è‡¨åºŠ / è«®å•†å¿ƒç†å¸«ã€‚"
                ]
            }
        ]

    elif intent == "mother_in_law_childcare":
        title = "å©†å©†ç…§é¡§å°å­©æ–¹å¼è·Ÿæˆ‘å¾ˆä¸ä¸€æ¨£ï¼Œæˆ‘è©²æ€éº¼è¾¦ï¼Ÿ"
        sections = [
            {
                "title": "1ï¸âƒ£ å…ˆåˆ†è¾¨ï¼šæ˜¯ã€åšæ³•ä¸åŒã€é‚„æ˜¯ã€å®‰å…¨æœ‰ç–‘æ…®ã€",
                "items": [
                    "åšæ³•ä¸åŒä½†å®‰å…¨ï¼šä¾‹å¦‚é›¶é£Ÿå¤šä¸€é»ã€çœ‹é›»è¦–ä¹…ä¸€é»ï¼Œå¯ä»¥å…ˆç•¶æˆã€Œé¢¨æ ¼å·®ç•°ã€ã€‚",
                    "æ¶‰åŠå®‰å…¨ï¼šä¾‹å¦‚è®“å°å­©å–®ç¨åœ¨é™½å°ã€åƒå®¹æ˜“å™åˆ°çš„é£Ÿç‰©ï¼Œå°±éœ€è¦æ¯”è¼ƒå …å®šåœ°æºé€šã€‚"
                ]
            },
            {
                "title": "2ï¸âƒ£ æºé€šæ™‚ï¼Œå…ˆæ„Ÿè¬å†è¡¨é”æ“”å¿ƒï¼ˆç”¨ã€Œæˆ‘ã€è¨Šæ¯ï¼‰",
                "items": [
                    "å¦‚ï¼šã€Œæˆ‘çœŸçš„å¾ˆæ„Ÿè¬å¦³å¹«å¿™é¡§å°å­©ï¼Œæˆ‘æœƒæ¯”è¼ƒæ”¾å¿ƒã€‚ã€",
                    "å†æ¥ï¼šã€Œåªæ˜¯æˆ‘æœ‰é»æ“”å¿ƒï¼Œä»–åƒå¤ªå¤šç³–å°ç‰™é½’ä¸å¥½ï¼Œæˆ‘æƒ³æˆ‘å€‘å¯ä¸å¯ä»¥ä¸€èµ·å¹«ä»–å°‘ä¸€é»ï¼Ÿã€",
                    "é¿å…ç”¨ã€Œå¦³é€™æ¨£ä¸å°ã€ã€Œå¦³æŠŠå°å­©å¸¶å£äº†ã€ï¼Œæ¯”è¼ƒä¸æœƒç«‹åˆ»è®Šæˆåµæ¶ã€‚"
                ]
            },
            {
                "title": "3ï¸âƒ£ å„˜é‡è®“å¦ä¸€åŠç•¶æ©‹æ¨‘",
                "items": [
                    "è‡ªå·±çš„çˆ¸åª½é€šå¸¸æ¯”è¼ƒè½è‡ªå·±å°å­©çš„è©±ã€‚",
                    "å¯ä»¥å…ˆè·Ÿå¦ä¸€åŠç§ä¸‹æºé€šå¥½ç«‹å ´èˆ‡åº•ç·šï¼Œå†ç”±ä»– / å¥¹è·Ÿå©†å©†èªªã€‚"
                ]
            },
            {
                "title": "4ï¸âƒ£ å»ºç«‹å¹¾æ¢ã€Œå…¨å®¶å…±åŒçš„åŸå‰‡ã€",
                "items": [
                    "ä¾‹å¦‚ï¼šç”¨è—¥ä¸€å®šè¦å•çˆ¸åª½ã€ä¸èƒ½æ‰“å°å­©ã€å¤§ç´„å¹¾é»ç¡è¦ºã€‚",
                    "åŸå‰‡è¶Šæ¸…æ¥šï¼Œè¶Šä¸æœƒæ¯ä»¶å°äº‹éƒ½åµæˆä¸€åœ˜ã€‚"
                ]
            },
            {
                "title": "5ï¸âƒ£ å¦‚æœè¡çªå½±éŸ¿åˆ°ä½ è‡ªå·±çš„æƒ…ç·’",
                "items": [
                    "å¯ä»¥è€ƒæ…®å’Œè«®å•†å¿ƒç†å¸«è«‡è«‡ï¼Œæ•´ç†è‡ªå·±çš„å§”å±ˆèˆ‡è§’è‰²å£“åŠ›ï¼ˆåª³å©¦ / åª½åª½é›™é‡èº«åˆ†ï¼‰ã€‚",
                    "æœ‰æ™‚å€™å•é¡Œä¸åªæ˜¯æ•™é¤ŠæŠ€å·§ï¼Œä¹Ÿæ˜¯ä½ å’Œå…ˆç”Ÿã€ä½ å’Œå©†å©†ä¹‹é–“çš„ç•Œç·šã€‚"
                ]
            }
        ]

    else:
        title = "ä¸€èˆ¬å»ºè­°"
        sections = [
            {
                "title": "",
                "items": ["é€™å€‹å•é¡Œç›®å‰é‚„æ²’æœ‰å°ˆé–€å¯«å¥½çš„å»ºè­°å›ç­”ï¼Œå…ˆç”¨èª²ç¨‹èˆ‡æ–‡ç« æ¨è–¦æ¨¡å¼å¹«ä½ æ‰¾è³‡æ–™ã€‚"]
            }
        ]

    return {
        "type": "advice",
        "intent": intent,
        "query": q,
        "title": title,
        "sections": sections
    }


def build_nearby_points_response(address: str, results):
    """
    æŠŠ find_nearby_points çš„çµæœè½‰æˆ JSON-friendly çµæ§‹
    """
    if not results:
        return {
            "type": "xin_points",
            "address": address,
            "points": [],
            "message": f"åœ¨ã€Œ{address}ã€5 å…¬é‡Œå…§æ²’æœ‰æ‰¾åˆ°å¿ƒæ“šé»"
        }

    points = []
    for p, d in results:
        points.append({
            "title": p.get("title"),
            "address": p.get("address"),
            "tel": p.get("tel"),
            "distance_km": round(d, 2),
        })

    return {
        "type": "xin_points",
        "address": address,
        "points": points
    }


def build_recommendations_response(
    query: str,
    results: List[Dict[str, Any]],
    offset: int = 0,
    limit: int = TOP_K
):
    # æ²’æœ‰çµæœ
    if not results:
        return {
            "type": "course_recommendation",
            "query": query,
            "total": 0,
            "video_count": 0,
            "article_count": 0,
            "offset": offset,
            "limit": limit,
            "has_more": False,
            "results": [],
            "message": "ç›®å‰æ‰¾ä¸åˆ°å¾ˆç¬¦åˆçš„èª²ç¨‹ï¼Œå¯ä»¥è©¦è‘—ç”¨ï¼šå©†åª³ã€å£“åŠ›ã€æ†‚é¬±ã€å¤±çœ â€¦ ç­‰è©å†è©¦è©¦çœ‹ã€‚"
        }

    # âœ… å…ˆé‡æ’ï¼ˆåˆ†é å‰ï¼‰
    results = reorder_episode_pairs(results)

    total = len(results)
    # âœ… è¨ˆç®—ç¸½æ•¸ï¼ˆç”¨é‡æ’å¾Œçš„ results çµ±è¨ˆå³å¯ï¼‰
    video_count = sum(1 for r in results if not r.get("is_article"))
    article_count = sum(1 for r in results if r.get("is_article"))

    # âœ… æ­£ç¢ºåˆ‡åˆ†é 
    page_results = results[offset: offset + limit]

    items = []
    for r in page_results:  # âœ… ä¸€å®šè¦ç”¨ page_results
        title = r.get("title") or "(ç„¡æ¨™é¡Œ)"
        section_title = r.get("section_title") or "(æœªåˆ†é¡å°ç¯€)"
        score = r.get("_score", 0.0)

        is_article = bool(r.get("is_article"))
        youtube_url = r.get("youtube_url")

        entry: Dict[str, Any] = {
            "section_title": section_title,
            "title": title,
            "score": score,
            "is_article": is_article,  # âœ… çµ¦å‰ç«¯ç›´æ¥ç”¨
            "type": "article" if is_article else "video",
        }

        if is_article:
            article_url = r.get("article_url") or r.get("url")
            content_text = (r.get("content_text") or "").replace("\n", " ")
            snippet = content_text[:100] + ("..." if len(content_text) > 100 else "")
            entry["article_url"] = article_url
            entry["snippet"] = snippet
        else:
            seg = r.get("_best_segment")
            if seg:
                start_sec = seg.get("start_sec", 0.0)
                start_str = format_time(start_sec)
                seg_text = seg.get("text", "") or ""
                hint = f"è©²å–®å…ƒåœ¨ {start_str} æœ‰æåˆ°ï¼šã€Œ{seg_text[:30]}...ã€"
            else:
                hint = "å­—å¹•è£¡æ²’æœ‰ç‰¹åˆ¥å‘½ä¸­é—œéµå¥ï¼Œå¯ä»¥å¾é ­é–‹å§‹çœ‹ã€‚"
            entry["youtube_url"] = youtube_url
            entry["hint"] = hint

        items.append(entry)

    return {
        "type": "course_recommendation",
        "query": query,
        "total": total,
        "video_count": video_count,
        "article_count": article_count,
        "offset": offset,
        "limit": limit,
        "has_more": offset + limit < total,
        "results": items
    }



def load_xin_points() -> List[Dict[str, Any]]:
    try:
        data = json.loads(XIN_POINTS_FILE.read_text("utf-8"))
        return data.get("data", [])
    except Exception as e:
        print(f"[xin] âš ï¸ å¿ƒæ“šé»è¼‰å…¥å¤±æ•—ï¼š{e}")
        return []
    
def haversine_km(lon1, lat1, lon2, lat2) -> float:
    lon1, lat1, lon2, lat2 = map(radians, [lon1, lat1, lon2, lat2])
    dlon = lon2 - lon1
    dlat = lat2 - lat1
    a = sin(dlat/2)**2 + cos(lat1)*cos(lat2)*sin(dlon/2)**2
    c = 2 * asin(sqrt(a))
    return 6371 * c  # km

def geocode_address(address: str):
    if not address:
        return None

    def try_geocode(addr: str):
        url = "https://nominatim.openstreetmap.org/search"
        params = {"q": addr, "format": "json", "limit": 1}
        headers = {"User-Agent": "xin-bot/1.0"}
        try:
            r = requests.get(url, params=params, headers=headers, timeout=5)
            r.raise_for_status()
            data = r.json()
            if data:
                lat = float(data[0]["lat"])
                lon = float(data[0]["lon"])
                print(f"[geocode] å‘½ä¸­ï¼š'{addr}' -> lat={lat}, lon={lon}")
                return lat, lon
        except Exception as e:
            print(f"[geocode] éŒ¯èª¤ï¼š{e}")
        return None

    print(f"[geocode] å˜—è©¦ï¼š{address}")
    result = try_geocode(address)
    if result:
        return result

    if "è‡º" in address:
        addr2 = address.replace("è‡º", "å°")
        print(f"[geocode] å˜—è©¦ï¼š{addr2}")
        result = try_geocode(addr2)
        if result:
            return result

    addr3 = re.sub(r"\d+è™Ÿ.*", "", address)
    if addr3 != address:
        print(f"[geocode] å˜—è©¦ï¼ˆå»è™Ÿï¼‰ï¼š{addr3}")
        result = try_geocode(addr3)
        if result:
            return result

    addr4 = re.sub(r"\d+å¼„.*", "", address)
    if addr4 != address:
        print(f"[geocode] å˜—è©¦ï¼ˆå»å¼„ï¼‰ï¼š{addr4}")
        result = try_geocode(addr4)
        if result:
            return result

    addr5 = re.sub(r"\d+å··.*", "", address)
    if addr5 != address:
        print(f"[geocode] å˜—è©¦ï¼ˆå»å··ï¼‰ï¼š{addr5}")
        result = try_geocode(addr5)
        if result:
            return result

    m = re.match(
        r"(å°åŒ—å¸‚|è‡ºåŒ—å¸‚|æ–°åŒ—å¸‚|æ¡ƒåœ’å¸‚|è‡ºä¸­å¸‚|å°ä¸­å¸‚|è‡ºå—å¸‚|å°å—å¸‚|é«˜é›„å¸‚|"
        r"åŸºéš†å¸‚|æ–°ç«¹å¸‚|å˜‰ç¾©å¸‚|æ–°ç«¹ç¸£|è‹—æ —ç¸£|å½°åŒ–ç¸£|å—æŠ•ç¸£|é›²æ—ç¸£|å˜‰ç¾©ç¸£|"
        r"å±æ±ç¸£|å®œè˜­ç¸£|èŠ±è“®ç¸£|è‡ºæ±ç¸£|å°æ±ç¸£|æ¾æ¹–ç¸£|é‡‘é–€ç¸£|é€£æ±Ÿç¸£)"
        r"(.+?(å€|å¸‚|é®|é„‰))",
        address
    )
    if m:
        addr6 = m.group(1) + m.group(2)
        print(f"[geocode] å˜—è©¦ï¼ˆå¸‚+å€/é„‰/é®/å¸‚ï¼‰ï¼š{addr6}")
        result = try_geocode(addr6)
        if result:
            return result

    print(f"[geocode] å®Œå…¨æŸ¥ä¸åˆ°ï¼š{address}")
    return None
    
def find_nearby_points(lat, lon, max_km=5, top_k=5):
    points = load_xin_points()
    results = []

    for p in points:
        if p.get("lat") and p.get("lon"):
            d = haversine_km(lon, lat, p["lon"], p["lat"])
            if d <= max_km:
                results.append((p, d))

    results.sort(key=lambda x: x[1])
    return results[:top_k]

def load_all_units() -> List[Dict[str, Any]]:
    data = json.loads(UNITS_FILE.read_text("utf-8"))
    raw_units = data.get("units", [])

    units: List[Dict[str, Any]] = []

    for u in raw_units:
        u = dict(u)  

        section_title = u.get("section_title") or ""

        subtitle_texts = " ".join(
            seg.get("text", "") for seg in u.get("subtitles", []) or []
        )

        content_text = u.get("content_text", "") or ""

        search_text = " ".join(
            s for s in [
                section_title,
                u.get("title") or "",
                content_text,
                subtitle_texts,
            ] if s
        )

        u["_search_text"] = search_text

        units.append(u)

    print(f"[load] âœ… å…±è¼‰å…¥ {len(units)} å€‹å–®å…ƒï¼ˆå«å½±ç‰‡ + æ–‡ç« ï¼‰")
    return units

def extract_address_from_query(q: str) -> str:
    original = q

    if "é™„è¿‘" in q:
        q = q.split("é™„è¿‘")[0]

    for kw in ["å¿ƒæ“šé»", "é–€è¨º", "çœ‹è¨º"]:
        if kw in q:
            q = q.split(kw)[0]

    prefixes = ["æˆ‘ä½åœ¨", "æˆ‘ä½", "å®¶åœ¨", "å®¶ä½", "ä½åœ¨", "ä½", "åœ¨"]
    q = q.strip()
    for p in prefixes:
        if q.startswith(p):
            q = q[len(p):].strip()
            break

    tail_words = ["æœ‰æ²’æœ‰", "æœ‰å—", "å—", "å‘¢", "å•Š", "å•¦"]
    for t in tail_words:
        if q.endswith(t):
            q = q[: -len(t)].strip()

    q = q.strip(" ?ï¼Ÿ!")

    if len(q) < 4:
        return ""

    print(f"[debug] extract_address_from_query: '{original}' -> '{q}'")
    return q

def normalize_query(q: str):
    q = q.strip().lower()
    if not q: return [], []

    # --- 1. å®šç¾©ã€ŒåŠŸèƒ½æ€§/æŒ‡ä»¤é¡ã€è©å½™ï¼Œä¸æ‡‰è¨ˆå…¥æœå°‹è©•åˆ† ---
    # é€™äº›è©æ˜¯ç”¨ä¾†è¡¨é”æ„åœ–ï¼Œè€Œéä¸»é¡Œå…§å®¹ï¼Œå¿…é ˆéæ¿¾æ‰
    functional_words = [
        "æ–‡ç« ", "å½±ç‰‡", "æƒ³çœ‹", "çµ¦æˆ‘", "åªæœ‰", "åªæƒ³çœ‹", "æ¨è–¦", 
        "å½±éŸ³", "æ’­æ”¾", "æŸ¥è©¢", "æ‰¾", "æœ‰å“ªäº›", "ä»‹ç´¹"
    ]
    
    user_core_terms = []   # ä½¿ç”¨è€…è¼¸å…¥çš„å…§å®¹æ ¸å¿ƒè©
    expanded_terms = []    # å¾ JSON åˆ†é¡è¯æƒ³å‡ºçš„è©
    
    # --- 2. åµæ¸¬ä½¿ç”¨è€…è¼¸å…¥äº†å“ªäº›åˆ†é¡è© (åŸé‚è¼¯ä¿ç•™) ---
    for category, kws in KEYWORDS_DATA.items():
        found_in_q = [kw for kw in kws if kw in q]
        if found_in_q:
            user_core_terms.extend(found_in_q)
            expanded_terms.extend(kws)
    
    # --- 3. è™•ç†å‰©é¤˜è©å½™ä¸¦å‰”é™¤åŠŸèƒ½æ€§æŒ‡ä»¤èˆ‡åœç”¨è© ---
    # é€™è£¡çš„ re.split æœƒæ ¹æ“šæ¨™é»ç¬¦è™Ÿèˆ‡ç©ºæ ¼åˆ‡åˆ†å­—ä¸²
    parts = re.split(r"[ï¼Œã€‚ï¼!ï¼Ÿ?\sã€ï¼›;:ï¼š]+", q)
    for part in parts:
        # æ¢ä»¶ï¼šé•·åº¦å¤§æ–¼ç­‰æ–¼ 2ã€ä¸åœ¨ stop_words è£¡ã€ä¸”ä¸æ˜¯åŠŸèƒ½æ€§æŒ‡ä»¤è©
        if (len(part) >= 2 and 
            part not in STOP_WORDS and 
            part not in functional_words):
            
            if part not in user_core_terms:
                user_core_terms.append(part)

    # å»é‡ä¸¦ç¢ºä¿è¯æƒ³è©ä¸åŒ…å«å·²åœ¨æ ¸å¿ƒè©è£¡çš„
    expanded_terms = list(set(expanded_terms) - set(user_core_terms))
    
    return user_core_terms, expanded_terms

def score_unit(unit, user_core_terms, expanded_terms):
    text = unit.get("_search_text", "") or ""
    title = (unit.get("section_title") or "") + (unit.get("title") or "")
    if not text: return 0.0, None

    score = 0.0

    # A. è™•ç†ä½¿ç”¨è€…è¼¸å…¥çš„ã€é»ƒé‡‘æ ¸å¿ƒè©ã€‘
    for kw in user_core_terms:
        if kw in title:
            score += 6.0  # æ¨™é¡Œå‘½ä¸­åŠ æ¬Š
        
        # å…§æ–‡å‘½ä¸­æ¬¡æ•¸åŠ åˆ† (é™åˆ¶ä¸Šé™é¿å…æ´—åˆ†)
        text_count = text.count(kw)
        if text_count > 0:
            score += min(text_count, 5) * 4.0 # å…§æ–‡å‘½ä¸­åŠ æ¬Š

    # B. è™•ç†åˆ†é¡æ“´å±•çš„ã€è¼”åŠ©è¯æƒ³è©ã€‘
    for kw in expanded_terms:
        if kw in title:
            score += 3.0  # è¯æƒ³è©åœ¨æ¨™é¡Œï¼Œçµ¦ä¸€åŠåˆ†æ•¸
        if kw in text:
            score += 1.0  # è¯æƒ³è©åœ¨å…§æ–‡ï¼Œè¼•å¾®åŠ åˆ†

    # C. å½±ç‰‡å­—å¹•é€£çºŒæ€§åŠ åˆ† (å­—å¹•ç‰¹åˆ¥åŠ æˆ)
    best_seg = None
    best_seg_score = 0
    for seg in unit.get("subtitles", []):
        seg_text = seg.get("text", "")
        # è¨ˆç®—è©²æ®µè½å‘½ä¸­äº†å¤šå°‘æ ¸å¿ƒè©
        seg_hits = sum(1 for t in user_core_terms if t in seg_text)
        if seg_hits >= 2: # å¦‚æœä¸€æ®µè©±å‡ºç¾å…©å€‹ä»¥ä¸Šæ ¸å¿ƒè©
            score += 2.0
        
        if seg_hits > best_seg_score:
            best_seg_score = seg_hits
            best_seg = seg

    return score, best_seg

# æŠŠã€Œ(ä¸Š)/(ä¸‹)/(ï¼ˆä¸Šï¼‰)/(ï¼ˆä¸‹ï¼‰)/ä¸Šç¯‡/ä¸‹ç¯‡/ä¸Šé›†/ä¸‹é›†ã€è¦–ç‚ºé›†æ•¸æ¨™è¨˜ï¼ˆå¯å‡ºç¾åœ¨ä»»ä½•ä½ç½®ï¼‰
EP_TAG_RE = re.compile(r"(ï¼ˆä¸Šï¼‰|ï¼ˆä¸‹ï¼‰|\(ä¸Š\)|\(ä¸‹\)|ä¸Šç¯‡|ä¸‹ç¯‡|ä¸Šé›†|ä¸‹é›†)")

def get_episode_tag(title: str) -> Optional[str]:
    """å›å‚³ 'ä¸Š' / 'ä¸‹' / Noneï¼ˆä¸é™å‡ºç¾åœ¨çµå°¾ï¼‰"""
    if not title:
        return None
    t = title.strip()
    if re.search(r"(ï¼ˆä¸Šï¼‰|\(ä¸Š\)|ä¸Šç¯‡|ä¸Šé›†)", t):
        return "ä¸Š"
    if re.search(r"(ï¼ˆä¸‹ï¼‰|\(ä¸‹\)|ä¸‹ç¯‡|ä¸‹é›†)", t):
        return "ä¸‹"
    return None

def get_base_key(section_title: str, title: str) -> str:
    """
    ç”¨ä¾†æŠŠã€Œä¸Š/ä¸‹ã€è¦–ç‚ºåŒä¸€çµ„çš„ key
    - ç§»é™¤æ¨™é¡Œä¸­çš„ä¸Š/ä¸‹æ¨™è¨˜ï¼ˆä¸é™ä½ç½®ï¼‰
    - å†ç”¨ section_title + æ¸…ç†å¾Œ title ç•¶ key
    """
    s = (section_title or "").strip()
    t = (title or "").strip()
    t2 = EP_TAG_RE.sub("", t)  # âœ… ä¸é™çµå°¾ï¼Œç›´æ¥æŠŠ(ä¸Š)/(ä¸‹)ç§»é™¤
    t2 = re.sub(r"\s+", "", t2)  # å¯é¸ï¼šå»ç©ºç™½ï¼Œè®“ key æ›´ç©©
    s2 = re.sub(r"\s+", "", s)
    return f"{s2}||{t2}"


def reorder_episode_pairs(results: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    """
    ç›®æ¨™ï¼š
    - åŒä¸€ç³»åˆ—ï¼ˆåŒ base_keyï¼‰çš„ä¸Š/ä¸‹è¦é»åœ¨ä¸€èµ·ï¼šä¸Š â†’ ä¸‹
    - ç³»åˆ—èˆ‡ç³»åˆ—ä¹‹é–“ï¼Œç”¨è©²ç³»åˆ—çš„ä»£è¡¨åˆ†æ•¸æ’åºï¼ˆé è¨­å–ç³»åˆ—å…§æœ€é«˜ _scoreï¼‰
    - æ²’æœ‰ä¸Š/ä¸‹æ¨™è¨˜çš„å–®ç¯‡ï¼Œè¦–ç‚ºä¸€å€‹ç¨ç«‹ç³»åˆ—ï¼Œä¿ç•™åŸæœ¬é …ç›®æœ¬èº«
    """

    # 1) å»ºç«‹ç¾¤çµ„ï¼škey -> {"items": [...], "best_score": float, "first_idx": int}
    groups: Dict[str, Dict[str, Any]] = {}

    for idx, r in enumerate(results):
        key = get_base_key(r.get("section_title"), r.get("title"))
        score = float(r.get("_score", 0.0))

        g = groups.get(key)
        if g is None:
            groups[key] = {
                "items": [],
                "best_score": score,
                "first_idx": idx,  # è‹¥åˆ†æ•¸ä¸€æ¨£ï¼Œç”¨æœ€æ—©å‡ºç¾é †åºç•¶ tie-break
            }
            g = groups[key]

        g["items"].append(r)
        if score > g["best_score"]:
            g["best_score"] = score

    # 2) æ¯çµ„å…§ï¼šä¸Šâ†’ä¸‹â†’å…¶ä»–ï¼ˆå¦‚æœæœ‰å¥‡æ€ªçš„æ²’æ¨™è¨˜ï¼‰
    def item_rank(r: Dict[str, Any]) -> int:
        tag = get_episode_tag(r.get("title") or "")
        if tag == "ä¸Š":
            return 0
        if tag == "ä¸‹":
            return 1
        return 2

    for g in groups.values():
        # åŒä¸€çµ„å¯èƒ½æœ‰å¤šå€‹ä¸Šæˆ–å¤šå€‹ä¸‹ï¼šåŒ rank å…§å†ç”¨åˆ†æ•¸é«˜çš„æ’å‰
        g["items"].sort(key=lambda r: (item_rank(r), -float(r.get("_score", 0.0))))

    # 3) çµ„èˆ‡çµ„ä¹‹é–“æ’åºï¼šåˆ†æ•¸é«˜çš„çµ„æ’å‰ï¼›åˆ†æ•¸åŒå‰‡ç”¨ first_idx ä¿æŒç©©å®š
    ordered_groups = sorted(
        groups.values(),
        key=lambda g: (-g["best_score"], g["first_idx"])
    )

    # 4) æ”¤å¹³æˆä¸€æ¢ list
    out: List[Dict[str, Any]] = []
    for g in ordered_groups:
        out.extend(g["items"])

    return out

def format_time(seconds: float) -> str:
    s = int(seconds)
    h = s // 3600
    m = (s % 3600) // 60
    sec = s % 60
    if h > 0:
        return f"{h:02d}:{m:02d}:{sec:02d}"
    return f"{m:02d}:{sec:02d}"

def search_units(units: List[Dict[str, Any]], query: str, top_k: int = TOP_K):
    user_core, expanded = normalize_query(query)
    if not user_core: return []

    results = []
    for u in units:
        score, best_seg = score_unit(u, user_core, expanded)
        if score > 0:
            r = dict(u)
            r["_score"] = score
            r["_best_segment"] = best_seg
            results.append(r)

    results.sort(key=lambda x: x["_score"], reverse=True)
    return results

def detect_media_preference(q: str) -> Optional[str]:
    """
    åµæ¸¬ä½¿ç”¨è€…æ˜¯å¦æŒ‡å®šæƒ³çœ‹ã€æ–‡ç« ã€æˆ–ã€å½±ç‰‡ã€
    """
    if any(w in q for w in ["æƒ³çœ‹æ–‡ç« ", "çµ¦æˆ‘æ–‡ç« ", "åªæœ‰æ–‡ç« ", "æ–‡ç« æ¨è–¦"]):
        return "article"
    if any(w in q for w in ["æƒ³çœ‹å½±ç‰‡", "çµ¦æˆ‘å½±ç‰‡", "æ’­æ”¾å½±ç‰‡", "å½±éŸ³", "youtube"]):
        return "video"
    return None

# ---------- äº’å‹•ä¸»è¿´åœˆ ----------
app = FastAPI(title="å¿ƒå¿«æ´»èª²ç¨‹æ¨è–¦ API")

# è‹¥å‰ç«¯ç¶²é æœƒè·¨ç¶²åŸŸå‘¼å«ï¼Œå¯é–‹ CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # ä¸Šç·šæ™‚å»ºè­°æ”¹æˆä½ çš„ç¶²åŸŸ
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# â–¼â–¼ æ–°å¢ï¼šè®“ /static åº•ä¸‹å¯ä»¥ç›´æ¥æŠ“æª”æ¡ˆ â–¼â–¼
app.mount("/static", StaticFiles(directory="static"), name="static")
@app.get("/", include_in_schema=False)
def serve_index():
    return FileResponse("static/index.html")

# å•Ÿå‹•æ™‚å°±å…ˆè¼‰å…¥æ‰€æœ‰å–®å…ƒ
UNITS_CACHE: List[Dict[str, Any]] = load_all_units()

# ç°¡å–®æ”¾åœ¨è¨˜æ†¶é«”çš„èŠå¤©ç´€éŒ„ï¼ˆserver é‡å•Ÿæœƒæ¸…ç©ºï¼‰
HISTORY: Dict[str, List[Dict[str, Any]] ] = {}


class ChatRequest(BaseModel):
    query: str
    session_id: Optional[str] = None


class NearbyRequest(BaseModel):
    address: str


class RecommendRequest(BaseModel):
    query: str


@app.get("/reload_keywords")
def reload_keywords():
    """æ‰‹å‹•è§¸ç™¼é‡æ–°è¼‰å…¥é—œéµå­—è¨­å®š"""
    load_keywords_from_json()
    return {
        "status": "success", 
        "mental_keywords_count": len(MENTAL_KEYWORDS),
        "stop_words_count": len(STOP_WORDS)
    }

@app.get("/ping")
def ping():
    return {"status": "ok"}


@app.post("/chat")
def chat(req: ChatRequest):
    q = req.query.strip()
    session_id = req.session_id or "anonymous"
    resp: Dict[str, Any]
    
    print(f">>> [/chat] session_id: {session_id} | query: {q}")

    # --- å…§éƒ¨è¼”åŠ©å‡½å¼ ---
    def detect_media_preference(text: str) -> Optional[str]:
        """åµæ¸¬ä½¿ç”¨è€…æ˜¯å¦æŒ‡å®šç‰¹å®šåª’é«”é¡å‹"""
        if any(w in text for w in ["æƒ³çœ‹æ–‡ç« ", "çµ¦æˆ‘æ–‡ç« ", "åªæœ‰æ–‡ç« ", "æ–‡ç« æ¨è–¦", "æ‰¾æ–‡ç« ", "åªæƒ³çœ‹æ–‡ç« "]):
            return "article"
        if any(w in text for w in ["æƒ³çœ‹å½±ç‰‡", "çµ¦æˆ‘å½±ç‰‡", "æ’­æ”¾å½±ç‰‡", "å½±éŸ³", "çœ‹å½±ç‰‡", "youtube", "åªæƒ³çœ‹å½±ç‰‡"]):
            return "video"
        return None

    # 1. è™•ç†ã€Œå¿ƒæ“šé»ã€/ã€Œçœ‹è¨ºã€åœ°å€æœå°‹é‚è¼¯
    if ("é™„è¿‘" in q) and ("å¿ƒæ“šé»" in q or "çœ‹è¨º" in q or "é–€è¨º" in q):
        addr = extract_address_from_query(q)
        if not addr:
            resp = {
                "type": "xin_points",
                "address": None,
                "points": [],
                "message": "æˆ‘æœ‰é»æŠ“ä¸åˆ°åœ°å€ï¼Œè«‹å˜—è©¦è¼¸å…¥å®Œæ•´åœ°å€ï¼Œä¾‹å¦‚ï¼šå°å—å¸‚æ±å€å¤§å­¸è·¯1è™Ÿ"
            }
        else:
            geo = geocode_address(addr)
            if not geo:
                resp = {
                    "type": "xin_points", "address": addr, "points": [],
                    "message": f"æŸ¥ä¸åˆ°ã€Œ{addr}ã€é€™å€‹åœ°å€ï¼Œè«‹æ”¹æˆæ›´æ­£å¼çš„å¯«æ³•è©¦è©¦çœ‹"
                }
            else:
                lat, lon = geo
                results = find_nearby_points(lat, lon, max_km=5, top_k=TOP_K)
                resp = build_nearby_points_response(addr, results)

    # 2. ç›´æ¥è¼¸å…¥å®Œæ•´åœ°å€
    elif ADDR_HEAD_RE.match(q):
        addr = q
        geo = geocode_address(addr)
        if not geo:
            resp = {
                "type": "xin_points", "address": addr, "points": [],
                "message": f"æŸ¥ä¸åˆ°ã€Œ{addr}ã€é€™å€‹åœ°å€ï¼Œè«‹æ”¹æˆæ›´æ­£å¼çš„å¯«æ³•è©¦è©¦çœ‹"
            }
        else:
            lat, lon = geo
            results = find_nearby_points(lat, lon, max_km=5, top_k=TOP_K)
            resp = build_nearby_points_response(addr, results)

    # 3. è™•ç†ã€Œä¸‹ä¸€é ã€åˆ†é é‚è¼¯
    elif detect_pagination_intent(q):
        history = HISTORY.get(session_id, [])
        last = next(
            (h for h in reversed(history) if h["response"].get("type") == "course_recommendation"),
            None
        )
        if not last:
            resp = {"type": "text", "message": "ç›®å‰æ²’æœ‰ä¸Šä¸€ç­†æ¨è–¦çµæœï¼Œå¯ä»¥å…ˆå•ä¸€å€‹å•é¡Œ ğŸ˜Š"}
        else:
            prev = last["response"]
            new_offset = prev["offset"] + prev["limit"]
            full_results = search_units(UNITS_CACHE, prev["query"], top_k=9999)
            resp = build_recommendations_response(prev["query"], full_results, offset=new_offset, limit=TOP_K)

    # 4. ç‰¹å®šæƒ…å¢ƒå»ºè­°èˆ‡ä¸€èˆ¬èª²ç¨‹æœå°‹
    else:
        special_intent = detect_special_intent(q)
        if special_intent:
            resp = build_special_intent_response(special_intent, q)
        else:
            # --- A. åµæ¸¬åª’é«”åå¥½ ---
            media_pref = detect_media_preference(q)
            
            # --- B. å»¶çºŒä¸Šæ–‡é‚è¼¯ ---
            # å¦‚æœä½¿ç”¨è€…åªè¼¸å…¥ã€Œæˆ‘æƒ³çœ‹æ–‡ç« ã€ï¼Œè£¡é¢æ²’é—œéµå­—ï¼Œå°±å»æŠ“æ­·å²ç´€éŒ„çš„ä¸»é¡Œ
            search_q = q
            user_core, _ = normalize_query(q)
            
            if not user_core and media_pref:
                history = HISTORY.get(session_id, [])
                last_rec = next((h for h in reversed(history) if h["response"].get("type") == "course_recommendation"), None)
                if last_rec:
                    search_q = last_rec["query"]
                    print(f"[chat] å»¶çºŒä¸Šæ–‡ä¸»é¡Œ: {search_q}")

            # --- C. åŸ·è¡Œæœå°‹èˆ‡éæ¿¾ ---
            full_results = search_units(UNITS_CACHE, search_q, top_k=9999)
            
            if media_pref == "article":
                full_results = [r for r in full_results if r.get("is_article")]
            elif media_pref == "video":
                full_results = [r for r in full_results if not r.get("is_article")]
            
            resp = build_recommendations_response(search_q, full_results, offset=0, limit=TOP_K)
            
            # å¦‚æœæœå°‹çµæœç‚ºç©ºä¸”æœ‰åª’é«”åå¥½ï¼Œçµ¦äºˆæç¤º
            if media_pref and not resp["results"]:
                type_name = "æ–‡ç« " if media_pref == "article" else "å½±ç‰‡"
                resp["message"] = f"é—œæ–¼ã€Œ{search_q}ã€ç›®å‰æ²’æœ‰ç›¸é—œçš„{type_name}ï¼Œå»ºè­°æ‚¨å˜—è©¦å¦ä¸€ç¨®åª’é«”é¡å‹ã€‚"

    # --- è¨˜éŒ„æ­·å²ç´€éŒ„ ---
    history_list = HISTORY.setdefault(session_id, [])
    history_list.append({"query": q, "response": resp})
    if len(history_list) > 50:
        history_list.pop(0)

    return resp

@app.get("/history")
def get_history(session_id: str):
    """
    ä¾ session_id å›å‚³å°ˆå±¬èŠå¤©ç´€éŒ„ã€‚
    /history?session_id=xxxx
    """
    print(">>> /history called session_id =", session_id)
    items = HISTORY.get(session_id, [])
    print(">>> HISTORY keys =", list(HISTORY.keys()))
    return {
        "items": items
    }

@app.post("/nearby")
def nearby(req: NearbyRequest):
    addr = req.address.strip()
    if not addr:
        return {
            "type": "xin_points",
            "address": None,
            "points": [],
            "message": "è«‹æä¾›å®Œæ•´åœ°å€ï¼Œä¾‹å¦‚ï¼šå°å—å¸‚æ±å€å¤§å­¸è·¯1è™Ÿ"
        }

    geo = geocode_address(addr)
    if not geo:
        return {
            "type": "xin_points",
            "address": addr,
            "points": [],
            "message": f"æŸ¥ä¸åˆ°ã€Œ{addr}ã€é€™å€‹åœ°å€ï¼Œè«‹æ”¹æˆæ›´æ­£å¼çš„å¯«æ³•è©¦è©¦çœ‹"
        }

    lat, lon = geo
    results = find_nearby_points(lat, lon, max_km=5, top_k=TOP_K)
    return build_nearby_points_response(addr, results)


@app.post("/recommend")
def recommend(req: RecommendRequest):
    q = req.query.strip()
    full_results = search_units(UNITS_CACHE, q, top_k=9999)
    resp = build_recommendations_response(q, full_results, offset=0, limit=TOP_K)
    return resp

if __name__ == "__main__":
    import uvicorn
    uvicorn.run("xin_api:app", host="0.0.0.0", port=8000, reload=True)