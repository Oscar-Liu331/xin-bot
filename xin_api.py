import json
import re
from pathlib import Path
from typing import List, Dict, Any

import requests
from math import radians, sin, cos, asin, sqrt

from fastapi import FastAPI
from pydantic import BaseModel
from fastapi.middleware.cors import CORSMiddleware

from fastapi.staticfiles import StaticFiles
from fastapi.responses import FileResponse

from typing import Optional


CITY_PATTERN = (
    r"(å°åŒ—å¸‚|è‡ºåŒ—å¸‚|æ–°åŒ—å¸‚|æ¡ƒåœ’å¸‚|è‡ºä¸­å¸‚|å°ä¸­å¸‚|è‡ºå—å¸‚|å°å—å¸‚|é«˜é›„å¸‚|"
    r"åŸºéš†å¸‚|æ–°ç«¹å¸‚|å˜‰ç¾©å¸‚|æ–°ç«¹ç¸£|è‹—æ —ç¸£|å½°åŒ–ç¸£|å—æŠ•ç¸£|é›²æ—ç¸£|å˜‰ç¾©ç¸£|"
    r"å±æ±ç¸£|å®œè˜­ç¸£|èŠ±è“®ç¸£|è‡ºæ±ç¸£|å°æ±ç¸£|æ¾æ¹–ç¸£|é‡‘é–€ç¸£|é€£æ±Ÿç¸£)"
)

ADDR_HEAD_RE = re.compile(rf"^{CITY_PATTERN}(.*?(å€|é„‰|é®|å¸‚))")

SECTION = "pro"
DATASET_PATTERN = f"elearn_{SECTION}_*_*_dataset.json"
TOP_K = 5  

XIN_POINTS_FILE = Path("xin_points.json")
UNITS_FILE = Path("wellbeing_elearn_pro_all_with_articles.json")

# --- é—œéµå­—è¼‰å…¥é‚è¼¯ ---
KEYWORDS_FILE = Path("keywords.json")
# --- å…¨åŸŸè®Šæ•¸ ---
KEYWORDS_DATA = {} # å­˜æ”¾åŸå§‹åˆ†é¡çµæ§‹
MENTAL_KEYWORDS = [] # æ‰å¹³åŒ–å¾Œçš„æ¸…å–®ï¼Œä¾›åŸæœå°‹é‚è¼¯ä½¿ç”¨
STOP_WORDS = []

def load_keywords_from_json():
    global KEYWORDS_DATA, MENTAL_KEYWORDS, STOP_WORDS
    try:
        if KEYWORDS_FILE.exists():
            with open(KEYWORDS_FILE, "r", encoding="utf-8") as f:
                data = json.load(f)
                KEYWORDS_DATA = data.get("mental_keywords", {})
                
                # å°‡æ‰€æœ‰åˆ†é¡çš„é—œéµå­—æ”¤å¹³æˆä¸€å€‹æ¸…å–®ï¼Œç›¸å®¹èˆŠæœ‰çš„æœå°‹é‚è¼¯
                all_kws = []
                for category_list in KEYWORDS_DATA.values():
                    all_kws.extend(category_list)
                
                # å»é™¤é‡è¤‡é …
                MENTAL_KEYWORDS = list(set(all_kws))
                STOP_WORDS = data.get("stop_words", [])
                
            print(f"[load] âœ… åˆ†é¡è¼‰å…¥æˆåŠŸã€‚å…± {len(KEYWORDS_DATA)} å€‹é¡åˆ¥ï¼Œ{len(MENTAL_KEYWORDS)} å€‹é—œéµå­—ã€‚")
    except Exception as e:
        print(f"[load] âŒ åˆ†é¡è¼‰å…¥å¤±æ•—: {e}")

# åˆå§‹è¼‰å…¥
load_keywords_from_json()

def detect_pagination_intent(q: str) -> bool:
    return any(w in q for w in ["çµ¦æˆ‘å¾Œäº”å€‹","çµ¦æˆ‘ä¸‹äº”å€‹","å¾Œäº”å€‹","ä¸‹äº”å€‹","ä¸‹ä¸€é ","æ›´å¤šæ¨è–¦"])

def check_category_intent(text: str, category_name: str) -> bool:
    """å·¥å…·å‡½å¼ï¼šæª¢æŸ¥æ–‡å­—ä¸­æ˜¯å¦åŒ…å«ç‰¹å®šåˆ†é¡çš„é—œéµå­—"""
    keywords = KEYWORDS_DATA.get(category_name, [])
    return any(kw in text for kw in keywords)

def detect_special_intent(q: str) -> Optional[str]:
    """
    ç²¾ç¢ºåµæ¸¬å››å¤§ç‰¹å®šå•é¡Œï¼Œä½¿ç”¨ç´°åˆ†å¾Œçš„ JSON é¡åˆ¥
    """
    text = re.sub(r"\s+", "", q).lower()
    
    def has_cat(category_name):
        return any(kw in text for kw in KEYWORDS_DATA.get(category_name, []))

    # ---------- 1. æ†‚é¬±å°±é†«å»ºè­° ----------
    # é‚è¼¯ï¼š(æ†‚é¬±é¡ æˆ– å‹•åŠ›é¡) + (å°±é†«é—œéµå­— æˆ– ç–‘å•å¥)
    if has_cat("depressive_mood") or has_cat("low_motivation"):
        if any(w in text for w in ["é†«å¸«", "é†«ç”Ÿ", "å¿ƒç†å¸«", "èº«å¿ƒç§‘", "è©²ä¸è©²", "è¦ä¸è¦", "çœ‹è¨º"]):
            return "depression_go_doctor"

    # ---------- 2. é•·è¼©å¤±æ™ºç¢ºèªèˆ‡çœ‹è¨º ----------
    # é‚è¼¯ï¼šç›´æ¥å‘½ä¸­ã€Œå¤±æ™ºã€ + (é•·è¼©é¡ æˆ– è©¢å•ç§‘åˆ¥/ç¢ºèª)
    if "å¤±æ™º" in text:
        if has_cat("family_elder") or any(w in text for w in ["å“ªä¸€ç§‘", "ç¢ºå®š", "æª¢æŸ¥", "è¨ºæ–·"]):
            return "dementia_parent"

    # ---------- 3. å­©å­æ‰‹æ©Ÿå•é¡Œ ----------
    # é‚è¼¯ï¼š(å­©å­é¡) + (æ•¸ä½æˆç™®é¡)
    if has_cat("child_teen") and has_cat("digital_addiction"):
        return "child_phone"

    # ---------- 4. å©†å©†æ•™é¤Šè§€å¿µè¡çª ----------
    # é‚è¼¯ï¼š(å©†åª³é¡) + (æ•™é¤Šè¡çªé¡ æˆ– ç…§é¡§è¡Œç‚º)
    if has_cat("in_laws"):
        if has_cat("parenting_conflict") or any(w in text for w in ["ç…§é¡§", "å¾ˆä¸ä¸€æ¨£", "å·®ç•°"]):
            return "mother_in_law_childcare"

    return None

def build_special_intent_response(intent: str, q: str) -> Dict[str, Any]:
    """
    æŠŠå››ç¨®æƒ…å¢ƒçš„å»ºè­°ï¼ŒåŒ…æˆçµæ§‹åŒ– JSONï¼Œçµ¦å‰ç«¯æ¸²æŸ“ã€‚
    """
    if intent == "depression_go_doctor":
        title = "è¦ºå¾—è‡ªå·±æœ‰é»æ†‚é¬±ï¼Œè©²ä¸è©²å»çœ‹å¿ƒç†é†«å¸«ï¼Ÿ"
        sections = [
            {
                "title": "1ï¸âƒ£ ä»€éº¼æƒ…æ³æ¯”è¼ƒå»ºè­°æ‰¾å°ˆæ¥­é†«å¸«æˆ–å¿ƒç†å¸«ï¼Ÿ",
                "items": [
                    "æƒ…ç·’ä½è½ã€æ²’å‹•åŠ›ã€å®¹æ˜“æƒ³å“­ï¼ŒæŒçºŒè¶…é 2 é€±ä»¥ä¸Šã€‚",
                    "å½±éŸ¿åˆ°å·¥ä½œã€èª²æ¥­ã€ç¡çœ ã€é£Ÿæ…¾æˆ–äººéš›é—œä¿‚ã€‚",
                    "å‡ºç¾ã€Œä¸å¦‚æ¶ˆå¤±ç®—äº†ã€ã€ã€Œæ´»è‘—å¥½ç´¯ã€é€™é¡è² é¢æˆ–è‡ªå‚·çš„æƒ³æ³•ã€‚"
                ]
            },
            {
                "title": "2ï¸âƒ£ å¯ä»¥çœ‹çš„ç§‘åˆ¥ / å°ˆæ¥­",
                "items": [
                    "é†«é™¢çš„ã€Œèº«å¿ƒç§‘ / ç²¾ç¥ç§‘ã€é–€è¨ºï¼Œå¯ä»¥è©•ä¼°æ˜¯å¦éœ€è¦ç”¨è—¥æˆ–é€²ä¸€æ­¥æª¢æŸ¥ã€‚",
                    "é†«é™¢æˆ–ç¤¾å€çš„ã€Œè‡¨åºŠå¿ƒç†å¸«ã€è«®å•†å¿ƒç†å¸«ã€åšå¿ƒç†è«®å•†ã€‚",
                    "å¦‚æœé‚„ä¸ç¢ºå®šï¼Œä¹Ÿå¯ä»¥å…ˆå¾ã€Œå®¶é†«ç§‘ã€æˆ–ç¤¾å€å¿ƒç†è¡›ç”Ÿä¸­å¿ƒè«®è©¢é–‹å§‹ã€‚"
                ]
            },
            {
                "title": "3ï¸âƒ£ å¦‚æœç¾åœ¨é‚„å‹‰å¼·æ’å¾—ä½ï¼Œå¯ä»¥å…ˆå˜—è©¦çš„èª¿æ•´",
                "items": [
                    "å›ºå®šç¡è¦ºã€èµ·åºŠæ™‚é–“ï¼Œç›¡é‡å°‘ç†¬å¤œã€‚",
                    "æ‰¾ä¸€å€‹ä¿¡ä»»çš„äººèŠèŠï¼ŒæŠŠå£“åŠ›èªªå‡ºä¾†ã€‚",
                    "å…ˆå¾çŸ­æ™‚é–“æ•£æ­¥æˆ–ç°¡å–®é‹å‹•é–‹å§‹ï¼Œè®“èº«é«”å‹•èµ·ä¾†ã€‚"
                ]
            },
            {
                "title": "âš ï¸ ä»€éº¼æƒ…æ³è¦ç«‹åˆ»å°‹æ±‚å”åŠ©ï¼Ÿ",
                "items": [
                    "æœ‰æ˜é¡¯çš„è‡ªæ®ºå¿µé ­ã€è¡å‹•ï¼Œæˆ–å·²ç¶“æƒ³å¥½æ–¹æ³•ã€‚",
                    "æ­¤æ™‚è«‹å„˜å¿«åˆ°é†«é™¢æ€¥è¨ºï¼Œæˆ–è«‹å®¶äººæœ‹å‹é™ªåŒå°±é†«ï¼Œä¸¦å¯è¯çµ¡è‡ªæ®ºé˜²æ²» / å¿ƒç†æ”¯æŒå°ˆç·šã€‚"
                ]
            }
        ]

    elif intent == "dementia_parent":
        title = "æ“”å¿ƒçˆ¸çˆ¸ / å®¶äººå¯èƒ½æœ‰å¤±æ™ºï¼Œæ€éº¼ç¢ºå®šï¼Ÿçœ‹å“ªä¸€ç§‘ï¼Ÿ"
        sections = [
            {
                "title": "1ï¸âƒ£ å¸¸è¦‹çš„å¤±æ™ºè­¦è¨Šï¼ˆèˆ‰ä¾‹ï¼‰",
                "items": [
                    "è¨˜æ†¶åŠ›æ˜é¡¯è®Šå·®ï¼šåŒä¸€ä»¶äº‹å•å¾ˆå¤šæ¬¡ï¼Œå¿˜è¨˜å‰›ç™¼ç”Ÿçš„äº‹æƒ…ã€‚",
                    "å®¹æ˜“è¿·è·¯ï¼šåœ¨ç†Ÿæ‚‰çš„ç’°å¢ƒåè€Œæœƒèµ°éŒ¯ã€æ‰¾ä¸åˆ°è·¯ã€‚",
                    "åˆ¤æ–·åŠ›è®Šå·®ï¼šä¾‹å¦‚å®¹æ˜“è¢«è©é¨™ã€åšä¸€äº›ä»¥å‰ä¸æœƒåšçš„æ€ªæ±ºå®šã€‚",
                    "æ€§æ ¼æˆ–è¡Œç‚ºæ”¹è®Šï¼šè®Šå¾—æ˜é¡¯æš´èºã€é€€ç¸®ï¼Œæˆ–è·Ÿä»¥å‰å€‹æ€§å·®å¾ˆå¤šã€‚"
                ]
            },
            {
                "title": "2ï¸âƒ£ è¦æ€éº¼æ¯”è¼ƒç¢ºå®šæ˜¯ä¸æ˜¯å¤±æ™ºï¼Ÿ",
                "items": [
                    "éœ€è¦ç”±é†«å¸«åšå®Œæ•´è©•ä¼°ï¼Œå¯èƒ½åŒ…æ‹¬å•è¨ºã€ç¥ç¶“å¿ƒç†é‡è¡¨ã€è¡€æ¶²æª¢æŸ¥ã€å½±åƒæª¢æŸ¥ç­‰ã€‚",
                    "å®¶å±¬å¯ä»¥å…ˆæ•´ç†æœ€è¿‘è§€å¯Ÿåˆ°çš„æ”¹è®Šï¼ˆå¾ä»€éº¼æ™‚å€™é–‹å§‹ã€ç™¼ç”Ÿåœ¨ä»€éº¼æƒ…å¢ƒï¼‰ã€‚"
                ]
            },
            {
                "title": "3ï¸âƒ£ å»ºè­°å…ˆçœ‹å“ªä¸€ç§‘ï¼Ÿ",
                "items": [
                    "å¤§å‹é†«é™¢å¸¸è¦‹ç§‘åˆ¥ï¼šç¥ç¶“å…§ç§‘ã€å®¶é†«ç§‘ã€è€å¹´é†«å­¸ç§‘ï¼Œéƒ¨åˆ†é†«é™¢æœ‰ã€Œå¤±æ™ºå…±åŒç…§è­·é–€è¨ºã€ã€‚",
                    "è‹¥é•·è¼©æœ‰æ˜é¡¯æƒ…ç·’æˆ–è¡Œç‚ºæ”¹è®Šï¼ˆå¦‚å¦„æƒ³ã€å¹»è¦ºã€åš´é‡ç„¦æ…®ï¼‰ï¼Œèº«å¿ƒç§‘ / ç²¾ç¥ç§‘ä¹Ÿèƒ½å”åŠ©è©•ä¼°ã€‚"
                ]
            },
            {
                "title": "4ï¸âƒ£ å¸¶é•·è¼©çœ‹è¨ºçš„å°æŠ€å·§",
                "items": [
                    "å¯ä»¥ç”¨ã€Œåšå¥åº·æª¢æŸ¥ã€çš„æ–¹å¼é‚€è«‹ï¼Œè€Œä¸æ˜¯ç›´æ¥èªªã€Œæ‡·ç–‘ä½ å¤±æ™ºã€ã€‚",
                    "çœ‹è¨ºæ™‚ï¼Œå®¶å±¬å¯æŠŠåœ¨å®¶è§€å¯Ÿåˆ°çš„ç‹€æ³æ•´ç†åœ¨ç´™ä¸Šçµ¦é†«å¸«çœ‹ï¼Œæ¯”è¼ƒä¸æœƒæ¼è¬›ã€‚"
                ]
            }
        ]

    elif intent == "child_phone":
        title = "åœ‹å°å­©å­æ‰‹æ©Ÿè¶Šç©è¶Šå…‡ï¼Œæˆ‘è©²æ€éº¼è¾¦ï¼Ÿ"
        sections = [
            {
                "title": "1ï¸âƒ£ å…ˆäº†è§£ã€Œæ€éº¼ç©ã€è€Œä¸åªæ˜¯ã€Œç©å¤šä¹…ã€",
                "items": [
                    "ä¸»è¦æ˜¯åœ¨ç©éŠæˆ²ã€çœ‹å½±ç‰‡ï¼Œé‚„æ˜¯è·ŸåŒå­¸èŠå¤©ï¼Ÿ",
                    "é€šå¸¸åœ¨ä»€éº¼æ™‚é–“é»ç©ï¼šå¯«åŠŸèª²å‰ï¼Ÿç¡å‰ï¼Ÿå‡æ—¥æ•´å¤©ï¼Ÿ"
                ]
            },
            {
                "title": "2ï¸âƒ£ è·Ÿå­©å­ä¸€èµ·ã€Œè«‡è¦å‰‡ã€ï¼Œä¸æ˜¯åªä¸‹å‘½ä»¤",
                "items": [
                    "ä¾‹å¦‚ï¼šå¹³æ—¥æ¯å¤©å¯ä»¥ç© 30â€“60 åˆ†é˜ï¼Œå…ˆå¯«å®Œä½œæ¥­ã€æ´—æ¾¡å†é–‹æ©Ÿã€‚",
                    "é¿å…ç¡å‰ 1 å°æ™‚ç”¨æ‰‹æ©Ÿï¼Œè®“å¤§è…¦æœ‰æ™‚é–“ã€Œé™é€Ÿã€æ¯”è¼ƒå¥½ç¡ã€‚",
                    "æŠŠè¦å‰‡å¯«åœ¨ç´™ä¸Šè²¼å‡ºä¾†ï¼Œæ¸›å°‘è‡¨æ™‚åµæ¶ã€‚"
                ]
            },
            {
                "title": "3ï¸âƒ£ æä¾›æ›¿ä»£æ´»å‹•ï¼Œä¸æ˜¯åªæœ‰ã€Œä¸å‡†ç©ã€",
                "items": [
                    "å®‰æ’å¯ä»¥ä¸€èµ·åšçš„äº‹ï¼šæ¡ŒéŠã€é‹å‹•ã€æ•£æ­¥ã€ç•«ç•«ã€åšæ–™ç†ã€‚",
                    "å‡æ—¥ç´„å®šã€Œç„¡æ‰‹æ©Ÿæ™‚æ®µã€ï¼Œå…¨å®¶ä¸€èµ·åšåˆ¥çš„æ´»å‹•ã€‚"
                ]
            },
            {
                "title": "4ï¸âƒ£ å¤§äººä¹Ÿè¦ç•¶æ¦œæ¨£",
                "items": [
                    "å¦‚æœçˆ¶æ¯ä¸€ç›´æ»‘æ‰‹æ©Ÿï¼Œå°å­©å¾ˆé›£æ¥å—ã€Œä½ ä¸å¯ä»¥æ»‘ã€ã€‚",
                    "å¯ä»¥ä¸€èµ·ç´„å®šï¼šåƒé£¯ã€ç¡å‰åŠå°æ™‚ï¼Œå…¨å®¶éƒ½ä¸çœ‹æ‰‹æ©Ÿã€‚"
                ]
            },
            {
                "title": "5ï¸âƒ£ ä»€éº¼æ™‚å€™éœ€è¦å°ˆæ¥­å”åŠ©ï¼Ÿ",
                "items": [
                    "å·²ç¶“å½±éŸ¿åˆ°å­¸æ¥­ã€ç¡çœ ï¼Œæˆ–ç‚ºäº†æ‰‹æ©Ÿæœƒå¤§åµå¤§é¬§ã€æ‘”æ±è¥¿ã€‚",
                    "å¯ä»¥è€ƒæ…®è«®è©¢ï¼šå­¸æ ¡è¼”å°è€å¸«ã€å…’ç«¥é’å°‘å¹´èº«å¿ƒç§‘ã€å…’ç«¥è‡¨åºŠ / è«®å•†å¿ƒç†å¸«ã€‚"
                ]
            }
        ]

    elif intent == "mother_in_law_childcare":
        title = "å©†å©†ç…§é¡§å°å­©æ–¹å¼è·Ÿæˆ‘å¾ˆä¸ä¸€æ¨£ï¼Œæˆ‘è©²æ€éº¼è¾¦ï¼Ÿ"
        sections = [
            {
                "title": "1ï¸âƒ£ å…ˆåˆ†è¾¨ï¼šæ˜¯ã€åšæ³•ä¸åŒã€é‚„æ˜¯ã€å®‰å…¨æœ‰ç–‘æ…®ã€",
                "items": [
                    "åšæ³•ä¸åŒä½†å®‰å…¨ï¼šä¾‹å¦‚é›¶é£Ÿå¤šä¸€é»ã€çœ‹é›»è¦–ä¹…ä¸€é»ï¼Œå¯ä»¥å…ˆç•¶æˆã€Œé¢¨æ ¼å·®ç•°ã€ã€‚",
                    "æ¶‰åŠå®‰å…¨ï¼šä¾‹å¦‚è®“å°å­©å–®ç¨åœ¨é™½å°ã€åƒå®¹æ˜“å™åˆ°çš„é£Ÿç‰©ï¼Œå°±éœ€è¦æ¯”è¼ƒå …å®šåœ°æºé€šã€‚"
                ]
            },
            {
                "title": "2ï¸âƒ£ æºé€šæ™‚ï¼Œå…ˆæ„Ÿè¬å†è¡¨é”æ“”å¿ƒï¼ˆç”¨ã€Œæˆ‘ã€è¨Šæ¯ï¼‰",
                "items": [
                    "å¦‚ï¼šã€Œæˆ‘çœŸçš„å¾ˆæ„Ÿè¬å¦³å¹«å¿™é¡§å°å­©ï¼Œæˆ‘æœƒæ¯”è¼ƒæ”¾å¿ƒã€‚ã€",
                    "å†æ¥ï¼šã€Œåªæ˜¯æˆ‘æœ‰é»æ“”å¿ƒï¼Œä»–åƒå¤ªå¤šç³–å°ç‰™é½’ä¸å¥½ï¼Œæˆ‘æƒ³æˆ‘å€‘å¯ä¸å¯ä»¥ä¸€èµ·å¹«ä»–å°‘ä¸€é»ï¼Ÿã€",
                    "é¿å…ç”¨ã€Œå¦³é€™æ¨£ä¸å°ã€ã€Œå¦³æŠŠå°å­©å¸¶å£äº†ã€ï¼Œæ¯”è¼ƒä¸æœƒç«‹åˆ»è®Šæˆåµæ¶ã€‚"
                ]
            },
            {
                "title": "3ï¸âƒ£ å„˜é‡è®“å¦ä¸€åŠç•¶æ©‹æ¨‘",
                "items": [
                    "è‡ªå·±çš„çˆ¸åª½é€šå¸¸æ¯”è¼ƒè½è‡ªå·±å°å­©çš„è©±ã€‚",
                    "å¯ä»¥å…ˆè·Ÿå¦ä¸€åŠç§ä¸‹æºé€šå¥½ç«‹å ´èˆ‡åº•ç·šï¼Œå†ç”±ä»– / å¥¹è·Ÿå©†å©†èªªã€‚"
                ]
            },
            {
                "title": "4ï¸âƒ£ å»ºç«‹å¹¾æ¢ã€Œå…¨å®¶å…±åŒçš„åŸå‰‡ã€",
                "items": [
                    "ä¾‹å¦‚ï¼šç”¨è—¥ä¸€å®šè¦å•çˆ¸åª½ã€ä¸èƒ½æ‰“å°å­©ã€å¤§ç´„å¹¾é»ç¡è¦ºã€‚",
                    "åŸå‰‡è¶Šæ¸…æ¥šï¼Œè¶Šä¸æœƒæ¯ä»¶å°äº‹éƒ½åµæˆä¸€åœ˜ã€‚"
                ]
            },
            {
                "title": "5ï¸âƒ£ å¦‚æœè¡çªå½±éŸ¿åˆ°ä½ è‡ªå·±çš„æƒ…ç·’",
                "items": [
                    "å¯ä»¥è€ƒæ…®å’Œè«®å•†å¿ƒç†å¸«è«‡è«‡ï¼Œæ•´ç†è‡ªå·±çš„å§”å±ˆèˆ‡è§’è‰²å£“åŠ›ï¼ˆåª³å©¦ / åª½åª½é›™é‡èº«åˆ†ï¼‰ã€‚",
                    "æœ‰æ™‚å€™å•é¡Œä¸åªæ˜¯æ•™é¤ŠæŠ€å·§ï¼Œä¹Ÿæ˜¯ä½ å’Œå…ˆç”Ÿã€ä½ å’Œå©†å©†ä¹‹é–“çš„ç•Œç·šã€‚"
                ]
            }
        ]

    else:
        title = "ä¸€èˆ¬å»ºè­°"
        sections = [
            {
                "title": "",
                "items": ["é€™å€‹å•é¡Œç›®å‰é‚„æ²’æœ‰å°ˆé–€å¯«å¥½çš„å»ºè­°å›ç­”ï¼Œå…ˆç”¨èª²ç¨‹èˆ‡æ–‡ç« æ¨è–¦æ¨¡å¼å¹«ä½ æ‰¾è³‡æ–™ã€‚"]
            }
        ]

    return {
        "type": "advice",
        "intent": intent,
        "query": q,
        "title": title,
        "sections": sections
    }


def build_nearby_points_response(address: str, results):
    """
    æŠŠ find_nearby_points çš„çµæœè½‰æˆ JSON-friendly çµæ§‹
    """
    if not results:
        return {
            "type": "xin_points",
            "address": address,
            "points": [],
            "message": f"åœ¨ã€Œ{address}ã€5 å…¬é‡Œå…§æ²’æœ‰æ‰¾åˆ°å¿ƒæ“šé»"
        }

    points = []
    for p, d in results:
        points.append({
            "title": p.get("title"),
            "address": p.get("address"),
            "tel": p.get("tel"),
            "distance_km": round(d, 2),
        })

    return {
        "type": "xin_points",
        "address": address,
        "points": points
    }


def build_recommendations_response(
    query: str,
    results: List[Dict[str, Any]],
    offset: int = 0,
    limit: int = TOP_K
):
    # æ²’æœ‰çµæœ
    if not results:
        return {
            "type": "course_recommendation",
            "query": query,
            "total": 0,
            "video_count": 0,
            "article_count": 0,
            "offset": offset,
            "limit": limit,
            "has_more": False,
            "results": [],
            "message": "ç›®å‰æ‰¾ä¸åˆ°å¾ˆç¬¦åˆçš„èª²ç¨‹ï¼Œå¯ä»¥è©¦è‘—ç”¨ï¼šå©†åª³ã€å£“åŠ›ã€æ†‚é¬±ã€å¤±çœ â€¦ ç­‰è©å†è©¦è©¦çœ‹ã€‚"
        }

    # âœ… å…ˆé‡æ’ï¼ˆåˆ†é å‰ï¼‰
    results = reorder_episode_pairs(results)

    total = len(results)
    # âœ… è¨ˆç®—ç¸½æ•¸ï¼ˆç”¨é‡æ’å¾Œçš„ results çµ±è¨ˆå³å¯ï¼‰
    video_count = sum(1 for r in results if not r.get("is_article"))
    article_count = sum(1 for r in results if r.get("is_article"))

    # âœ… æ­£ç¢ºåˆ‡åˆ†é 
    page_results = results[offset: offset + limit]

    items = []
    for r in page_results:  # âœ… ä¸€å®šè¦ç”¨ page_results
        title = r.get("title") or "(ç„¡æ¨™é¡Œ)"
        section_title = r.get("section_title") or "(æœªåˆ†é¡å°ç¯€)"
        score = r.get("_score", 0.0)

        is_article = bool(r.get("is_article"))
        youtube_url = r.get("youtube_url")

        entry: Dict[str, Any] = {
            "section_title": section_title,
            "title": title,
            "score": score,
            "is_article": is_article,  # âœ… çµ¦å‰ç«¯ç›´æ¥ç”¨
            "type": "article" if is_article else "video",
        }

        if is_article:
            article_url = r.get("article_url") or r.get("url")
            content_text = (r.get("content_text") or "").replace("\n", " ")
            snippet = content_text[:100] + ("..." if len(content_text) > 100 else "")
            entry["article_url"] = article_url
            entry["snippet"] = snippet
        else:
            seg = r.get("_best_segment")
            if seg:
                start_sec = seg.get("start_sec", 0.0)
                start_str = format_time(start_sec)
                seg_text = seg.get("text", "") or ""
                hint = f"è©²å–®å…ƒåœ¨ {start_str} æœ‰æåˆ°ï¼šã€Œ{seg_text[:30]}...ã€"
            else:
                hint = "å­—å¹•è£¡æ²’æœ‰ç‰¹åˆ¥å‘½ä¸­é—œéµå¥ï¼Œå¯ä»¥å¾é ­é–‹å§‹çœ‹ã€‚"
            entry["youtube_url"] = youtube_url
            entry["hint"] = hint

        items.append(entry)

    return {
        "type": "course_recommendation",
        "query": query,
        "total": total,
        "video_count": video_count,
        "article_count": article_count,
        "offset": offset,
        "limit": limit,
        "has_more": offset + limit < total,
        "results": items
    }



def load_xin_points() -> List[Dict[str, Any]]:
    try:
        data = json.loads(XIN_POINTS_FILE.read_text("utf-8"))
        return data.get("data", [])
    except Exception as e:
        print(f"[xin] âš ï¸ å¿ƒæ“šé»è¼‰å…¥å¤±æ•—ï¼š{e}")
        return []
    
def haversine_km(lon1, lat1, lon2, lat2) -> float:
    lon1, lat1, lon2, lat2 = map(radians, [lon1, lat1, lon2, lat2])
    dlon = lon2 - lon1
    dlat = lat2 - lat1
    a = sin(dlat/2)**2 + cos(lat1)*cos(lat2)*sin(dlon/2)**2
    c = 2 * asin(sqrt(a))
    return 6371 * c  # km

def geocode_address(address: str):
    if not address:
        return None

    def try_geocode(addr: str):
        url = "https://nominatim.openstreetmap.org/search"
        params = {"q": addr, "format": "json", "limit": 1}
        headers = {"User-Agent": "xin-bot/1.0"}
        try:
            r = requests.get(url, params=params, headers=headers, timeout=5)
            r.raise_for_status()
            data = r.json()
            if data:
                lat = float(data[0]["lat"])
                lon = float(data[0]["lon"])
                print(f"[geocode] å‘½ä¸­ï¼š'{addr}' -> lat={lat}, lon={lon}")
                return lat, lon
        except Exception as e:
            print(f"[geocode] éŒ¯èª¤ï¼š{e}")
        return None

    print(f"[geocode] å˜—è©¦ï¼š{address}")
    result = try_geocode(address)
    if result:
        return result

    if "è‡º" in address:
        addr2 = address.replace("è‡º", "å°")
        print(f"[geocode] å˜—è©¦ï¼š{addr2}")
        result = try_geocode(addr2)
        if result:
            return result

    addr3 = re.sub(r"\d+è™Ÿ.*", "", address)
    if addr3 != address:
        print(f"[geocode] å˜—è©¦ï¼ˆå»è™Ÿï¼‰ï¼š{addr3}")
        result = try_geocode(addr3)
        if result:
            return result

    addr4 = re.sub(r"\d+å¼„.*", "", address)
    if addr4 != address:
        print(f"[geocode] å˜—è©¦ï¼ˆå»å¼„ï¼‰ï¼š{addr4}")
        result = try_geocode(addr4)
        if result:
            return result

    addr5 = re.sub(r"\d+å··.*", "", address)
    if addr5 != address:
        print(f"[geocode] å˜—è©¦ï¼ˆå»å··ï¼‰ï¼š{addr5}")
        result = try_geocode(addr5)
        if result:
            return result

    m = re.match(
        r"(å°åŒ—å¸‚|è‡ºåŒ—å¸‚|æ–°åŒ—å¸‚|æ¡ƒåœ’å¸‚|è‡ºä¸­å¸‚|å°ä¸­å¸‚|è‡ºå—å¸‚|å°å—å¸‚|é«˜é›„å¸‚|"
        r"åŸºéš†å¸‚|æ–°ç«¹å¸‚|å˜‰ç¾©å¸‚|æ–°ç«¹ç¸£|è‹—æ —ç¸£|å½°åŒ–ç¸£|å—æŠ•ç¸£|é›²æ—ç¸£|å˜‰ç¾©ç¸£|"
        r"å±æ±ç¸£|å®œè˜­ç¸£|èŠ±è“®ç¸£|è‡ºæ±ç¸£|å°æ±ç¸£|æ¾æ¹–ç¸£|é‡‘é–€ç¸£|é€£æ±Ÿç¸£)"
        r"(.+?(å€|å¸‚|é®|é„‰))",
        address
    )
    if m:
        addr6 = m.group(1) + m.group(2)
        print(f"[geocode] å˜—è©¦ï¼ˆå¸‚+å€/é„‰/é®/å¸‚ï¼‰ï¼š{addr6}")
        result = try_geocode(addr6)
        if result:
            return result

    print(f"[geocode] å®Œå…¨æŸ¥ä¸åˆ°ï¼š{address}")
    return None
    
def find_nearby_points(lat, lon, max_km=5, top_k=5):
    points = load_xin_points()
    results = []

    for p in points:
        if p.get("lat") and p.get("lon"):
            d = haversine_km(lon, lat, p["lon"], p["lat"])
            if d <= max_km:
                results.append((p, d))

    results.sort(key=lambda x: x[1])
    return results[:top_k]

def load_all_units() -> List[Dict[str, Any]]:
    data = json.loads(UNITS_FILE.read_text("utf-8"))
    raw_units = data.get("units", [])

    units: List[Dict[str, Any]] = []

    for u in raw_units:
        u = dict(u)  

        section_title = u.get("section_title") or ""

        subtitle_texts = " ".join(
            seg.get("text", "") for seg in u.get("subtitles", []) or []
        )

        content_text = u.get("content_text", "") or ""

        search_text = " ".join(
            s for s in [
                section_title,
                u.get("title") or "",
                content_text,
                subtitle_texts,
            ] if s
        )

        u["_search_text"] = search_text

        units.append(u)

    print(f"[load] âœ… å…±è¼‰å…¥ {len(units)} å€‹å–®å…ƒï¼ˆå«å½±ç‰‡ + æ–‡ç« ï¼‰")
    return units

def extract_address_from_query(q: str) -> str:
    original = q

    if "é™„è¿‘" in q:
        q = q.split("é™„è¿‘")[0]

    for kw in ["å¿ƒæ“šé»", "é–€è¨º", "çœ‹è¨º"]:
        if kw in q:
            q = q.split(kw)[0]

    prefixes = ["æˆ‘ä½åœ¨", "æˆ‘ä½", "å®¶åœ¨", "å®¶ä½", "ä½åœ¨", "ä½", "åœ¨"]
    q = q.strip()
    for p in prefixes:
        if q.startswith(p):
            q = q[len(p):].strip()
            break

    tail_words = ["æœ‰æ²’æœ‰", "æœ‰å—", "å—", "å‘¢", "å•Š", "å•¦"]
    for t in tail_words:
        if q.endswith(t):
            q = q[: -len(t)].strip()

    q = q.strip(" ?ï¼Ÿ!")

    if len(q) < 4:
        return ""

    print(f"[debug] extract_address_from_query: '{original}' -> '{q}'")
    return q

def normalize_query(q: str):
    q = q.strip().lower()
    if not q: return [], [], [] # å›å‚³ä¸‰å€‹ list

    functional_words = [
        "æ–‡ç« ", "å½±ç‰‡", "æƒ³çœ‹", "çµ¦æˆ‘", "åªæœ‰", "åªæƒ³çœ‹", "æ¨è–¦", 
        "å½±éŸ³", "æ’­æ”¾", "æŸ¥è©¢", "æ‰¾", "æœ‰å“ªäº›", "ä»‹ç´¹"
    ]
    
    user_input_core = []   # ä½¿ç”¨è€…çœŸçš„æ‰“å‡ºä¾†çš„è© (ä¾‹å¦‚: ç„¦æ…®)
    category_expanded = [] # åŒä¸€ç¾¤çµ„çš„è¯æƒ³è© (ä¾‹å¦‚: ç·Šå¼µã€ææ…Œ)
    other_terms = []       # å…¶ä»–æƒ…å¢ƒè©
    
    found_categories = set()

    # 1. æƒæä½¿ç”¨è€…è¼¸å…¥ï¼ŒæŠ“å‡ºå‘½ä¸­å“ªäº›åˆ†é¡
    for category, kws in KEYWORDS_DATA.items():
        for kw in kws:
            if kw in q:
                if kw not in user_input_core:
                    user_input_core.append(kw)
                found_categories.add(category) # è¨˜ä½é€™å€‹åˆ†é¡ (ä¾‹å¦‚ anxiety_panic)
    
    # 2. æ ¹æ“šå‘½ä¸­çš„åˆ†é¡ï¼Œé€²è¡Œã€ŒåŒç¾©è©æ“´å±•ã€
    for cat in found_categories:
        group_kws = KEYWORDS_DATA[cat]
        for kw in group_kws:
            # å¦‚æœé€™å€‹è©ä¸æ˜¯ä½¿ç”¨è€…æ‰“çš„ï¼Œå°±åŠ å…¥æ“´å±•æ¸…å–®
            if kw not in user_input_core and kw not in category_expanded:
                category_expanded.append(kw)

    # 3. è™•ç†å‰©é¤˜çš„ã€Œå…¶ä»–è©ã€
    # å…ˆæŠŠæ ¸å¿ƒè©å¾å­—ä¸²ä¸­ç§»é™¤
    temp_q = q
    for kw in user_input_core:
        temp_q = temp_q.replace(kw, " ") 
    for fw in functional_words:
        temp_q = temp_q.replace(fw, " ")

    parts = re.split(r"[ï¼Œã€‚ï¼!ï¼Ÿ?\sã€ï¼›;:ï¼š]+", temp_q)
    for part in parts:
        if len(part) >= 2 and part not in STOP_WORDS:
            if part not in other_terms:
                other_terms.append(part)
                
    return user_input_core, category_expanded, other_terms

def score_unit(unit, user_core, expanded_core, other_terms):
    title = (unit.get("section_title") or "") + (unit.get("title") or "")
    content = unit.get("content_text", "") or "" 
    
    if not title and not content: 
        return 0.0, None

    score = 0.0
    
    # --- A. ä½¿ç”¨è€…è¦ªè‡ªè¼¸å…¥çš„è© (æ¬Šé‡æœ€é«˜: ç²¾æº–å‘½ä¸­) ---
    for kw in user_core:
        # æ¨™é¡Œå‘½ä¸­ +10
        if kw in title:
            score += 10.0
        # å…§æ–‡å‘½ä¸­ +2
        cnt = content.count(kw)
        if cnt > 0:
            score += cnt * 4.0

    # --- B. åŒç¾©æ“´å±•è© (æ¬Šé‡æ¬¡é«˜: æ¨¡ç³Šå‘½ä¸­) ---
    # âœ¨âœ¨âœ¨ã€é€™è£¡è¦ä¿®æ­£ã€‘åŸæœ¬å°‘äº†è¿´åœˆï¼Œä¸”åˆ†æ•¸æ‡‰è©²æ˜¯ 5.0 âœ¨âœ¨âœ¨
    for kw in expanded_core:
        # æ¨™é¡Œå‘½ä¸­ +5
        if kw in title:
            score += 5.0
        # å…§æ–‡å‘½ä¸­ +1
        cnt = content.count(kw)
        if cnt > 0:
            score += cnt * 2.0

    # --- C. å…¶ä»–æƒ…å¢ƒè© (æ¬Šé‡æœ€ä½) ---
    for kw in other_terms:
        if kw in title:
            score += 1.0
        cnt = content.count(kw)
        if cnt > 0:
            score += cnt * 0.5

    # --- D. å­—å¹•é€£çºŒæ€§è¨ˆåˆ† ---
    subtitles = unit.get("subtitles", [])
    best_seg = None
    best_seg_score = 0
    
    has_core_list = []
    
    for seg in subtitles:
        seg_text = seg.get("text", "")
        # å„ªå…ˆçµ±è¨ˆ user_core
        hits = sum(1 for kw in user_core if kw in seg_text)
        
        # å¦‚æœä½¿ç”¨è€…è¼¸å…¥çš„æ²’ä¸­ï¼Œå¯¬å®¹æª¢æŸ¥ expanded_core (ç®—åŠæ¬¡)
        if hits == 0:
            hits = sum(1 for kw in expanded_core if kw in seg_text) * 0.5 

        has_core = (hits > 0)
        has_core_list.append(has_core)

        if hits > best_seg_score:
            best_seg_score = hits
            best_seg = seg

    # æª¢æŸ¥é€£çºŒä¸‰å€‹ True
    count_continuous_hits = 0
    if len(has_core_list) >= 3:
        for i in range(len(has_core_list) - 2):
            if has_core_list[i] and has_core_list[i+1] and has_core_list[i+2]:
                count_continuous_hits += 1
    
    score += count_continuous_hits * 2.0

    return score, best_seg

# æŠŠã€Œ(ä¸Š)/(ä¸‹)/(ï¼ˆä¸Šï¼‰)/(ï¼ˆä¸‹ï¼‰)/ä¸Šç¯‡/ä¸‹ç¯‡/ä¸Šé›†/ä¸‹é›†ã€è¦–ç‚ºé›†æ•¸æ¨™è¨˜ï¼ˆå¯å‡ºç¾åœ¨ä»»ä½•ä½ç½®ï¼‰
EP_TAG_RE = re.compile(r"(ï¼ˆä¸Šï¼‰|ï¼ˆä¸‹ï¼‰|\(ä¸Š\)|\(ä¸‹\)|ä¸Šç¯‡|ä¸‹ç¯‡|ä¸Šé›†|ä¸‹é›†)")

def get_episode_tag(title: str) -> Optional[str]:
    """å›å‚³ 'ä¸Š' / 'ä¸‹' / Noneï¼ˆä¸é™å‡ºç¾åœ¨çµå°¾ï¼‰"""
    if not title:
        return None
    t = title.strip()
    if re.search(r"(ï¼ˆä¸Šï¼‰|\(ä¸Š\)|ä¸Šç¯‡|ä¸Šé›†)", t):
        return "ä¸Š"
    if re.search(r"(ï¼ˆä¸‹ï¼‰|\(ä¸‹\)|ä¸‹ç¯‡|ä¸‹é›†)", t):
        return "ä¸‹"
    return None

def get_base_key(section_title: str, title: str) -> str:
    """
    ç”¨ä¾†æŠŠã€Œä¸Š/ä¸‹ã€è¦–ç‚ºåŒä¸€çµ„çš„ key
    - ç§»é™¤æ¨™é¡Œä¸­çš„ä¸Š/ä¸‹æ¨™è¨˜ï¼ˆä¸é™ä½ç½®ï¼‰
    - å†ç”¨ section_title + æ¸…ç†å¾Œ title ç•¶ key
    """
    s = (section_title or "").strip()
    t = (title or "").strip()
    t2 = EP_TAG_RE.sub("", t)  # âœ… ä¸é™çµå°¾ï¼Œç›´æ¥æŠŠ(ä¸Š)/(ä¸‹)ç§»é™¤
    t2 = re.sub(r"\s+", "", t2)  # å¯é¸ï¼šå»ç©ºç™½ï¼Œè®“ key æ›´ç©©
    s2 = re.sub(r"\s+", "", s)
    return f"{s2}||{t2}"


def reorder_episode_pairs(results: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    """
    ç›®æ¨™ï¼š
    - åŒä¸€ç³»åˆ—ï¼ˆåŒ base_keyï¼‰çš„ä¸Š/ä¸‹è¦é»åœ¨ä¸€èµ·ï¼šä¸Š â†’ ä¸‹
    - ç³»åˆ—èˆ‡ç³»åˆ—ä¹‹é–“ï¼Œç”¨è©²ç³»åˆ—çš„ä»£è¡¨åˆ†æ•¸æ’åºï¼ˆé è¨­å–ç³»åˆ—å…§æœ€é«˜ _scoreï¼‰
    - æ²’æœ‰ä¸Š/ä¸‹æ¨™è¨˜çš„å–®ç¯‡ï¼Œè¦–ç‚ºä¸€å€‹ç¨ç«‹ç³»åˆ—ï¼Œä¿ç•™åŸæœ¬é …ç›®æœ¬èº«
    """

    # 1) å»ºç«‹ç¾¤çµ„ï¼škey -> {"items": [...], "best_score": float, "first_idx": int}
    groups: Dict[str, Dict[str, Any]] = {}

    for idx, r in enumerate(results):
        key = get_base_key(r.get("section_title"), r.get("title"))
        score = float(r.get("_score", 0.0))

        g = groups.get(key)
        if g is None:
            groups[key] = {
                "items": [],
                "best_score": score,
                "first_idx": idx,  # è‹¥åˆ†æ•¸ä¸€æ¨£ï¼Œç”¨æœ€æ—©å‡ºç¾é †åºç•¶ tie-break
            }
            g = groups[key]

        g["items"].append(r)
        if score > g["best_score"]:
            g["best_score"] = score

    # 2) æ¯çµ„å…§ï¼šä¸Šâ†’ä¸‹â†’å…¶ä»–ï¼ˆå¦‚æœæœ‰å¥‡æ€ªçš„æ²’æ¨™è¨˜ï¼‰
    def item_rank(r: Dict[str, Any]) -> int:
        tag = get_episode_tag(r.get("title") or "")
        if tag == "ä¸Š":
            return 0
        if tag == "ä¸‹":
            return 1
        return 2

    for g in groups.values():
        # åŒä¸€çµ„å¯èƒ½æœ‰å¤šå€‹ä¸Šæˆ–å¤šå€‹ä¸‹ï¼šåŒ rank å…§å†ç”¨åˆ†æ•¸é«˜çš„æ’å‰
        g["items"].sort(key=lambda r: (item_rank(r), -float(r.get("_score", 0.0))))

    # 3) çµ„èˆ‡çµ„ä¹‹é–“æ’åºï¼šåˆ†æ•¸é«˜çš„çµ„æ’å‰ï¼›åˆ†æ•¸åŒå‰‡ç”¨ first_idx ä¿æŒç©©å®š
    ordered_groups = sorted(
        groups.values(),
        key=lambda g: (-g["best_score"], g["first_idx"])
    )

    # 4) æ”¤å¹³æˆä¸€æ¢ list
    out: List[Dict[str, Any]] = []
    for g in ordered_groups:
        out.extend(g["items"])

    return out

def format_time(seconds: float) -> str:
    s = int(seconds)
    h = s // 3600
    m = (s % 3600) // 60
    sec = s % 60
    if h > 0:
        return f"{h:02d}:{m:02d}:{sec:02d}"
    return f"{m:02d}:{sec:02d}"

def search_units(units: List[Dict[str, Any]], query: str, top_k: int = TOP_K):
    # æ¥æ”¶ä¸‰å€‹å›å‚³å€¼
    user_core, expanded_core, other_terms = normalize_query(query)
    
    # ä¿åº•ï¼šå¦‚æœéƒ½æ²’æŠ“åˆ°æ ¸å¿ƒè©ï¼ŒæŠŠ query æœ¬èº«ç•¶ä½œæ ¸å¿ƒè©
    if not user_core and len(query) >= 2:
        user_core = [query]

    if not user_core and not other_terms: 
        return []

    results = []
    for u in units:
        # å‚³å…¥ä¸‰å€‹åƒæ•¸
        score, best_seg = score_unit(u, user_core, expanded_core, other_terms)
        
        if score > 0:
            r = dict(u)
            r["_score"] = score
            r["_best_segment"] = best_seg
            results.append(r)

    results.sort(key=lambda x: x["_score"], reverse=True)
    return results

def detect_media_preference(q: str) -> Optional[str]:
    """
    åµæ¸¬ä½¿ç”¨è€…æ˜¯å¦æŒ‡å®šæƒ³çœ‹ã€æ–‡ç« ã€æˆ–ã€å½±ç‰‡ã€
    """
    if any(w in q for w in ["æƒ³çœ‹æ–‡ç« ", "çµ¦æˆ‘æ–‡ç« ", "åªæœ‰æ–‡ç« ", "æ–‡ç« æ¨è–¦"]):
        return "article"
    if any(w in q for w in ["æƒ³çœ‹å½±ç‰‡", "çµ¦æˆ‘å½±ç‰‡", "æ’­æ”¾å½±ç‰‡", "å½±éŸ³", "youtube"]):
        return "video"
    return None

# ---------- äº’å‹•ä¸»è¿´åœˆ ----------
app = FastAPI(title="å¿ƒå¿«æ´»èª²ç¨‹æ¨è–¦ API")

# è‹¥å‰ç«¯ç¶²é æœƒè·¨ç¶²åŸŸå‘¼å«ï¼Œå¯é–‹ CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # ä¸Šç·šæ™‚å»ºè­°æ”¹æˆä½ çš„ç¶²åŸŸ
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# â–¼â–¼ æ–°å¢ï¼šè®“ /static åº•ä¸‹å¯ä»¥ç›´æ¥æŠ“æª”æ¡ˆ â–¼â–¼
app.mount("/static", StaticFiles(directory="static"), name="static")
@app.get("/", include_in_schema=False)
def serve_index():
    return FileResponse("static/index.html")

# å•Ÿå‹•æ™‚å°±å…ˆè¼‰å…¥æ‰€æœ‰å–®å…ƒ
UNITS_CACHE: List[Dict[str, Any]] = load_all_units()

# ç°¡å–®æ”¾åœ¨è¨˜æ†¶é«”çš„èŠå¤©ç´€éŒ„ï¼ˆserver é‡å•Ÿæœƒæ¸…ç©ºï¼‰
HISTORY: Dict[str, List[Dict[str, Any]] ] = {}


class ChatRequest(BaseModel):
    query: str
    session_id: Optional[str] = None


class NearbyRequest(BaseModel):
    address: str


class RecommendRequest(BaseModel):
    query: str


@app.get("/reload_keywords")
def reload_keywords():
    """æ‰‹å‹•è§¸ç™¼é‡æ–°è¼‰å…¥é—œéµå­—è¨­å®š"""
    load_keywords_from_json()
    return {
        "status": "success", 
        "mental_keywords_count": len(MENTAL_KEYWORDS),
        "stop_words_count": len(STOP_WORDS)
    }

@app.get("/ping")
def ping():
    return {"status": "ok"}


@app.post("/chat")
def chat(req: ChatRequest):
    q = req.query.strip()
    session_id = req.session_id or "anonymous"
    resp: Dict[str, Any]
    
    print(f">>> [/chat] session_id: {session_id} | query: {q}")

    # --- å…§éƒ¨è¼”åŠ©å‡½å¼ ---
    def detect_media_preference(text: str) -> Optional[str]:
        """åµæ¸¬ä½¿ç”¨è€…æ˜¯å¦æŒ‡å®šç‰¹å®šåª’é«”é¡å‹"""
        if any(w in text for w in ["æƒ³çœ‹æ–‡ç« ", "çµ¦æˆ‘æ–‡ç« ", "åªæœ‰æ–‡ç« ", "æ–‡ç« æ¨è–¦", "æ‰¾æ–‡ç« ", "åªæƒ³çœ‹æ–‡ç« "]):
            return "article"
        if any(w in text for w in ["æƒ³çœ‹å½±ç‰‡", "çµ¦æˆ‘å½±ç‰‡", "æ’­æ”¾å½±ç‰‡", "å½±éŸ³", "çœ‹å½±ç‰‡", "youtube", "åªæƒ³çœ‹å½±ç‰‡"]):
            return "video"
        return None

    # 1. åµæ¸¬åª’é«”åå¥½
    media_pref_check = detect_media_preference(q)

    # 2. å»ºç«‹ã€Œæ¸…æ´—å¾Œã€çš„å­—ä¸²ï¼Œç”¨ä¾†åˆ¤æ–·æ˜¯å¦åŒ…å«æ–°çš„é—œéµå­—
    q_cleaned = q
    if media_pref_check == "article":
        for w in ["æƒ³çœ‹æ–‡ç« ", "çµ¦æˆ‘æ–‡ç« ", "åªæœ‰æ–‡ç« ", "æ–‡ç« æ¨è–¦", "æ‰¾æ–‡ç« ", "åªæƒ³çœ‹æ–‡ç« ", "æ–‡ç« "]:
            q_cleaned = q_cleaned.replace(w, "")
    elif media_pref_check == "video":
        for w in ["æƒ³çœ‹å½±ç‰‡", "çµ¦æˆ‘å½±ç‰‡", "æ’­æ”¾å½±ç‰‡", "å½±éŸ³", "çœ‹å½±ç‰‡", "youtube", "åªæƒ³çœ‹å½±ç‰‡", "å½±ç‰‡"]:
            q_cleaned = q_cleaned.replace(w, "")
    
    q_cleaned = q_cleaned.strip()
    
    # 3. å˜—è©¦è§£ææ ¸å¿ƒè©ï¼ˆä½¿ç”¨æ¸…æ´—å¾Œçš„å­—ä¸²ï¼‰
    # âœ¨ ä¿®æ­£é»ï¼šé€™è£¡æ¥æ”¶ 3 å€‹å›å‚³å€¼ (user_core, expanded_core, other_terms)
    # æˆ‘å€‘åªéœ€è¦ user_core ä¾†åˆ¤æ–·æ˜¯å¦éœ€è¦å¼·åˆ¶è¦–ç‚ºæ–°ä¸»é¡Œ
    user_core, _, _ = normalize_query(q_cleaned)
    
    # å¦‚æœæ¸…æ´—å¾Œé‚„æœ‰å‰©é¤˜æ–‡å­—ï¼ˆä¸”é•·åº¦å¤ ï¼‰ï¼Œä½† normalize æ²’æŠ“åˆ°ï¼ˆä¾‹å¦‚"å¤±çœ "ä¸åœ¨é—œéµå­—è¡¨ï¼‰ï¼Œå¼·è¿«å°‡å…¶è¦–ç‚ºæ–°ä¸»é¡Œ
    if not user_core and len(q_cleaned) >= 2:
        user_core = [q_cleaned]


    # --- é€²å…¥åˆ¤æ–·æµç¨‹ ---

    # A. è™•ç†ã€Œå¿ƒæ“šé»ã€/ã€Œçœ‹è¨ºã€åœ°å€æœå°‹é‚è¼¯
    if ("é™„è¿‘" in q) and ("å¿ƒæ“šé»" in q or "çœ‹è¨º" in q or "é–€è¨º" in q):
        addr = extract_address_from_query(q)
        if not addr:
            resp = {
                "type": "xin_points", "address": None, "points": [],
                "message": "æˆ‘æœ‰é»æŠ“ä¸åˆ°åœ°å€ï¼Œè«‹å˜—è©¦è¼¸å…¥å®Œæ•´åœ°å€ï¼Œä¾‹å¦‚ï¼šå°å—å¸‚æ±å€å¤§å­¸è·¯1è™Ÿ"
            }
        else:
            geo = geocode_address(addr)
            if not geo:
                resp = {
                    "type": "xin_points", "address": addr, "points": [],
                    "message": f"æŸ¥ä¸åˆ°ã€Œ{addr}ã€é€™å€‹åœ°å€ï¼Œè«‹æ”¹æˆæ›´æ­£å¼çš„å¯«æ³•è©¦è©¦çœ‹"
                }
            else:
                lat, lon = geo
                results = find_nearby_points(lat, lon, max_km=5, top_k=TOP_K)
                resp = build_nearby_points_response(addr, results)

    # B. ç›´æ¥è¼¸å…¥å®Œæ•´åœ°å€
    elif ADDR_HEAD_RE.match(q):
        addr = q
        geo = geocode_address(addr)
        if not geo:
            resp = {
                "type": "xin_points", "address": addr, "points": [],
                "message": f"æŸ¥ä¸åˆ°ã€Œ{addr}ã€é€™å€‹åœ°å€ï¼Œè«‹æ”¹æˆæ›´æ­£å¼çš„å¯«æ³•è©¦è©¦çœ‹"
            }
        else:
            lat, lon = geo
            results = find_nearby_points(lat, lon, max_km=5, top_k=TOP_K)
            resp = build_nearby_points_response(addr, results)

    # C. è™•ç†ã€Œä¸‹ä¸€é ã€åˆ†é é‚è¼¯
    elif detect_pagination_intent(q):
        history = HISTORY.get(session_id, [])
        last = next(
            (h for h in reversed(history) if h["response"].get("type") == "course_recommendation"),
            None
        )
        if not last:
            resp = {"type": "text", "message": "ç›®å‰æ²’æœ‰ä¸Šä¸€ç­†æ¨è–¦çµæœï¼Œå¯ä»¥å…ˆå•ä¸€å€‹å•é¡Œ ğŸ˜Š"}
        else:
            prev_resp = last["response"]
            # å„ªå…ˆä½¿ç”¨ query_rawï¼Œè‹¥ç„¡å‰‡ç”¨ query
            prev_query = prev_resp.get("query_raw") or prev_resp.get("query")
            prev_filter = prev_resp.get("filter_type", None)

            new_offset = prev_resp["offset"] + prev_resp["limit"]
            full_results = search_units(UNITS_CACHE, prev_query, top_k=9999)
            
            if prev_filter == "article":
                full_results = [r for r in full_results if r.get("is_article")]
            elif prev_filter == "video":
                full_results = [r for r in full_results if not r.get("is_article")]

            resp = build_recommendations_response(prev_query, full_results, offset=new_offset, limit=TOP_K)
            resp["filter_type"] = prev_filter
            resp["query_raw"] = prev_query

    # D. ã€ä¿®æ­£é‡é»ã€‘è™•ç†ã€Œç´”ç²¹çš„åª’é«”åˆ‡æ›æŒ‡ä»¤ã€
    # æ¢ä»¶ï¼šæœ‰æŒ‡å®šåª’é«” (ä¾‹å¦‚"æ–‡ç« ") ä¸” æ¸…æ´—å¾Œçš„å­—ä¸²æ˜¯ç©ºçš„ (ä»£è¡¨æ²’æœ‰è¼¸å…¥æ–°ä¸»é¡Œ)
    elif media_pref_check and not q_cleaned:
        history = HISTORY.get(session_id, [])
        # æŠ“å–ä¸Šä¸€ç­†æ¨è–¦ç´€éŒ„
        last = next(
            (h for h in reversed(history) if isinstance(h.get("response"), dict) and h["response"].get("type") == "course_recommendation"),
            None
        )

        if not last:
            resp = {
                "type": "course_recommendation", "query": q, "total": 0, "video_count": 0, "article_count": 0,
                "offset": 0, "limit": TOP_K, "has_more": False, "results": [],
                "message": "é€™çœ‹èµ·ä¾†åƒæ˜¯æƒ³è¦ç¯©é¸æ–‡ç« æˆ–å½±ç‰‡ï¼Œä½†æˆ‘é‚„ä¸çŸ¥é“ä½ æƒ³æ‰¾ä»€éº¼ä¸»é¡Œã€‚è«‹å…ˆè¼¸å…¥ä¸€å€‹ä¸»é¡Œï¼Œä¾‹å¦‚ã€Œç„¦æ…®ã€æˆ–ã€Œå¤±çœ ã€ã€‚"
            }
        else:
            prev_resp = last["response"]
            # é€™è£¡å¾ˆé‡è¦ï¼šä¸€å®šè¦æŠ“åˆ°ä¸Šä¸€ç­†çš„ã€ŒåŸå§‹ä¸»é¡Œã€(ä¾‹å¦‚: å¤±çœ )
            original_topic = prev_resp.get("query_raw") or prev_resp.get("query")
            
            print(f"[chat] è§¸ç™¼ç¯©é¸åˆ‡æ›: ä¸»é¡Œ='{original_topic}' -> é¡å‹='{media_pref_check}'")

            # é‡æ–°æœå°‹è©²ä¸»é¡Œ
            full_results = search_units(UNITS_CACHE, original_topic, top_k=9999)

            # å¼·åˆ¶å¥—ç”¨æ–°çš„éæ¿¾æ¢ä»¶
            if media_pref_check == "article":
                full_results = [r for r in full_results if r.get("is_article")]
            elif media_pref_check == "video":
                full_results = [r for r in full_results if not r.get("is_article")]

            resp = build_recommendations_response(original_topic, full_results, offset=0, limit=TOP_K)
            
            # æ›´æ–°ç‹€æ…‹
            resp["filter_type"] = media_pref_check
            resp["query_raw"] = original_topic # ç¢ºä¿å‚³æ‰¿åŸå§‹ä¸»é¡Œ

            if not resp["results"]:
                type_name = "æ–‡ç« " if media_pref_check == "article" else "å½±ç‰‡"
                resp["message"] = f"é—œæ–¼ã€Œ{original_topic}ã€ç›®å‰æ²’æœ‰ç›¸é—œçš„{type_name}å…§å®¹ã€‚"

    # E. ç‰¹å®šæƒ…å¢ƒå»ºè­° èˆ‡ ä¸€èˆ¬æœå°‹
    else:
        # special_intent = detect_special_intent(q)
        # if special_intent:
        #     resp = build_special_intent_response(special_intent, q)
        # else:
            # é€™æ˜¯ä¸€æ¬¡å…¨æ–°çš„æœå°‹ï¼ˆä¾‹å¦‚è¼¸å…¥ã€Œå¤±çœ ã€ï¼Œæˆ–æ˜¯ã€Œå¤±çœ æ–‡ç« ã€ï¼‰
            # å¦‚æœ q_cleaned æœ‰æ±è¥¿ï¼Œå°±ç”¨ q_cleaned (å»é™¤"æ–‡ç« "å¾Œçš„ç´”ä¸»é¡Œ)ï¼Œå¦å‰‡ç”¨åŸå­—ä¸²
        search_q = q_cleaned if q_cleaned else q
        
        print(f"[chat] åŸ·è¡Œæ–°æœå°‹: '{search_q}'")
        
        full_results = search_units(UNITS_CACHE, search_q, top_k=9999)
        
        # å¦‚æœé€™å¥è©±æœ¬èº«å°±åŒ…å«ç¯©é¸æ„åœ– (ä¾‹å¦‚ "å¤±çœ æ–‡ç« ")
        final_filter = None
        if media_pref_check == "article":
            full_results = [r for r in full_results if r.get("is_article")]
            final_filter = "article"
        elif media_pref_check == "video":
            full_results = [r for r in full_results if not r.get("is_article")]
            final_filter = "video"
        
        resp = build_recommendations_response(search_q, full_results, offset=0, limit=TOP_K)
        
        # ã€é—œéµä¿®æ­£ã€‘ç¢ºä¿é€™è£¡å¯«å…¥ query_raw
        resp["filter_type"] = final_filter
        resp["query_raw"] = search_q 

        if media_pref_check and not resp["results"]:
            type_name = "æ–‡ç« " if media_pref_check == "article" else "å½±ç‰‡"
            resp["message"] = f"é—œæ–¼ã€Œ{search_q}ã€ç›®å‰æ²’æœ‰ç›¸é—œçš„{type_name}å…§å®¹ã€‚"

    # --- è¨˜éŒ„æ­·å²ç´€éŒ„ ---
    history_list = HISTORY.setdefault(session_id, [])
    # é€™è£¡çš„ append æœƒæŠŠ resp (åŒ…å«æ­£ç¢ºçš„ query_raw) å­˜é€²å»
    history_list.append({"query": q, "response": resp})
    
    if len(history_list) > 50:
        history_list.pop(0)

    return resp

@app.get("/history")
def get_history(session_id: str):
    """
    ä¾ session_id å›å‚³å°ˆå±¬èŠå¤©ç´€éŒ„ã€‚
    /history?session_id=xxxx
    """
    print(">>> /history called session_id =", session_id)
    items = HISTORY.get(session_id, [])
    print(">>> HISTORY keys =", list(HISTORY.keys()))
    return {
        "items": items
    }

@app.post("/nearby")
def nearby(req: NearbyRequest):
    addr = req.address.strip()
    if not addr:
        return {
            "type": "xin_points",
            "address": None,
            "points": [],
            "message": "è«‹æä¾›å®Œæ•´åœ°å€ï¼Œä¾‹å¦‚ï¼šå°å—å¸‚æ±å€å¤§å­¸è·¯1è™Ÿ"
        }

    geo = geocode_address(addr)
    if not geo:
        return {
            "type": "xin_points",
            "address": addr,
            "points": [],
            "message": f"æŸ¥ä¸åˆ°ã€Œ{addr}ã€é€™å€‹åœ°å€ï¼Œè«‹æ”¹æˆæ›´æ­£å¼çš„å¯«æ³•è©¦è©¦çœ‹"
        }

    lat, lon = geo
    results = find_nearby_points(lat, lon, max_km=5, top_k=TOP_K)
    return build_nearby_points_response(addr, results)


@app.post("/recommend")
def recommend(req: RecommendRequest):
    q = req.query.strip()
    sid = "anonymous" 
    
    # 1. åµæ¸¬åª’é«”åå¥½ (æ–‡ç« /å½±ç‰‡)
    pref = detect_media_preference(q)
    
    # 2. ç²å–æ ¸å¿ƒè©
    # âœ¨âœ¨âœ¨ã€é€™è£¡è¦ä¿®æ­£ã€‘è®Šæˆæ¥æ”¶ 3 å€‹å€¼ âœ¨âœ¨âœ¨
    user_core, _, _ = normalize_query(q)
    
    # 3. ä¸Šä¸‹æ–‡ç¹¼æ‰¿ï¼šå¦‚æœæ²’æ ¸å¿ƒè©ä½†æœ‰åå¥½ï¼Œå»ç¿» HISTORY
    search_q = q
    if not user_core and pref:
        history = HISTORY.get(sid, [])
        last_rec = next((h for h in reversed(history) if h["response"].get("type") == "course_recommendation"), None)
        if last_rec:
            search_q = last_rec["response"].get("query")
            print(f"[recommend] ç¹¼æ‰¿ä¸»é¡Œ: {search_q}")

    # 4. åŸ·è¡Œæœå°‹
    full_results = search_units(UNITS_CACHE, search_q, top_k=9999)
    
    # 5. åª’é«”éæ¿¾
    if pref == "article":
        full_results = [r for r in full_results if r.get("is_article")]
    elif pref == "video":
        full_results = [r for r in full_results if not r.get("is_article")]

    # 6. å»ºç«‹å›æ‡‰
    resp = build_recommendations_response(search_q, full_results, offset=0, limit=TOP_K)
    
    # 7. å­˜å›æ­·å²
    history_list = HISTORY.setdefault(sid, [])
    history_list.append({"query": q, "response": resp})
    
    return resp

if __name__ == "__main__":
    import uvicorn
    uvicorn.run("xin_api:app", host="0.0.0.0", port=8000, reload=True)