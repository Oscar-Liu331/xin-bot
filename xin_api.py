import time
import json
import re
import os
import requests
from pathlib import Path
from typing import List, Dict, Any
import numpy as np
from math import radians, sin, cos, asin, sqrt
import urllib.parse

from fastapi import FastAPI
from pydantic import BaseModel
from fastapi.middleware.cors import CORSMiddleware
from fastapi.staticfiles import StaticFiles
from fastapi.responses import FileResponse
from typing import Optional

from langdetect import detect, LangDetectException
from deep_translator import GoogleTranslator

# --- å¸¸æ•¸è¨­å®š ---
CITY_PATTERN = (
    r"(å°åŒ—å¸‚|è‡ºåŒ—å¸‚|æ–°åŒ—å¸‚|æ¡ƒåœ’å¸‚|è‡ºä¸­å¸‚|å°ä¸­å¸‚|è‡ºå—å¸‚|å°å—å¸‚|é«˜é›„å¸‚|"
    r"åŸºéš†å¸‚|æ–°ç«¹å¸‚|å˜‰ç¾©å¸‚|æ–°ç«¹ç¸£|è‹—æ —ç¸£|å½°åŒ–ç¸£|å—æŠ•ç¸£|é›²æ—ç¸£|å˜‰ç¾©ç¸£|"
    r"å±æ±ç¸£|å®œè˜­ç¸£|èŠ±è“®ç¸£|è‡ºæ±ç¸£|å°æ±ç¸£|æ¾æ¹–ç¸£|é‡‘é–€ç¸£|é€£æ±Ÿç¸£)"
)
ADDR_HEAD_RE = re.compile(rf"^{CITY_PATTERN}(.*?(å€|é„‰|é®|å¸‚))")
TOP_K = 5  

XIN_POINTS_FILE = Path("xin_points.json")
UNITS_FILE = Path("wellbeing_elearn_pro_all_with_articles.json")

CORPUS_VECTORS = None 

JINA_API_URL = "https://api.jina.ai/v1/embeddings"
JINA_API_KEY = None

KEYWORDS_FILE = Path("keywords.json")
KEYWORDS_DATA = {} 
MENTAL_KEYWORDS = [] 
STOP_WORDS = []

# ç¿»è­¯ç”¨å¿«å–
TRANSLATION_CACHE = {}

MODEL_CONFIGS = {
    "v4": {
        "api_model_name": "jina-embeddings-v4", # æœ€æ–°ç™¼å¸ƒçš„ç‰ˆæœ¬
        "vector_filename": "vectors_v4.json",
        "dimensions": 2048 # âš ï¸ æ³¨æ„ï¼šv4 é è¨­ç¶­åº¦æ˜¯ 2048
    },
    "v3": {
        "api_model_name": "jina-embeddings-v3",
        "vector_filename": "vectors_v3.json",
        "dimensions": 1024
    },
    "v2-zh": {
        "api_model_name": "jina-embeddings-v2-base-zh",
        "vector_filename": "vectors_v2_zh.json",
        "dimensions": 768
    }
}

CURRENT_MODEL_KEY = "v3"

CURRENT_CONFIG = MODEL_CONFIGS[CURRENT_MODEL_KEY]

VECTORS_FILE = Path(CURRENT_CONFIG["vector_filename"])

# --- æ ¸å¿ƒå·¥å…·å‡½å¼ ---

def detect_language(text: str) -> str:
    """
    èªè¨€åµæ¸¬æœ€çµ‚ç‰ˆ
    """
    if not text: return "zh-TW"
    
    # 1. [çµ•å°å„ªå…ˆ] æª¢æŸ¥å¸¸è¦‹æ—¥æ–‡ç‰¹å¾µå­—
    if re.search(r'[ã®ã¯ã§ã™ãŒã¾ã™ãã ã•ã„ã¦ã«ã‚’æ°—]', text):
        return "ja"
    if re.search(r'[\u3040-\u309f\u30a0-\u30ff]', text):
        return "ja"

    # 2. æª¢æŸ¥éŸ“æ–‡
    if re.search(r'[\uac00-\ud7af]', text):
        return "ko"

    # 3. æª¢æŸ¥ç´”è‹±æ–‡
    clean_text = re.sub(r'[0-9\s,.?!:;\'"()\[\]]', '', text)
    if clean_text and all(ord(c) < 128 for c in clean_text):
        return "en"

    # 4. æª¢æŸ¥ä¸­æ–‡
    if re.search(r'[\u4e00-\u9fa5]', text):
        return "zh-TW"

    try:
        lang = detect(text)
        if lang.startswith("zh"): return "zh-TW"
        if lang == 'ja': return 'ja'
        return lang
    except LangDetectException:
        return "zh-TW"

def translate_text(text: str, target: str) -> str:
    if not text: return ""
    if target == "zh-TW" and detect_language(text) == "zh-TW":
        return text
    
    cache_key = f"{text}_{target}"
    if cache_key in TRANSLATION_CACHE:
        return TRANSLATION_CACHE[cache_key]
    
    try:
        translator = GoogleTranslator(source='auto', target=target)
        result = translator.translate(text)
        
        # é˜²å‘†
        if result == text and len(text) > 5 and target != "zh-TW":
             clean = re.sub(r"[ã€ã€‘ã€Šã€‹ã€Œã€]", " ", text).strip()
             if clean != text:
                 retry = translator.translate(clean)
                 if retry != clean:
                     result = retry

        TRANSLATION_CACHE[cache_key] = result
        return result
    except Exception as e:
        print(f"!!! [Translate Error] Text: {text[:10]}... | Error: {e}")
        return text 

def load_keywords_from_json():
    global KEYWORDS_DATA, MENTAL_KEYWORDS, STOP_WORDS
    try:
        if KEYWORDS_FILE.exists():
            with open(KEYWORDS_FILE, "r", encoding="utf-8") as f:
                data = json.load(f)
                KEYWORDS_DATA = data.get("mental_keywords", {})
                all_kws = []
                for category_list in KEYWORDS_DATA.values():
                    all_kws.extend(category_list)
                MENTAL_KEYWORDS = list(set(all_kws))
                STOP_WORDS = data.get("stop_words", [])
            print(f"[load] âœ… åˆ†é¡è¼‰å…¥æˆåŠŸã€‚å…± {len(KEYWORDS_DATA)} å€‹é¡åˆ¥ã€‚")
    except Exception as e:
        print(f"[load] âŒ åˆ†é¡è¼‰å…¥å¤±æ•—: {e}")

load_keywords_from_json()

# è«‹ç¢ºä¿å…¨åŸŸè®Šæ•¸å®£å‘ŠåŒ…å« VECTOR_CACHE
# VECTOR_CACHE = {} 

def init_vector_model():
    global VECTOR_CACHE, JINA_API_KEY
    
    # ä½ çš„ API KEY (å»ºè­°ä¹‹å¾Œé‚„æ˜¯æ›æˆç’°å¢ƒè®Šæ•¸æ¯”è¼ƒå®‰å…¨)
    JINA_API_KEY = os.environ.get("JINA_API_KEY")

    if not JINA_API_KEY:
        print("[init] âš ï¸ è­¦å‘Šï¼šæ‰¾ä¸åˆ° JINA_API_KEYï¼Œèªæ„æœå°‹å°‡ç„¡æ³•é‹ä½œï¼")
    else:
        print("[init] âœ… Jina API Key å·²è¨­å®š")

    print("[init] ğŸš€ æ­£åœ¨åˆå§‹åŒ–å¤šæ¨¡å‹ç³»çµ±...")
    
    # åˆå§‹åŒ–å¿«å–å­—å…¸
    VECTOR_CACHE = {} 

    # è¿´åœˆè®€å– MODEL_CONFIGS è£¡é¢çš„æ¯ä¸€çµ„è¨­å®š
    for key, config in MODEL_CONFIGS.items():
        fname = Path(config["vector_filename"])
        expected_dim = config["dimensions"]
        
        if fname.exists():
            try:
                print(f"   Using > æ­£åœ¨è¼‰å…¥ [{key}] å‘é‡æª”: {fname} ...")
                
                with open(fname, "r", encoding="utf-8") as f:
                    data = json.load(f)
                    matrix = np.array(data, dtype="float32")
                
                # é˜²å‘†æª¢æŸ¥ï¼šæª¢æŸ¥ç¶­åº¦æ˜¯å¦æ­£ç¢º
                current_dim = matrix.shape[1] if len(matrix) > 0 else 0
                if current_dim != expected_dim:
                    print(f"   âš ï¸ è­¦å‘Šï¼š[{key}] æª”æ¡ˆç¶­åº¦ ({current_dim}) èˆ‡è¨­å®š ({expected_dim}) ä¸ç¬¦ï¼å¯èƒ½éœ€è¦é‡æ–°ç”Ÿæˆã€‚")
                
                # å­˜å…¥å¿«å–
                VECTOR_CACHE[key] = matrix
                print(f"   âœ… [{key}] è¼‰å…¥æˆåŠŸ (å…± {len(matrix)} ç­†, ç¶­åº¦ {current_dim})")
                
            except Exception as e:
                print(f"   âŒ [{key}] è®€å–å¤±æ•—: {e}")
        else:
            print(f"   âš ï¸ [{key}] æ‰¾ä¸åˆ°æª”æ¡ˆ {fname}ï¼Œè·³éæ­¤ç‰ˆæœ¬ã€‚")
            
    print(f"[init] å®Œæˆï¼å…±è¼‰å…¥ {len(VECTOR_CACHE)} å€‹æ¨¡å‹ç‰ˆæœ¬ã€‚\n")

def get_jina_embedding(text, model_name):
    if not JINA_API_KEY:
        raise Exception("JINA_API_KEY not set")
    
    headers = { "Content-Type": "application/json", "Authorization": f"Bearer {JINA_API_KEY}" }
    
    payload = { 
        "model": model_name, 
        "input": [text] 
    }
    
    # v3 å’Œ v4 å»ºè­°åŠ ä¸Š task åƒæ•¸
    if "v3" in model_name or "v4" in model_name:
        payload["task"] = "retrieval.passage"

    try:
        resp = requests.post(JINA_API_URL, headers=headers, json=payload, timeout=10)
        resp.raise_for_status()
        return resp.json()["data"][0]["embedding"]
    except Exception as e:
        print(f"[Jina API Error] {e}")
        return None

def search_units_semantic(query: str, model_key: str, top_k: int = 5):
    # 1. å¾å…¨åŸŸå¿«å–ä¸­å–å¾—å°æ‡‰ç‰ˆæœ¬çš„å‘é‡çŸ©é™£
    # è«‹ç¢ºä¿ä½ æœ‰å®£å‘Š global VECTOR_CACHE
    corpus = VECTOR_CACHE.get(model_key)
    
    if corpus is None:
        print(f"[search] éŒ¯èª¤ï¼šæ‰¾ä¸åˆ°ç‰ˆæœ¬ {model_key} çš„å‘é‡è³‡æ–™")
        return []
    
    config = MODEL_CONFIGS.get(model_key)
    if not config: return []

    try:
        # 2. å‘¼å«æŒ‡å®šç‰ˆæœ¬çš„ API å–å¾— Query Vector
        # æ³¨æ„ï¼šé€™è£¡æœƒå‘¼å« get_jina_embeddingï¼Œå‚³å…¥å°æ‡‰çš„æ¨¡å‹åç¨± (ä¾‹å¦‚ jina-embeddings-v3)
        query_vec_list = get_jina_embedding(query, config["api_model_name"])
        
        if not query_vec_list: return []
        
        query_vec = np.array(query_vec_list, dtype="float32")
        
        # 3. è¨ˆç®—ç›¸ä¼¼åº¦ (çŸ©é™£é‹ç®—)
        scores = np.dot(corpus, query_vec)
        top_indices = np.argsort(scores)[-top_k:][::-1]
        
        results = []
        for idx in top_indices:
            score = float(scores[idx])
            # é–€æª»å€¼å¯ä»¥è‡ªå·±å¾®èª¿
            if score > 0.25: 
                r = dict(UNITS_CACHE[idx])
                r["_score"] = score
                r["_best_segment"] = None
                results.append(r)
        return results
    except Exception as e:
        print(f"[search] å‘é‡æœå°‹ç™¼ç”ŸéŒ¯èª¤: {e}")
        return []
    
def detect_pagination_intent(q: str) -> bool:
    q = q.lower().strip()
    keywords = [
        "çµ¦æˆ‘å¾Œäº”å€‹", "çµ¦æˆ‘ä¸‹äº”å€‹", "å¾Œäº”å€‹", "ä¸‹äº”å€‹", "ä¸‹ä¸€é ", "æ›´å¤šæ¨è–¦", 
        "next 5", "show me more", "more results", 
        "æ¬¡ã®5ä»¶", "ã‚‚ã£ã¨è¦‹ã‚‹", "ç¶šã", "æœ€å¾Œã®5ã¤", "æœ€å¾Œã®5ã¤ã‚’ãã ã•ã„"
    ]
    return any(kw in q for kw in keywords)

def extract_address_from_query(q: str) -> str:
    original = q
    if "é™„è¿‘" in q: q = q.split("é™„è¿‘")[0]
    for kw in ["å¿ƒæ“šé»", "é–€è¨º", "çœ‹è¨º"]:
        if kw in q: q = q.split(kw)[0]
    prefixes = ["æˆ‘ä½åœ¨", "æˆ‘ä½", "å®¶åœ¨", "å®¶ä½", "ä½åœ¨", "ä½", "åœ¨"]
    q = q.strip()
    for p in prefixes:
        if q.startswith(p):
            q = q[len(p):].strip()
            break
    tail_words = ["æœ‰æ²’æœ‰", "æœ‰å—", "å—", "å‘¢", "å•Š", "å•¦"]
    for t in tail_words:
        if q.endswith(t): q = q[: -len(t)].strip()
    q = q.strip(" ?ï¼Ÿ!")
    if len(q) < 4: return ""
    return q

def normalize_query(q: str):
    q = q.strip().lower()
    if not q: return [], [], []
    functional_words = ["æ–‡ç« ", "å½±ç‰‡", "æƒ³çœ‹", "çµ¦æˆ‘", "åªæœ‰", "åªæƒ³çœ‹", "æ¨è–¦", "å½±éŸ³", "æ’­æ”¾", "æŸ¥è©¢", "æ‰¾", "æœ‰å“ªäº›", "ä»‹ç´¹"]
    user_input_core = []
    category_expanded = []
    other_terms = []
    found_categories = set()
    for category, kws in KEYWORDS_DATA.items():
        for kw in kws:
            if kw in q:
                if kw not in user_input_core: user_input_core.append(kw)
                found_categories.add(category)
    for cat in found_categories:
        group_kws = KEYWORDS_DATA[cat]
        for kw in group_kws:
            if kw not in user_input_core and kw not in category_expanded: category_expanded.append(kw)
    temp_q = q
    for kw in user_input_core: temp_q = temp_q.replace(kw, " ") 
    for fw in functional_words: temp_q = temp_q.replace(fw, " ")
    parts = re.split(r"[ï¼Œã€‚ï¼!ï¼Ÿ?\sã€ï¼›;:ï¼š]+", temp_q)
    for part in parts:
        if len(part) >= 2 and part not in STOP_WORDS:
            if part not in other_terms: other_terms.append(part)
    return user_input_core, category_expanded, other_terms

def score_unit(unit, user_core, expanded_core, other_terms):
    title = (unit.get("section_title") or "") + (unit.get("title") or "")
    content = unit.get("content_text", "") or "" 
    if not title and not content: return 0.0, None
    score = 0.0
    for kw in user_core:
        if kw in title: score += 10.0
        cnt = content.count(kw)
        if cnt > 0: score += cnt * 4.0
    for kw in expanded_core:
        if kw in title: score += 5.0
        cnt = content.count(kw)
        if cnt > 0: score += cnt * 2.0
    for kw in other_terms:
        if kw in title: score += 1.0
        cnt = content.count(kw)
        if cnt > 0: score += cnt * 0.5
    subtitles = unit.get("subtitles", [])
    best_seg = None
    best_seg_score = 0
    has_core_list = []
    for seg in subtitles:
        seg_text = seg.get("text", "")
        hits = sum(1 for kw in user_core if kw in seg_text)
        if hits == 0: hits = sum(1 for kw in expanded_core if kw in seg_text) * 0.5 
        has_core = (hits > 0)
        has_core_list.append(has_core)
        if hits > best_seg_score:
            best_seg_score = hits
            best_seg = seg
    count_continuous_hits = 0
    if len(has_core_list) >= 3:
        for i in range(len(has_core_list) - 2):
            if has_core_list[i] and has_core_list[i+1] and has_core_list[i+2]: count_continuous_hits += 1
    score += count_continuous_hits * 2.0
    return score, best_seg

EP_TAG_RE = re.compile(r"(ï¼ˆä¸Šï¼‰|ï¼ˆä¸‹ï¼‰|\(ä¸Š\)|\(ä¸‹\)|ä¸Šç¯‡|ä¸‹ç¯‡|ä¸Šé›†|ä¸‹é›†)")
def get_episode_tag(title: str) -> Optional[str]:
    if not title: return None
    t = title.strip()
    if re.search(r"(ï¼ˆä¸Šï¼‰|\(ä¸Š\)|ä¸Šç¯‡|ä¸Šé›†)", t): return "ä¸Š"
    if re.search(r"(ï¼ˆä¸‹ï¼‰|\(ä¸‹\)|ä¸‹ç¯‡|ä¸‹é›†)", t): return "ä¸‹"
    return None

def get_base_key(section_title: str, title: str) -> str:
    s = (section_title or "").strip()
    t = (title or "").strip()
    t2 = EP_TAG_RE.sub("", t)
    t2 = re.sub(r"\s+", "", t2)
    s2 = re.sub(r"\s+", "", s)
    return f"{s2}||{t2}"

def reorder_episode_pairs(results: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    groups: Dict[str, Dict[str, Any]] = {}
    for idx, r in enumerate(results):
        key = get_base_key(r.get("section_title"), r.get("title"))
        score = float(r.get("_score", 0.0))
        g = groups.get(key)
        if g is None:
            groups[key] = { "items": [], "best_score": score, "first_idx": idx }
            g = groups[key]
        g["items"].append(r)
        if score > g["best_score"]: g["best_score"] = score
    def item_rank(r: Dict[str, Any]) -> int:
        tag = get_episode_tag(r.get("title") or "")
        if tag == "ä¸Š": return 0
        if tag == "ä¸‹": return 1
        return 2
    for g in groups.values():
        g["items"].sort(key=lambda r: (item_rank(r), -float(r.get("_score", 0.0))))
    ordered_groups = sorted(groups.values(), key=lambda g: (-g["best_score"], g["first_idx"]))
    out: List[Dict[str, Any]] = []
    for g in ordered_groups: out.extend(g["items"])
    return out

def format_time(seconds: float) -> str:
    s = int(seconds)
    h = s // 3600
    m = (s % 3600) // 60
    sec = s % 60
    if h > 0: return f"{h:02d}:{m:02d}:{sec:02d}"
    return f"{m:02d}:{sec:02d}"

def search_units(units: List[Dict[str, Any]], query: str, top_k: int = TOP_K):
    user_core, expanded_core, other_terms = normalize_query(query)
    if not user_core and len(query) >= 2: user_core = [query]
    if not user_core and not other_terms: return []
    results = []
    for u in units:
        score, best_seg = score_unit(u, user_core, expanded_core, other_terms)
        if score > 0:
            r = dict(u)
            r["_score"] = score
            r["_best_segment"] = best_seg
            results.append(r)
    results.sort(key=lambda x: x["_score"], reverse=True)
    return results

def load_xin_points() -> List[Dict[str, Any]]:
    try:
        data = json.loads(XIN_POINTS_FILE.read_text("utf-8"))
        return data.get("data", [])
    except Exception as e:
        print(f"[xin] âš ï¸ å¿ƒæ“šé»è¼‰å…¥å¤±æ•—ï¼š{e}")
        return []

def haversine_km(lon1, lat1, lon2, lat2) -> float:
    lon1, lat1, lon2, lat2 = map(radians, [lon1, lat1, lon2, lat2])
    dlon = lon2 - lon1
    dlat = lat2 - lat1
    a = sin(dlat/2)**2 + cos(lat1)*cos(lat2)*sin(dlon/2)**2
    c = 2 * asin(sqrt(a))
    return 6371 * c 

def geocode_address(address: str):
    if not address: return None
    def try_geocode(addr: str):
        url = "https://nominatim.openstreetmap.org/search"
        params = {"q": addr, "format": "json", "limit": 1}
        headers = {"User-Agent": "xin-bot/1.0"}
        try:
            r = requests.get(url, params=params, headers=headers, timeout=5)
            r.raise_for_status()
            data = r.json()
            if data:
                return float(data[0]["lat"]), float(data[0]["lon"])
        except Exception: pass
        return None

    res = try_geocode(address)
    if res: return res
    if "è‡º" in address:
        res = try_geocode(address.replace("è‡º", "å°"))
        if res: return res
    
    # æ¨¡ç³Šæœå°‹
    addr3 = re.sub(r"\d+è™Ÿ.*", "", address)
    if addr3 != address:
        res = try_geocode(addr3)
        if res: return res
    
    m = re.match(
        r"(å°åŒ—å¸‚|è‡ºåŒ—å¸‚|æ–°åŒ—å¸‚|æ¡ƒåœ’å¸‚|è‡ºä¸­å¸‚|å°ä¸­å¸‚|è‡ºå—å¸‚|å°å—å¸‚|é«˜é›„å¸‚|"
        r"åŸºéš†å¸‚|æ–°ç«¹å¸‚|å˜‰ç¾©å¸‚|æ–°ç«¹ç¸£|è‹—æ —ç¸£|å½°åŒ–ç¸£|å—æŠ•ç¸£|é›²æ—ç¸£|å˜‰ç¾©ç¸£|"
        r"å±æ±ç¸£|å®œè˜­ç¸£|èŠ±è“®ç¸£|è‡ºæ±ç¸£|å°æ±ç¸£|æ¾æ¹–ç¸£|é‡‘é–€ç¸£|é€£æ±Ÿç¸£)"
        r"(.+?(å€|å¸‚|é®|é„‰))",
        address
    )
    if m:
        addr6 = m.group(1) + m.group(2)
        res = try_geocode(addr6)
        if res: return res

    return None

def find_nearby_points(lat, lon, max_km=5, top_k=5):
    points = load_xin_points()
    results = []
    for p in points:
        if p.get("lat") and p.get("lon"):
            d = haversine_km(lon, lat, p["lon"], p["lat"])
            if d <= max_km: results.append((p, d))
    results.sort(key=lambda x: x[1])
    return results[:top_k]

def build_nearby_points_response(address: str, results):
    if not results:
        return {
            "type": "xin_points",
            "address": address,
            "points": [],
            "message": f"åœ¨ã€Œ{address}ã€5 å…¬é‡Œå…§æ²’æœ‰æ‰¾åˆ°å¿ƒæ“šé»"
        }

    points = []
    origin_encoded = urllib.parse.quote(address)

    for p, d in results:
        dest_address = p.get("address", "")
        dest_encoded = urllib.parse.quote(dest_address)
        map_url = f"https://www.google.com/maps/dir/?api=1&origin={origin_encoded}&destination={dest_encoded}&hl=zh-TW"
        points.append({
            "title": p.get("title"),
            "address": dest_address,
            "tel": p.get("tel"),
            "distance_km": round(d, 2),
            "map_url": map_url
        })

    return {
        "type": "xin_points",
        "address": address,
        "points": points
    }

def load_all_units() -> List[Dict[str, Any]]:
    data = json.loads(UNITS_FILE.read_text("utf-8"))
    raw_units = data.get("units", [])
    units = []
    for u in raw_units:
        u = dict(u)
        section_title = u.get("section_title") or ""
        subtitle_texts = " ".join(seg.get("text", "") for seg in u.get("subtitles", []) or [])
        content_text = u.get("content_text", "") or ""
        search_text = " ".join(s for s in [section_title, u.get("title") or "", content_text, subtitle_texts] if s)
        u["_search_text"] = search_text
        units.append(u)
    print(f"[load] âœ… å…±è¼‰å…¥ {len(units)} å€‹å–®å…ƒ")
    return units

# --- ä»‹é¢å›æ‡‰å»ºæ§‹ ---
def build_recommendations_response(query: str, results: List[Dict[str, Any]], 
                                   offset: int = 0, limit: int = TOP_K, 
                                   target_lang: str = "zh-TW"):
    
    # 1. UI æ¨¡æ¿
    ui = {}
    if target_lang == 'ja':
        ui = {
            "not_found": "æ¡ä»¶ã«åˆã†ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚ã€Œã‚¹ãƒˆãƒ¬ã‚¹ã€ã€ã€Œä¸çœ ã€ã€ã€Œä¸å®‰ã€ãªã©ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã§è©¦ã—ã¦ã¿ã¦ãã ã•ã„ã€‚",
            "found_msg": "ğŸ“š åˆè¨ˆ {total} ä»¶è¦‹ã¤ã‹ã‚Šã¾ã—ãŸï¼ˆğŸ¥ å‹•ç”» {v_count}ã€ğŸ“„ è¨˜äº‹ {a_count}ï¼‰\nè¡¨ç¤ºä¸­: {start}ï½{end} ä»¶\n\nã”ç›¸è«‡å†…å®¹ã«åŸºã¥ãã€ä»¥ä¸‹ã®ã‚³ãƒ¼ã‚¹/è¨˜äº‹ã‚’æ¤œç´¢ã—ã¾ã—ãŸï¼š",
            "hint_prefix": "ãƒ’ãƒ³ãƒˆ: ",
            "hint_default": "å­—å¹•ã«ç‰¹å®šã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã¯è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚æœ€åˆã‹ã‚‰ã”è¦§ãã ã•ã„ã€‚",
            "video_link": "ğŸ¥ ãƒªãƒ³ã‚¯: ",
            "more_btn": "ğŸ‘‰ ã€Œæ¬¡ã®5ä»¶ã€ã‚’ã‚¯ãƒªãƒƒã‚¯ã—ã¦ã‚‚ã£ã¨è¦‹ã‚‹"
        }
    elif target_lang == 'en':
        ui = {
            "not_found": "Currently no relevant courses found. Try keywords like: stress, insomnia...",
            "found_msg": "ğŸ“š Found {total} results (ğŸ¥ Video {v_count}, ğŸ“„ Article {a_count})\nShowing items {start}-{end}\n\nBased on your description, I found these courses/articles:",
            "hint_prefix": "Tips: ",
            "hint_default": "No specific keywords found in subtitles, you can watch from the beginning.",
            "video_link": "ğŸ¥ Link: ",
            "more_btn": "ğŸ‘‰ Click 'Next 5' for more"
        }
    elif target_lang == 'vi':
        ui = {
            "not_found": "Hiá»‡n khÃ´ng tÃ¬m tháº¥y ná»™i dung phÃ¹ há»£p. Báº¡n cÃ³ thá»ƒ thá»­ cÃ¡c tá»« khÃ³a nhÆ°: cÄƒng tháº³ng, máº¥t ngá»§, tráº§m cáº£m...",
            "found_msg": "ğŸ“š TÃ¬m tháº¥y {total} káº¿t quáº£ (ğŸ¥ Video {v_count}, ğŸ“„ BÃ i viáº¿t {a_count})\nÄang hiá»ƒn thá»‹ má»¥c {start}ï½{end}\n\nDá»±a trÃªn mÃ´ táº£ cá»§a báº¡n, tÃ´i Ä‘Ã£ tÃ¬m tháº¥y cÃ¡c khÃ³a há»c / bÃ i viáº¿t nÃ y:",
            "hint_prefix": "ğŸ’¡ Gá»£i Ã½:",
            "hint_default": "KhÃ´ng tÃ¬m tháº¥y tá»« khÃ³a cá»¥ thá»ƒ trong phá»¥ Ä‘á», báº¡n cÃ³ thá»ƒ xem tá»« Ä‘áº§u.",
            "video_link": "ğŸ¥ Link video:",
            "more_btn": "ğŸ‘‰ Nháº¥n \"5 má»¥c tiáº¿p theo\" Ä‘á»ƒ xem thÃªm"
        }
    elif target_lang == 'ms':
        ui = {
            "not_found": "Tiada kursus berkaitan ditemui buat masa ini. Cuba kata kunci seperti: stres, insomnia, kemurungan...",
            "found_msg": "ğŸ“š Menjumpai {total} keputusan (ğŸ¥ Video {v_count}, ğŸ“„ Artikel {a_count})\nMenunjukkan item {start}ï½{end}\n\nBerdasarkan huraian anda, saya menemui kursus / artikel ini:",
            "hint_prefix": "ğŸ’¡ Tips:",
            "hint_default": "Tiada kata kunci khusus ditemui dalam sari kata, anda boleh tonton dari awal.",
            "video_link": "ğŸ¥ Pautan video:",
            "more_btn": "ğŸ‘‰ Klik \"5 Seterusnya\" untuk lihat lagi"
        }
    elif target_lang == 'zh-CN':
        ui = {
            "not_found": "ç›®å‰æ‰¾ä¸åˆ°å¾ˆç¬¦åˆçš„è¯¾ç¨‹ï¼Œå¯ä»¥è¯•ç€ç”¨ï¼šå©†åª³ã€å‹åŠ›ã€å¿§éƒã€å¤±çœ â€¦ ç­‰è¯å†è¯•è¯•çœ‹ã€‚",
            "found_msg": "ğŸ“š å…±æ‰¾åˆ° {total} ç¬”å†…å®¹ï¼ˆğŸ¥ è§†é¢‘ {v_count}ã€ğŸ“„ æ–‡ç«  {a_count}ï¼‰\nç›®å‰æ˜¾ç¤ºç¬¬ {start}ï½{end} ç¬”\n\næ ¹æ®ä½ çš„æè¿°ï¼Œæˆ‘å¸®ä½ æ‰¾äº†è¿™äº›è¯¾ç¨‹ / æ–‡ç« ï¼š",
            "hint_prefix": "ğŸ’¡ å°æé†’ï¼š",
            "hint_default": "å­—å¹•é‡Œæ²¡æœ‰ç‰¹åˆ«å‘½ä¸­å…³é”®å¥ï¼Œå¯ä»¥ä»å¤´å¼€å§‹çœ‹ã€‚",
            "video_link": "ğŸ¥ è§†é¢‘é“¾æ¥ï¼š",
            "more_btn": "ğŸ‘‰ ç‚¹å‡» â€œç»™æˆ‘åäº”ä¸ªâ€ æŸ¥çœ‹æ›´å¤š"
        }
    else:
        # é è¨­ä¸­æ–‡
        ui = {
            "not_found": "ç›®å‰æ‰¾ä¸åˆ°å¾ˆç¬¦åˆçš„èª²ç¨‹ï¼Œå¯ä»¥è©¦è‘—ç”¨ï¼šå©†åª³ã€å£“åŠ›ã€æ†‚é¬±ã€å¤±çœ â€¦ ç­‰è©å†è©¦è©¦çœ‹ã€‚",
            "found_msg": "ğŸ“š å…±æ‰¾åˆ° {total} ç­†å…§å®¹ï¼ˆğŸ¥ å½±ç‰‡ {v_count}ã€ğŸ“„ æ–‡ç«  {a_count}ï¼‰\nç›®å‰é¡¯ç¤ºç¬¬ {start}ï½{end} ç­†\n\næ ¹æ“šä½ çš„æ•˜è¿°ï¼Œæˆ‘å¹«ä½ æ‰¾äº†é€™äº›èª²ç¨‹ / æ–‡ç« ï¼š",
            "hint_prefix": "ğŸ’¡ å°æé†’ï¼š",
            "hint_default": "å­—å¹•è£¡æ²’æœ‰ç‰¹åˆ¥å‘½ä¸­é—œéµå¥ï¼Œå¯ä»¥å¾é ­é–‹å§‹çœ‹ã€‚",
            "video_link": "ğŸ¥ å½±ç‰‡é€£çµï¼š",
            "more_btn": "ğŸ‘‰ é»æ“Š ã€Œçµ¦æˆ‘å¾Œäº”å€‹ã€ å¯ä»¥çœ‹æ›´å¤š"
        }

    # å…¶ä»–èªè¨€å‹•æ…‹ç¿»è­¯
    if target_lang not in ['ja', 'en', 'zh-TW']:
        for k, v in ui.items():
            if "{total}" not in v:
                ui[k] = translate_text(v, target_lang)

    # 2. è™•ç†ç„¡çµæœ
    if not results:
        return {
            "type": "course_recommendation", "query": query, "total": 0, "video_count": 0, "article_count": 0,
            "offset": offset, "limit": limit, "has_more": False, "results": [],
            "message": ui["not_found"]
        }

    # 3. æ•¸æ“šè¨ˆç®—èˆ‡ Header
    results = reorder_episode_pairs(results)
    total = len(results)
    video_count = sum(1 for r in results if not r.get("is_article"))
    article_count = sum(1 for r in results if r.get("is_article"))
    page_results = results[offset: offset + limit]
    
    start_idx = offset + 1
    end_idx = min(offset + limit, total)
    
    header_msg = ui["found_msg"].format(
        total=total, v_count=video_count, a_count=article_count,
        start=start_idx, end=end_idx
    )

    items = []
    
    # 4. é€ç­†è™•ç†
    for r in page_results:
        raw_title = r.get("title") or "(ç„¡æ¨™é¡Œ)"
        raw_section = r.get("section_title") or ""
        
        # æ¨™é¡Œç¿»è­¯èˆ‡æ ¼å¼
        if target_lang != "zh-TW":
            pre_trans_title = raw_title
            
            # [è£œä¸] é‡å°æ—¥æ–‡çš„é è™•ç†å­—å…¸
            if target_lang == 'ja':
                replacements = {
                    "éŠ€é«®æ—": "é«˜é½¢è€…", "å¥½çœ ": "å¿«çœ ", "ç¡çœ éšœç¤™": "ç¡çœ éšœå®³",
                    "å›°æ“¾": "æ‚©ã¿", "è™•æ–¹": "å‡¦æ–¹", "ç­†è¨˜": "ãƒãƒ¼ãƒˆ",
                    "å¦‚ä½•": "ã„ã‹ã«ã—ã¦", "è·äºº": "ãƒ—ãƒ­", "è‡¨åºŠå¿ƒç†å¸«": "è‡¨åºŠå¿ƒç†å£«",
                    "é†«å¸«": "åŒ»å¸«", "æ•™æˆ": "å…ˆç”Ÿ", "å½±ç‰‡": "å‹•ç”»", "æ–‡ç« ": "è¨˜äº‹",
                    "ï¼ˆä¸Šï¼‰": "ï¼ˆå‰ç·¨ï¼‰", "ï¼ˆä¸‹ï¼‰": "ï¼ˆå¾Œç·¨ï¼‰", "èˆ‡": "ã¨", "çš„": "ã®",
                    # æ“´å……
                    "ç”Ÿç†æœŸ": "ç”Ÿç†", "æ¨‚é½¡": "ã‚·ãƒ‹ã‚¢", "ä¹Ÿèƒ½": "ã‚‚", "å¥½å¥½": "ã¡ã‚ƒã‚“ã¨",
                    "è¨ºæ²»": "è¨ºæ–­ãƒ»æ²»ç™‚", "ç–¾æ‚£": "ç—…æ°—", "åŠ›é‡": "åŠ›", "ä¿å¥": "å¥åº·",
                    "ç¿’æ…£": "ç¿’æ…£", "ç¸½æ˜¯": "ã„ã¤ã‚‚", "ç¡ä¸å¥½": "ã‚ˆãçœ ã‚Œãªã„",
                    "æ“æœ‰": "æŒã¤", "ç§˜è¨£": "ç§˜è¨£", "ç–²ç´¯": "ç–²ã‚Œ", "é’å°‘å¹´": "é’å°‘å¹´",
                    "å½±éŸ¿": "å½±éŸ¿", "çŸ¥å¤šå°‘": "çŸ¥ã£ã¦ã„ã¾ã™ã‹",
                    "åˆ¥å®³æ€•": "æ€–ãŒã‚‰ãªã„ã§", "è€å¹´": "è€å¹´", "ç‰¹è‰²": "ç‰¹å¾´",
                    "é©åº¦": "é©åº¦ãª", "æ¸›è¼•": "è»½æ¸›", "é—œç¯€ç‚": "é–¢ç¯€ç‚", "æƒ…ç·’": "æ°—åˆ†"
                }
                for zh_term, ja_term in replacements.items():
                    pre_trans_title = pre_trans_title.replace(zh_term, ja_term)
                
                pre_trans_title = pre_trans_title.replace("ã€", "[").replace("ã€‘", "] ")

            trans_title = translate_text(pre_trans_title, target_lang)
            
            if trans_title and len(trans_title) > 2 and trans_title != raw_title:
                display_title = f"{raw_title}\n{trans_title}"
            else:
                display_title = raw_title
            
            if raw_section:
                trans_section = translate_text(raw_section, target_lang)
                if trans_section and trans_section != raw_section:
                    display_section = f"{raw_section} / {trans_section}"
                else:
                    display_section = raw_section
            else:
                display_section = ""
        else:
            display_title = raw_title
            display_section = raw_section

        score = r.get("_score", 0.0)
        is_article = bool(r.get("is_article"))
        youtube_url = r.get("youtube_url")

        entry = {
            "section_title": display_section, 
            "title": display_title, 
            "score": score,
            "is_article": is_article, 
            "type": "article" if is_article else "video",
        }

        if is_article:
            content_text = (r.get("content_text") or "").replace("\n", " ")
            snippet_raw = content_text[:100] + "..."
            entry["article_url"] = r.get("article_url") or r.get("url")
            
            if target_lang != "zh-TW":
                trans_snippet = translate_text(snippet_raw, target_lang)
                entry["snippet"] = trans_snippet
            else:
                entry["snippet"] = snippet_raw     
        else:
            seg = r.get("_best_segment")
            if seg:
                start_str = format_time(seg.get("start_sec", 0.0))
                seg_text = seg.get('text', '')[:30]
                
                if target_lang != "zh-TW":
                    trans_seg = translate_text(seg_text, target_lang)
                    if target_lang == "ja":
                        hint_body = f"{start_str} ã«ã¦è¨€åŠ: ã€Œ{trans_seg}...ã€"
                    else:
                        hint_body = f"Mentioned at {start_str}: \"{trans_seg}...\""
                else:
                    hint_body = f"è©²å–®å…ƒåœ¨ {start_str} æœ‰æåˆ°ï¼šã€Œ{seg_text}...ã€"
            else:
                hint_body = ui["hint_default"]
            
            entry["hint"] = f"{ui['hint_prefix']} {hint_body}"
            entry["youtube_url"] = youtube_url
            entry["link_label"] = ui["video_link"] 

        items.append(entry)
    
    # Debug tag
    debug_lang = f" (Debug: UI={target_lang})" if target_lang != 'zh-TW' else ""
    
    return {
        "type": "course_recommendation", 
        "query": query, 
        "total": total,
        "video_count": video_count, 
        "article_count": article_count,
        "offset": offset, 
        "limit": limit, 
        "has_more": offset + limit < total,
        "results": items,
        "header_text": header_msg, 
        "message": (ui["more_btn"] if (offset + limit < total) else "") + debug_lang
    }

def execute_hybrid_search(search_query: str, model_key: str = "v3") -> List[Dict[str, Any]]:
    # é˜²å‘†ï¼šå¦‚æœå‚³é€²ä¾†çš„ key ä¸åœ¨å¿«å–è£¡ (ä¾‹å¦‚å‰ç«¯äº‚å‚³)ï¼Œå°±é è¨­å› v3
    if model_key not in VECTOR_CACHE:
        print(f"[hybrid] âš ï¸ è«‹æ±‚çš„æ¨¡å‹ {model_key} ä¸å­˜åœ¨ï¼Œåˆ‡æ›å› v3")
        model_key = "v3"
        # å¦‚æœé€£ v3 éƒ½æ²’æœ‰ï¼Œå°±éš¨ä¾¿æŠ“ä¸€å€‹ï¼Œé¿å…å ±éŒ¯
        if "v3" not in VECTOR_CACHE and VECTOR_CACHE:
             model_key = list(VECTOR_CACHE.keys())[0]

    print(f"[hybrid] é–‹å§‹æœå°‹: {search_query} | ä½¿ç”¨æ¨¡å‹: {model_key}")
    
    # 1. é—œéµå­—æœå°‹ (é€™éƒ¨åˆ†ä¸å—æ¨¡å‹ç‰ˆæœ¬å½±éŸ¿)
    kw_results = search_units(UNITS_CACHE, search_query, top_k=9999)
    
    # 2. èªæ„æœå°‹ (â˜…é—œéµä¿®æ”¹ï¼šå‚³å…¥ model_key)
    vec_results = search_units_semantic(search_query, model_key, top_k=50)
    
    # 3. æ··åˆæœå°‹åŠ æ¬Šé‚è¼¯ (RRF æˆ– åŠ æ¬Šç›¸åŠ )
    combined_map = {}
    
    # å…ˆæ”¾å…¥é—œéµå­—çµæœ
    for r in kw_results:
        key = get_base_key(r.get("section_title"), r.get("title"))
        combined_map[key] = r

    # å†ç–ŠåŠ å‘é‡çµæœ
    for r in vec_results:
        key = get_base_key(r.get("section_title"), r.get("title"))
        
        # æ¬Šé‡è¨­å®š
        VECTOR_WEIGHT_BOOST = 20.0 
        VECTOR_WEIGHT_BASE = 10.0
        
        if key in combined_map:
            # å¦‚æœå…©é‚Šéƒ½æ‰¾åˆ°ï¼Œå¤§å¹…åŠ åˆ†
            combined_map[key]["_score"] += (r["_score"] * VECTOR_WEIGHT_BOOST)
        else:
            # å¦‚æœåªæœ‰å‘é‡æ‰¾åˆ°ï¼Œçµ¦äºˆåŸºç¤åˆ†
            if r["_score"] > 0.25: 
                r["_score"] = r["_score"] * VECTOR_WEIGHT_BASE
                combined_map[key] = r
    
    final_results = list(combined_map.values())
    final_results.sort(key=lambda x: x["_score"], reverse=True)
    return final_results

app = FastAPI(title="å¿ƒå¿«æ´»èª²ç¨‹æ¨è–¦ API")

app.add_middleware(
    CORSMiddleware, allow_origins=["*"], allow_credentials=True, allow_methods=["*"], allow_headers=["*"],
)
app.mount("/static", StaticFiles(directory="static"), name="static")

@app.get("/", include_in_schema=False)
def serve_index():
    return FileResponse("static/index.html")

UNITS_CACHE = load_all_units()
init_vector_model()

HISTORY: Dict[str, List[Dict[str, Any]] ] = {}

class ChatRequest(BaseModel):
    query: str
    session_id: Optional[str] = None
    model: Optional[str] = "v3"  # æ–°å¢é€™å€‹æ¬„ä½ï¼Œé è¨­ v3

class NearbyRequest(BaseModel):
    address: str

class RecommendRequest(BaseModel):
    query: str

@app.get("/ping")
def ping(): return {"status": "ok"}

@app.post("/chat")
def chat(req: ChatRequest):
    start_time = time.time()

    # 1. åŸºç¤åƒæ•¸åˆå§‹åŒ–
    q_origin = req.query.strip()
    session_id = req.session_id or "anonymous"
    target_model = req.model or "v3"  # å–å¾—å‰ç«¯å‚³ä¾†çš„æ¨¡å‹é¸æ“‡
    
    history_list = HISTORY.get(session_id, [])
    is_pagination = detect_pagination_intent(q_origin)
    
    # 2. èªè¨€åµæ¸¬èˆ‡æ­·å²åå¥½
    # A. åµæ¸¬ç•¶å‰è¼¸å…¥
    current_detected = detect_language(q_origin)
    
    # B. æª¢æŸ¥æ­·å²åå¥½
    historical_lang = "zh-TW"
    if history_list:
        for h in reversed(history_list):
            lang = h.get("detected_lang", "zh-TW")
            if lang != "zh-TW":
                historical_lang = lang
                break
    
    # C. æ±ºç­–é‚è¼¯
    final_lang = "zh-TW"
    if current_detected != "zh-TW":
        final_lang = current_detected
    elif is_pagination and historical_lang != "zh-TW":
        final_lang = historical_lang
    else:
        final_lang = "zh-TW"

    print(f">>> [/chat] Origin: {q_origin} | Detected: {current_detected} | History: {historical_lang} -> Final: {final_lang}")

    # 3. ç¿»è­¯èˆ‡å‰è™•ç†
    if final_lang != "zh-TW":
        q_search = translate_text(q_origin, "zh-TW")
    else:
        q_search = q_origin

    def detect_media_preference(text: str) -> Optional[str]:
        if any(w in text for w in ["æƒ³çœ‹æ–‡ç« ", "çµ¦æˆ‘æ–‡ç« ", "åªæœ‰æ–‡ç« ", "æ–‡ç« æ¨è–¦", "æ‰¾æ–‡ç« ", "åªæƒ³çœ‹æ–‡ç« "]): return "article"
        if any(w in text for w in ["æƒ³çœ‹å½±ç‰‡", "çµ¦æˆ‘å½±ç‰‡", "æ’­æ”¾å½±ç‰‡", "å½±éŸ³", "çœ‹å½±ç‰‡", "youtube", "åªæƒ³çœ‹å½±ç‰‡"]): return "video"
        return None

    media_pref_check = detect_media_preference(q_search)
    q_cleaned = q_search

    if media_pref_check == "article":
        for w in ["æƒ³çœ‹æ–‡ç« ", "çµ¦æˆ‘æ–‡ç« ", "åªæœ‰æ–‡ç« ", "æ–‡ç« æ¨è–¦", "æ‰¾æ–‡ç« ", "åªæƒ³çœ‹æ–‡ç« ", "æ–‡ç« "]: q_cleaned = q_cleaned.replace(w, "")
    elif media_pref_check == "video":
        for w in ["æƒ³çœ‹å½±ç‰‡", "çµ¦æˆ‘å½±ç‰‡", "æ’­æ”¾å½±ç‰‡", "å½±éŸ³", "çœ‹å½±ç‰‡", "youtube", "åªæƒ³çœ‹å½±ç‰‡", "å½±ç‰‡"]: q_cleaned = q_cleaned.replace(w, "")
    q_cleaned = q_cleaned.strip()
    
    user_core, _, _ = normalize_query(q_cleaned)
    if not user_core and len(q_cleaned) >= 2: user_core = [q_cleaned]

    resp = {}

    # 4. æ„åœ–è·¯ç”± (Routing)
    
    # Case A: åœ°å€æŸ¥è©¢
    if ("é™„è¿‘" in q_search) and ("å¿ƒæ“šé»" in q_search or "çœ‹è¨º" in q_search or "é–€è¨º" in q_search):
        addr = extract_address_from_query(q_search)
        if not addr: 
            msg = "æˆ‘æœ‰é»æŠ“ä¸åˆ°åœ°å€ï¼Œè«‹å˜—è©¦è¼¸å…¥å®Œæ•´åœ°å€"
            if final_lang != "zh-TW": msg = translate_text(msg, final_lang)
            resp = {"type": "xin_points", "address": None, "points": [], "message": msg}
        else:
            geo = geocode_address(addr)
            if not geo: 
                msg = f"æŸ¥ä¸åˆ°ã€Œ{addr}ã€é€™å€‹åœ°å€"
                if final_lang != "zh-TW": msg = translate_text(msg, final_lang)
                resp = {"type": "xin_points", "address": addr, "points": [], "message": msg}
            else:
                lat, lon = geo
                results = find_nearby_points(lat, lon, max_km=5, top_k=TOP_K)
                resp = build_nearby_points_response(addr, results)

    elif ADDR_HEAD_RE.match(q_search):
        geo = geocode_address(q_search)
        if not geo: 
            msg = f"æŸ¥ä¸åˆ°ã€Œ{q_search}ã€é€™å€‹åœ°å€"
            if final_lang != "zh-TW": msg = translate_text(msg, final_lang)
            resp = {"type": "xin_points", "address": q_search, "points": [], "message": msg}
        else:
            lat, lon = geo
            results = find_nearby_points(lat, lon, max_km=5, top_k=TOP_K)
            resp = build_nearby_points_response(q_search, results)

    # Case B: åˆ†é æŒ‡ä»¤ (ä¸‹ä¸€é )
    elif detect_pagination_intent(q_search):
        if not history_list:
            msg = "ç›®å‰æ²’æœ‰ä¸Šä¸€ç­†æ¨è–¦çµæœï¼Œå¯ä»¥å…ˆå•ä¸€å€‹å•é¡Œ ğŸ˜Š"
            if final_lang != "zh-TW": msg = translate_text(msg, final_lang)
            resp = {"type": "text", "message": msg}
        else:
            last_recommendation = next((h for h in reversed(history_list) if isinstance(h.get("response"), dict) and h["response"].get("type") == "course_recommendation"), None)
            
            if not last_recommendation:
                 msg = "ç›®å‰æ²’æœ‰ä¸Šä¸€ç­†æ¨è–¦çµæœï¼Œå¯ä»¥å…ˆå•ä¸€å€‹å•é¡Œ ğŸ˜Š"
                 if final_lang != "zh-TW": msg = translate_text(msg, final_lang)
                 resp = {"type": "text", "message": msg}
            else:
                prev_resp = last_recommendation["response"]
                prev_query = prev_resp.get("query_raw") or prev_resp.get("query")
                prev_filter = prev_resp.get("filter_type", None)
                new_offset = prev_resp["offset"] + prev_resp["limit"]
                
                # é€™è£¡ä¹Ÿè¦åŠ ä¸Š model_key
                full_results = execute_hybrid_search(prev_query, model_key=target_model)
                
                if prev_filter == "article": full_results = [r for r in full_results if r.get("is_article")]
                elif prev_filter == "video": full_results = [r for r in full_results if not r.get("is_article")]
                
                resp = build_recommendations_response(
                    prev_query, full_results, offset=new_offset, limit=TOP_K, 
                    target_lang=final_lang
                )
                resp["filter_type"] = prev_filter
                resp["query_raw"] = prev_query

    # Case C: åªæœ‰åª’é«”åå¥½ä¿®æ­£ (ä¾‹å¦‚ç”¨æˆ¶åªèªª "åªæƒ³çœ‹å½±ç‰‡")
    elif media_pref_check and not q_cleaned:
        if not history_list:
            msg = "è«‹å…ˆè¼¸å…¥ä¸€å€‹ä¸»é¡Œï¼Œä¾‹å¦‚ã€Œç„¦æ…®ã€æˆ–ã€Œå¤±çœ ã€ã€‚"
            if final_lang != "zh-TW": msg = translate_text(msg, final_lang)
            resp = {"type": "course_recommendation", "query": q_search, "total": 0, "video_count": 0, "article_count": 0, "offset": 0, "limit": TOP_K, "has_more": False, "results": [], "message": msg}
        else:
            last = next((h for h in reversed(history_list) if isinstance(h.get("response"), dict) and h["response"].get("type") == "course_recommendation"), None)
            if not last:
                 msg = "è«‹å…ˆè¼¸å…¥ä¸€å€‹ä¸»é¡Œï¼Œä¾‹å¦‚ã€Œç„¦æ…®ã€æˆ–ã€Œå¤±çœ ã€ã€‚"
                 if final_lang != "zh-TW": msg = translate_text(msg, final_lang)
                 resp = {"type": "course_recommendation", "query": q_search, "total": 0, "video_count": 0, "article_count": 0, "offset": 0, "limit": TOP_K, "has_more": False, "results": [], "message": msg}
            else:
                prev_resp = last["response"]
                original_topic = prev_resp.get("query_raw") or prev_resp.get("query")
                
                # é€™è£¡ä¹Ÿè¦åŠ ä¸Š model_key
                full_results = execute_hybrid_search(original_topic, model_key=target_model)
                
                if media_pref_check == "article": full_results = [r for r in full_results if r.get("is_article")]
                elif media_pref_check == "video": full_results = [r for r in full_results if not r.get("is_article")]
                
                resp = build_recommendations_response(
                    original_topic, full_results, offset=0, limit=TOP_K, 
                    target_lang=final_lang
                )
                resp["filter_type"] = media_pref_check
                resp["query_raw"] = original_topic
                
                if not resp["results"]: 
                    msg = f"é—œæ–¼ã€Œ{original_topic}ã€ç›®å‰æ²’æœ‰ç›¸é—œçš„å…§å®¹ã€‚" 
                    if final_lang != "zh-TW": msg = translate_text(msg, final_lang)
                    resp["message"] = msg

    # Case D: ä¸€èˆ¬æœå°‹ (é€™æ˜¯ä½ åŸæœ¬å ±éŒ¯çš„åœ°æ–¹ï¼Œç¾åœ¨ä¿®å¥½äº†)
    else:
        # 1. ç¢ºä¿ search_q æœ‰å€¼
        search_q = q_cleaned if q_cleaned else q_search
        
        # 2. åŸ·è¡Œæœå°‹ (å‚³å…¥ model_key)
        full_results = execute_hybrid_search(search_q, model_key=target_model)
        
        final_filter = None
        if media_pref_check == "article":
            full_results = [r for r in full_results if r.get("is_article")]
            final_filter = "article"
        elif media_pref_check == "video":
            full_results = [r for r in full_results if not r.get("is_article")]
            final_filter = "video"
        
        resp = build_recommendations_response(
            q_origin, 
            full_results, 
            offset=0, 
            limit=TOP_K, 
            target_lang=final_lang
        )
        resp["filter_type"] = final_filter
        
        resp["query_raw"] = search_q 
        
        resp["detected_lang"] = final_lang
        resp["query_search_zh"] = search_q

        if media_pref_check and not resp["results"]: 
            msg = f"é—œæ–¼ã€Œ{search_q}ã€ç›®å‰æ²’æœ‰ç›¸é—œçš„å…§å®¹ã€‚"
            if final_lang != "zh-TW": msg = translate_text(msg, final_lang)
            resp["message"] = msg

    # 5. å¾Œè™•ç† (å„²å­˜æ­·å²ã€è¨ˆç®—æ™‚é–“)
    
    # å›å‚³ä½¿ç”¨çš„æ¨¡å‹è³‡è¨Š (æ–¹ä¾¿å‰ç«¯é¡¯ç¤º)
    resp["used_model"] = target_model

    end_time = time.time()
    execution_time = end_time - start_time
    resp["process_time"] = f"{execution_time:.4f}s"

    print(f"DEBUG: è¨ˆç®—è€—æ™‚: {resp['process_time']} | Model: {target_model} | Keys: {list(resp.keys())}")

    # å„²å­˜åˆ°æ­·å²ç´€éŒ„
    history_list = HISTORY.setdefault(session_id, [])
    history_list.append({
        "query": q_origin, 
        "response": resp, 
        "detected_lang": final_lang
    })
    if len(history_list) > 50: history_list.pop(0)

    return resp

@app.get("/history")
def get_history(session_id: str):
    return { "items": HISTORY.get(session_id, []) }

@app.post("/nearby")
def nearby(req: NearbyRequest):
    start_time = time.time()
    addr = req.address.strip()
    resp = {}
    if not addr: 
        return {"type": "xin_points", "address": None, "points": [], "message": "è«‹æä¾›å®Œæ•´åœ°å€"}
    else:
        geo = geocode_address(addr)
        if not geo: 
            resp = {"type": "xin_points", "address": addr, "points": [], "message": f"æŸ¥ä¸åˆ°ã€Œ{addr}ã€é€™å€‹åœ°å€"}
        else:
            results = find_nearby_points(geo[0], geo[1], max_km=5, top_k=TOP_K)
            resp = build_nearby_points_response(addr, results)
    
    end_time = time.time()
    resp["process_time"] = f"{end_time - start_time:.3f}s"

    return resp

@app.post("/recommend")
def recommend(req: RecommendRequest):
    start_time = time.time()

    q = req.query.strip()
    sid = "anonymous" 
    pref = None
    if any(w in q for w in ["æ–‡ç« "]): pref = "article"
    elif any(w in q for w in ["å½±ç‰‡"]): pref = "video"
    
    full_results = execute_hybrid_search(q)
    
    if pref == "article": full_results = [r for r in full_results if r.get("is_article")]
    elif pref == "video": full_results = [r for r in full_results if not r.get("is_article")]

    resp = build_recommendations_response(q, full_results, offset=0, limit=TOP_K)
    
    end_time = time.time()
    resp["process_time"] = f"{end_time - start_time:.3f}s"

    history_list = HISTORY.setdefault(sid, [])
    history_list.append({"query": q, "response": resp})
    return resp

if __name__ == "__main__":
    import uvicorn
    uvicorn.run("xin_api:app", host="0.0.0.0", port=8000, reload=True)