import json
import re
import os
import requests
from pathlib import Path
from typing import List, Dict, Any
import numpy as np
from math import radians, sin, cos, asin, sqrt
import urllib.parse

from fastapi import FastAPI
from pydantic import BaseModel
from fastapi.middleware.cors import CORSMiddleware
from fastapi.staticfiles import StaticFiles
from fastapi.responses import FileResponse
from typing import Optional

from langdetect import detect, LangDetectException
from deep_translator import GoogleTranslator

# --- å¸¸æ•¸è¨­å®š ---
CITY_PATTERN = (
    r"(å°åŒ—å¸‚|è‡ºåŒ—å¸‚|æ–°åŒ—å¸‚|æ¡ƒåœ’å¸‚|è‡ºä¸­å¸‚|å°ä¸­å¸‚|è‡ºå—å¸‚|å°å—å¸‚|é«˜é›„å¸‚|"
    r"åŸºéš†å¸‚|æ–°ç«¹å¸‚|å˜‰ç¾©å¸‚|æ–°ç«¹ç¸£|è‹—æ —ç¸£|å½°åŒ–ç¸£|å—æŠ•ç¸£|é›²æ—ç¸£|å˜‰ç¾©ç¸£|"
    r"å±æ±ç¸£|å®œè˜­ç¸£|èŠ±è“®ç¸£|è‡ºæ±ç¸£|å°æ±ç¸£|æ¾æ¹–ç¸£|é‡‘é–€ç¸£|é€£æ±Ÿç¸£)"
)
ADDR_HEAD_RE = re.compile(rf"^{CITY_PATTERN}(.*?(å€|é„‰|é®|å¸‚))")
TOP_K = 5  

XIN_POINTS_FILE = Path("xin_points.json")
UNITS_FILE = Path("wellbeing_elearn_pro_all_with_articles.json")

VECTORS_FILE = Path("vectors.json")
CORPUS_VECTORS = None 

JINA_API_URL = "https://api.jina.ai/v1/embeddings"
JINA_API_KEY = None

KEYWORDS_FILE = Path("keywords.json")
KEYWORDS_DATA = {} 
MENTAL_KEYWORDS = [] 
STOP_WORDS = []

# ç¿»è­¯ç”¨å¿«å–
TRANSLATION_CACHE = {}

# --- æ ¸å¿ƒå·¥å…·å‡½å¼ ---

def detect_language(text: str) -> str:
    """
    èªè¨€åµæ¸¬æœ€çµ‚ç‰ˆï¼š
    1. å„ªå…ˆæª¢æŸ¥æ—¥æ–‡å‡å -> ja
    2. å„ªå…ˆæª¢æŸ¥éŸ“æ–‡è«ºæ–‡ -> ko
    3. æª¢æŸ¥æ¼¢å­— -> zh-TW
    4. ç´”è‹±æ•¸ -> en
    """
    if not text: return "zh-TW"
    
    # 1. [å„ªå…ˆ] æª¢æŸ¥æ—¥æ–‡ (å¹³å‡å \u3040-\u309f / ç‰‡å‡å \u30a0-\u30ff)
    if re.search(r'[\u3040-\u309f\u30a0-\u30ff]', text):
        return "ja"

    # 2. [å„ªå…ˆ] æª¢æŸ¥éŸ“æ–‡ (è«ºæ–‡ \uac00-\ud7af)
    if re.search(r'[\uac00-\ud7af]', text):
        return "ko"

    # 3. æª¢æŸ¥ä¸­æ–‡ (æ¼¢å­— \u4e00-\u9fa5)
    if re.search(r'[\u4e00-\u9fa5]', text):
        return "zh-TW"

    # 4. æª¢æŸ¥ç´”è‹±æ–‡
    clean_text = re.sub(r'[0-9\s,.?!:;\'"()\[\]]', '', text)
    if clean_text and all(ord(c) < 128 for c in clean_text):
        return "en"

    # 5. å…¶ä»–æƒ…æ³äº¤çµ¦æ¨¡å‹
    try:
        lang = detect(text)
        if lang.startswith("zh"): return "zh-TW"
        return lang
    except LangDetectException:
        return "zh-TW"

def translate_text(text: str, target: str) -> str:
    """
    ç¿»è­¯å‡½å¼ (æ•´åˆæ™ºæ…§é‡è©¦æ©Ÿåˆ¶)ï¼š
    1. è‹¥æ˜¯æ—¥æ–‡ç›®æ¨™ï¼Œå…ˆå˜—è©¦ç§»é™¤ç¬¦è™Ÿå†ç¿» (æé«˜æº–ç¢ºåº¦)ã€‚
    2. æ•´åˆå¿«å–ã€‚
    """
    if not text: return ""
    # å¦‚æœç›®æ¨™æ˜¯ä¸­æ–‡ï¼Œä¸”åŸæ–‡å°±æ˜¯ä¸­æ–‡ (ç°¡å–®åˆ¤æ–·)ï¼Œç›´æ¥å›å‚³
    if target == "zh-TW" and detect_language(text) == "zh-TW":
        return text
    
    # å»ºç«‹å¿«å–éµå€¼
    cache_key = f"{text}_{target}"
    if cache_key in TRANSLATION_CACHE:
        return TRANSLATION_CACHE[cache_key]
    
    try:
        translator = GoogleTranslator(source='auto', target=target)
        
        # [å„ªåŒ–] é‡å°æ—¥æ–‡ç¿»è­¯ï¼Œç‚ºäº†é¿å… API æŠŠ "ã€å½±ç‰‡ã€‘" ç•¶ä½œä¸éœ€ç¿»è­¯çš„ç¬¦è™Ÿï¼Œ
        # æˆ‘å€‘å„ªå…ˆé€å‡ºä¹¾æ·¨çš„æ–‡å­—ï¼Œé€™æ¨£ "å¥³æ€§èˆ‡ç¡çœ éšœç¤™" çš„ "èˆ‡" æ¯”è¼ƒå®¹æ˜“è¢«ç¿»æˆ "ã¨"
        text_to_translate = text
        if target == 'ja':
             # ç§»é™¤å¸¸è¦‹å¹²æ“¾ç¬¦è™Ÿ
            clean_text = re.sub(r"[ã€ã€‘ã€Šã€‹ã€Œã€()ï¼ˆï¼‰]", " ", text)
            clean_text = re.sub(r"\s+", " ", clean_text).strip()
            if clean_text:
                text_to_translate = clean_text

        # åŸ·è¡Œç¿»è­¯
        result = translator.translate(text_to_translate)
        
        # é˜²å‘†ï¼šå¦‚æœç¿»è­¯å¤±æ•— (çµæœè·ŸåŸæ–‡å®Œå…¨ä¸€æ¨£ï¼Œä¸”é•·åº¦è¶³å¤ )
        # ä¸”æˆ‘å€‘é‚„æ²’è©¦éç§»é™¤ç¬¦è™Ÿ (å³éæ—¥æ–‡æ¨¡å¼)ï¼Œå‰‡å˜—è©¦ç§»é™¤ç¬¦è™Ÿé‡è©¦
        if result == text_to_translate and len(text) > 5 and target != 'ja':
             clean_text = re.sub(r"[ã€ã€‘ã€Šã€‹ã€Œã€()ï¼ˆï¼‰]", " ", text)
             clean_text = re.sub(r"\s+", " ", clean_text).strip()
             if clean_text != text:
                retry_result = translator.translate(clean_text)
                if retry_result != clean_text:
                    result = retry_result
        
        # å¯«å…¥å¿«å–
        TRANSLATION_CACHE[cache_key] = result
        return result

    except Exception as e:
        print(f"!!! [Translate Error] Text: {text[:10]}... | Error: {e}")
        return text # å¤±æ•—æ™‚å›å‚³åŸæ–‡

def load_keywords_from_json():
    global KEYWORDS_DATA, MENTAL_KEYWORDS, STOP_WORDS
    try:
        if KEYWORDS_FILE.exists():
            with open(KEYWORDS_FILE, "r", encoding="utf-8") as f:
                data = json.load(f)
                KEYWORDS_DATA = data.get("mental_keywords", {})
                all_kws = []
                for category_list in KEYWORDS_DATA.values():
                    all_kws.extend(category_list)
                MENTAL_KEYWORDS = list(set(all_kws))
                STOP_WORDS = data.get("stop_words", [])
            print(f"[load] âœ… åˆ†é¡è¼‰å…¥æˆåŠŸã€‚å…± {len(KEYWORDS_DATA)} å€‹é¡åˆ¥ã€‚")
    except Exception as e:
        print(f"[load] âŒ åˆ†é¡è¼‰å…¥å¤±æ•—: {e}")

load_keywords_from_json()

def init_vector_model():
    global CORPUS_VECTORS, JINA_API_KEY
    
    JINA_API_KEY = os.environ.get("JINA_API_KEY")
    if not JINA_API_KEY:
        print("[init] âš ï¸ è­¦å‘Šï¼šæ‰¾ä¸åˆ° JINA_API_KEYï¼Œèªæ„æœå°‹å°‡ç„¡æ³•é‹ä½œï¼")
    else:
        print("[init] âœ… Jina API Key å·²è¨­å®š")

    if VECTORS_FILE.exists():
        print(f"[init] æ­£åœ¨è®€å–å‘é‡å¿«å–: {VECTORS_FILE} ...")
        try:
            with open(VECTORS_FILE, "r", encoding="utf-8") as f:
                data = json.load(f)
                CORPUS_VECTORS = np.array(data, dtype="float32") 
            print(f"[init] âœ… æˆåŠŸè¼‰å…¥ {len(CORPUS_VECTORS)} ç­†å‘é‡è³‡æ–™")
        except Exception as e:
            print(f"[init] âŒ è®€å–å‘é‡æª”å¤±æ•—: {e}")
    else:
        print("[init] âš ï¸ æ‰¾ä¸åˆ° vectors.json")

def get_jina_embedding(text):
    """é€é requests å‘¼å« Jina API"""
    if not JINA_API_KEY:
        raise Exception("JINA_API_KEY not set")
        
    headers = {
        "Content-Type": "application/json",
        "Authorization": f"Bearer {JINA_API_KEY}"
    }
    data = {
        "model": "jina-embeddings-v3",
        "input": [text] 
    }
    
    try:
        resp = requests.post(JINA_API_URL, headers=headers, json=data, timeout=10)
        resp.raise_for_status()
        result = resp.json()
        return result["data"][0]["embedding"]
    except Exception as e:
        print(f"[Jina API Error] {e}")
        return None

def search_units_semantic(query: str, top_k: int = 5):
    global CORPUS_VECTORS
    
    if not JINA_API_KEY or CORPUS_VECTORS is None:
        return []

    try:
        query_vec_list = get_jina_embedding(query)
        if not query_vec_list:
            return []
            
        query_vec = np.array(query_vec_list, dtype="float32")

        scores = np.dot(CORPUS_VECTORS, query_vec)

        top_indices = np.argsort(scores)[-top_k:][::-1]
        
        results = []
        for idx in top_indices:
            score = float(scores[idx])
            if score > 0.25: 
                r = dict(UNITS_CACHE[idx])
                r["_score"] = score
                r["_best_segment"] = None
                results.append(r)
        return results

    except Exception as e:
        print(f"[search] å‘é‡æœå°‹ç™¼ç”ŸéŒ¯èª¤: {e}")
        return []

def detect_pagination_intent(q: str) -> bool:
    return any(w in q for w in ["çµ¦æˆ‘å¾Œäº”å€‹","çµ¦æˆ‘ä¸‹äº”å€‹","å¾Œäº”å€‹","ä¸‹äº”å€‹","ä¸‹ä¸€é ","æ›´å¤šæ¨è–¦"])

def extract_address_from_query(q: str) -> str:
    original = q
    if "é™„è¿‘" in q: q = q.split("é™„è¿‘")[0]
    for kw in ["å¿ƒæ“šé»", "é–€è¨º", "çœ‹è¨º"]:
        if kw in q: q = q.split(kw)[0]
    prefixes = ["æˆ‘ä½åœ¨", "æˆ‘ä½", "å®¶åœ¨", "å®¶ä½", "ä½åœ¨", "ä½", "åœ¨"]
    q = q.strip()
    for p in prefixes:
        if q.startswith(p):
            q = q[len(p):].strip()
            break
    tail_words = ["æœ‰æ²’æœ‰", "æœ‰å—", "å—", "å‘¢", "å•Š", "å•¦"]
    for t in tail_words:
        if q.endswith(t): q = q[: -len(t)].strip()
    q = q.strip(" ?ï¼Ÿ!")
    if len(q) < 4: return ""
    return q

def normalize_query(q: str):
    q = q.strip().lower()
    if not q: return [], [], []
    functional_words = ["æ–‡ç« ", "å½±ç‰‡", "æƒ³çœ‹", "çµ¦æˆ‘", "åªæœ‰", "åªæƒ³çœ‹", "æ¨è–¦", "å½±éŸ³", "æ’­æ”¾", "æŸ¥è©¢", "æ‰¾", "æœ‰å“ªäº›", "ä»‹ç´¹"]
    user_input_core = []
    category_expanded = []
    other_terms = []
    found_categories = set()
    for category, kws in KEYWORDS_DATA.items():
        for kw in kws:
            if kw in q:
                if kw not in user_input_core: user_input_core.append(kw)
                found_categories.add(category)
    for cat in found_categories:
        group_kws = KEYWORDS_DATA[cat]
        for kw in group_kws:
            if kw not in user_input_core and kw not in category_expanded: category_expanded.append(kw)
    temp_q = q
    for kw in user_input_core: temp_q = temp_q.replace(kw, " ") 
    for fw in functional_words: temp_q = temp_q.replace(fw, " ")
    parts = re.split(r"[ï¼Œã€‚ï¼!ï¼Ÿ?\sã€ï¼›;:ï¼š]+", temp_q)
    for part in parts:
        if len(part) >= 2 and part not in STOP_WORDS:
            if part not in other_terms: other_terms.append(part)
    return user_input_core, category_expanded, other_terms

def score_unit(unit, user_core, expanded_core, other_terms):
    title = (unit.get("section_title") or "") + (unit.get("title") or "")
    content = unit.get("content_text", "") or "" 
    if not title and not content: return 0.0, None
    score = 0.0
    for kw in user_core:
        if kw in title: score += 10.0
        cnt = content.count(kw)
        if cnt > 0: score += cnt * 4.0
    for kw in expanded_core:
        if kw in title: score += 5.0
        cnt = content.count(kw)
        if cnt > 0: score += cnt * 2.0
    for kw in other_terms:
        if kw in title: score += 1.0
        cnt = content.count(kw)
        if cnt > 0: score += cnt * 0.5
    subtitles = unit.get("subtitles", [])
    best_seg = None
    best_seg_score = 0
    has_core_list = []
    for seg in subtitles:
        seg_text = seg.get("text", "")
        hits = sum(1 for kw in user_core if kw in seg_text)
        if hits == 0: hits = sum(1 for kw in expanded_core if kw in seg_text) * 0.5 
        has_core = (hits > 0)
        has_core_list.append(has_core)
        if hits > best_seg_score:
            best_seg_score = hits
            best_seg = seg
    count_continuous_hits = 0
    if len(has_core_list) >= 3:
        for i in range(len(has_core_list) - 2):
            if has_core_list[i] and has_core_list[i+1] and has_core_list[i+2]: count_continuous_hits += 1
    score += count_continuous_hits * 2.0
    return score, best_seg

EP_TAG_RE = re.compile(r"(ï¼ˆä¸Šï¼‰|ï¼ˆä¸‹ï¼‰|\(ä¸Š\)|\(ä¸‹\)|ä¸Šç¯‡|ä¸‹ç¯‡|ä¸Šé›†|ä¸‹é›†)")
def get_episode_tag(title: str) -> Optional[str]:
    if not title: return None
    t = title.strip()
    if re.search(r"(ï¼ˆä¸Šï¼‰|\(ä¸Š\)|ä¸Šç¯‡|ä¸Šé›†)", t): return "ä¸Š"
    if re.search(r"(ï¼ˆä¸‹ï¼‰|\(ä¸‹\)|ä¸‹ç¯‡|ä¸‹é›†)", t): return "ä¸‹"
    return None

def get_base_key(section_title: str, title: str) -> str:
    s = (section_title or "").strip()
    t = (title or "").strip()
    t2 = EP_TAG_RE.sub("", t)
    t2 = re.sub(r"\s+", "", t2)
    s2 = re.sub(r"\s+", "", s)
    return f"{s2}||{t2}"

def reorder_episode_pairs(results: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    groups: Dict[str, Dict[str, Any]] = {}
    for idx, r in enumerate(results):
        key = get_base_key(r.get("section_title"), r.get("title"))
        score = float(r.get("_score", 0.0))
        g = groups.get(key)
        if g is None:
            groups[key] = { "items": [], "best_score": score, "first_idx": idx }
            g = groups[key]
        g["items"].append(r)
        if score > g["best_score"]: g["best_score"] = score
    def item_rank(r: Dict[str, Any]) -> int:
        tag = get_episode_tag(r.get("title") or "")
        if tag == "ä¸Š": return 0
        if tag == "ä¸‹": return 1
        return 2
    for g in groups.values():
        g["items"].sort(key=lambda r: (item_rank(r), -float(r.get("_score", 0.0))))
    ordered_groups = sorted(groups.values(), key=lambda g: (-g["best_score"], g["first_idx"]))
    out: List[Dict[str, Any]] = []
    for g in ordered_groups: out.extend(g["items"])
    return out

def format_time(seconds: float) -> str:
    s = int(seconds)
    h = s // 3600
    m = (s % 3600) // 60
    sec = s % 60
    if h > 0: return f"{h:02d}:{m:02d}:{sec:02d}"
    return f"{m:02d}:{sec:02d}"

def search_units(units: List[Dict[str, Any]], query: str, top_k: int = TOP_K):
    user_core, expanded_core, other_terms = normalize_query(query)
    if not user_core and len(query) >= 2: user_core = [query]
    if not user_core and not other_terms: return []
    results = []
    for u in units:
        score, best_seg = score_unit(u, user_core, expanded_core, other_terms)
        if score > 0:
            r = dict(u)
            r["_score"] = score
            r["_best_segment"] = best_seg
            results.append(r)
    results.sort(key=lambda x: x["_score"], reverse=True)
    return results

def load_xin_points() -> List[Dict[str, Any]]:
    try:
        data = json.loads(XIN_POINTS_FILE.read_text("utf-8"))
        return data.get("data", [])
    except Exception as e:
        print(f"[xin] âš ï¸ å¿ƒæ“šé»è¼‰å…¥å¤±æ•—ï¼š{e}")
        return []

def haversine_km(lon1, lat1, lon2, lat2) -> float:
    lon1, lat1, lon2, lat2 = map(radians, [lon1, lat1, lon2, lat2])
    dlon = lon2 - lon1
    dlat = lat2 - lat1
    a = sin(dlat/2)**2 + cos(lat1)*cos(lat2)*sin(dlon/2)**2
    c = 2 * asin(sqrt(a))
    return 6371 * c 

def geocode_address(address: str):
    if not address:
        return None

    def try_geocode(addr: str):
        url = "https://nominatim.openstreetmap.org/search"
        params = {"q": addr, "format": "json", "limit": 1}
        headers = {"User-Agent": "xin-bot/1.0"}
        try:
            r = requests.get(url, params=params, headers=headers, timeout=5)
            r.raise_for_status()
            data = r.json()
            if data:
                lat = float(data[0]["lat"])
                lon = float(data[0]["lon"])
                print(f"[geocode] å‘½ä¸­ï¼š'{addr}' -> lat={lat}, lon={lon}")
                return lat, lon
        except Exception as e:
            print(f"[geocode] éŒ¯èª¤ï¼š{e}")
        return None

    print(f"[geocode] å˜—è©¦ï¼š{address}")
    result = try_geocode(address)
    if result:
        return result

    if "è‡º" in address:
        addr2 = address.replace("è‡º", "å°")
        print(f"[geocode] å˜—è©¦ï¼š{addr2}")
        result = try_geocode(addr2)
        if result:
            return result

    addr3 = re.sub(r"\d+è™Ÿ.*", "", address)
    if addr3 != address:
        print(f"[geocode] å˜—è©¦ï¼ˆå»è™Ÿï¼‰ï¼š{addr3}")
        result = try_geocode(addr3)
        if result:
            return result

    addr4 = re.sub(r"\d+å¼„.*", "", address)
    if addr4 != address:
        print(f"[geocode] å˜—è©¦ï¼ˆå»å¼„ï¼‰ï¼š{addr4}")
        result = try_geocode(addr4)
        if result:
            return result

    addr5 = re.sub(r"\d+å··.*", "", address)
    if addr5 != address:
        print(f"[geocode] å˜—è©¦ï¼ˆå»å··ï¼‰ï¼š{addr5}")
        result = try_geocode(addr5)
        if result:
            return result

    m = re.match(
        r"(å°åŒ—å¸‚|è‡ºåŒ—å¸‚|æ–°åŒ—å¸‚|æ¡ƒåœ’å¸‚|è‡ºä¸­å¸‚|å°ä¸­å¸‚|è‡ºå—å¸‚|å°å—å¸‚|é«˜é›„å¸‚|"
        r"åŸºéš†å¸‚|æ–°ç«¹å¸‚|å˜‰ç¾©å¸‚|æ–°ç«¹ç¸£|è‹—æ —ç¸£|å½°åŒ–ç¸£|å—æŠ•ç¸£|é›²æ—ç¸£|å˜‰ç¾©ç¸£|"
        r"å±æ±ç¸£|å®œè˜­ç¸£|èŠ±è“®ç¸£|è‡ºæ±ç¸£|å°æ±ç¸£|æ¾æ¹–ç¸£|é‡‘é–€ç¸£|é€£æ±Ÿç¸£)"
        r"(.+?(å€|å¸‚|é®|é„‰))",
        address
    )
    if m:
        addr6 = m.group(1) + m.group(2)
        print(f"[geocode] å˜—è©¦ï¼ˆå¸‚+å€/é„‰/é®/å¸‚ï¼‰ï¼š{addr6}")
        result = try_geocode(addr6)
        if result:
            return result

    print(f"[geocode] å®Œå…¨æŸ¥ä¸åˆ°ï¼š{address}")
    return None

def find_nearby_points(lat, lon, max_km=5, top_k=5):
    points = load_xin_points()
    results = []
    for p in points:
        if p.get("lat") and p.get("lon"):
            d = haversine_km(lon, lat, p["lon"], p["lat"])
            if d <= max_km: results.append((p, d))
    results.sort(key=lambda x: x[1])
    return results[:top_k]

def load_all_units() -> List[Dict[str, Any]]:
    data = json.loads(UNITS_FILE.read_text("utf-8"))
    raw_units = data.get("units", [])
    units = []
    for u in raw_units:
        u = dict(u)
        section_title = u.get("section_title") or ""
        subtitle_texts = " ".join(seg.get("text", "") for seg in u.get("subtitles", []) or [])
        content_text = u.get("content_text", "") or ""
        search_text = " ".join(s for s in [section_title, u.get("title") or "", content_text, subtitle_texts] if s)
        u["_search_text"] = search_text
        units.append(u)
    print(f"[load] âœ… å…±è¼‰å…¥ {len(units)} å€‹å–®å…ƒ")
    return units

def build_recommendations_response(query: str, results: List[Dict[str, Any]], 
                                   offset: int = 0, limit: int = TOP_K, 
                                   target_lang: str = "zh-TW"):
    
    # --- 1. å®šç¾©ä»‹é¢æ–‡å­— (ä½¿ç”¨ Hardcode ä¿è­‰ç¿»è­¯å“è³ª) ---
    if target_lang == 'en':
        ui = {
            "not_found": "Currently no relevant courses found. You can try keywords like: stress, insomnia, depression...",
            "found_pattern": "Found {total} results (ğŸ¥ Video {v_count}, ğŸ“„ Article {a_count})",
            "showing": "Showing items",
            "intro": "Based on your description, I found these courses/articles:",
            "hint_prefix": "Tips: ",
            "hint_default": "No specific keywords found in subtitles, you can watch from the beginning.",
            "video_link": "ğŸ¥ Link: ",
            "more_btn": "ğŸ‘‰ Click 'Next 5' for more"
        }
    elif target_lang == 'ja':
        # [æ–°å¢] æ—¥æ–‡ä»‹é¢ hardcode
        ui = {
            "not_found": "æ¡ä»¶ã«åˆã†ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚ã€Œã‚¹ãƒˆãƒ¬ã‚¹ã€ã€ã€Œä¸çœ ã€ã€ã€Œä¸å®‰ã€ãªã©ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã§è©¦ã—ã¦ã¿ã¦ãã ã•ã„ã€‚",
            "found_pattern": "åˆè¨ˆ {total} ä»¶è¦‹ã¤ã‹ã‚Šã¾ã—ãŸï¼ˆğŸ¥ å‹•ç”» {v_count}ã€ğŸ“„ è¨˜äº‹ {a_count}ï¼‰",
            "showing": "è¡¨ç¤ºä¸­: ",
            "intro": "ã”ç›¸è«‡å†…å®¹ã«åŸºã¥ãã€ä»¥ä¸‹ã®ã‚³ãƒ¼ã‚¹/è¨˜äº‹ã‚’æ¤œç´¢ã—ã¾ã—ãŸï¼š",
            "hint_prefix": "ãƒ’ãƒ³ãƒˆ: ",
            "hint_default": "å­—å¹•ã«ç‰¹å®šã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã¯è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚æœ€åˆã‹ã‚‰ã”è¦§ãã ã•ã„ã€‚",
            "video_link": "ğŸ¥ ãƒªãƒ³ã‚¯: ",
            "more_btn": "ğŸ‘‰ ã€Œæ¬¡ã®5ä»¶ã€ã‚’ã‚¯ãƒªãƒƒã‚¯ã—ã¦ã‚‚ã£ã¨è¦‹ã‚‹"
        }
    else:
        # é è¨­ä¸­æ–‡
        ui = {
            "not_found": "ç›®å‰æ‰¾ä¸åˆ°å¾ˆç¬¦åˆçš„èª²ç¨‹ï¼Œå¯ä»¥è©¦è‘—ç”¨ï¼šå©†åª³ã€å£“åŠ›ã€æ†‚é¬±ã€å¤±çœ â€¦ ç­‰è©å†è©¦è©¦çœ‹ã€‚",
            "found_pattern": "å…±æ‰¾åˆ° {total} ç­†å…§å®¹ï¼ˆğŸ¥ å½±ç‰‡ {v_count}ã€ğŸ“„ æ–‡ç«  {a_count}ï¼‰",
            "showing": "ç›®å‰é¡¯ç¤ºç¬¬",
            "intro": "æ ¹æ“šä½ çš„æ•˜è¿°ï¼Œæˆ‘å¹«ä½ æ‰¾äº†é€™äº›èª²ç¨‹ / æ–‡ç« ï¼š",
            "hint_prefix": "ğŸ’¡ å°æé†’ï¼š",
            "hint_default": "å­—å¹•è£¡æ²’æœ‰ç‰¹åˆ¥å‘½ä¸­é—œéµå¥ï¼Œå¯ä»¥å¾é ­é–‹å§‹çœ‹ã€‚",
            "video_link": "ğŸ¥ å½±ç‰‡é€£çµï¼š",
            "more_btn": "ğŸ‘‰ é»æ“Š ã€Œçµ¦æˆ‘å¾Œäº”å€‹ã€ å¯ä»¥çœ‹æ›´å¤š"
        }
        
        # å¦‚æœæ˜¯å…¶ä»–èªè¨€ (éŸ“æ–‡/æ­èªç­‰)ï¼Œæ‰ä½¿ç”¨å³æ™‚ç¿»è­¯
        if target_lang != "zh-TW":
            for k, v in ui.items():
                if "{total}" not in v: # ç°¡å–®å­—ä¸²ç›´æ¥ç¿»
                    ui[k] = translate_text(v, target_lang)

    # --- 2. è™•ç†ç„¡çµæœ ---
    if not results:
        return {
            "type": "course_recommendation", "query": query, "total": 0, "video_count": 0, "article_count": 0,
            "offset": offset, "limit": limit, "has_more": False, "results": [],
            "message": ui["not_found"]
        }

    # --- 3. æ•¸æ“šè¨ˆç®—èˆ‡ Header ---
    results = reorder_episode_pairs(results)
    total = len(results)
    video_count = sum(1 for r in results if not r.get("is_article"))
    article_count = sum(1 for r in results if r.get("is_article"))
    page_results = results[offset: offset + limit]
    
    # æ ¹æ“šèªè¨€çµ„è£ Header
    if target_lang == 'en':
        header_msg = (
            f"ğŸ“š {ui['found_pattern'].format(total=total, v_count=video_count, a_count=article_count)}\n"
            f"{ui['showing']} {offset + 1}-{min(offset + limit, total)}\n\n"
            f"{ui['intro']}"
        )
    elif target_lang == 'ja':
        header_msg = (
            f"ğŸ“š {ui['found_pattern'].format(total=total, v_count=video_count, a_count=article_count)}\n"
            f"{ui['showing']} {offset + 1}ï½{min(offset + limit, total)} ä»¶\n\n"
            f"{ui['intro']}"
        )
    else:
        # ä¸­æ–‡
        header_msg = (
            f"ğŸ“š {ui['found_pattern'].format(total=total, v_count=video_count, a_count=article_count)}\n"
            f"{ui['showing']} {offset + 1}ï½{min(offset + limit, total)} ç­†\n\n"
            f"{ui['intro']}"
        )

    items = []
    
    # --- 4. é€ç­†è™•ç† ---
    for r in page_results:
        raw_title = r.get("title") or "(Untitled)"
        raw_section = r.get("section_title") or ""
        
        # æ¨™é¡Œè™•ç†
        if target_lang != "zh-TW":
            trans_title = translate_text(raw_title, target_lang)
            
            # æª¢æŸ¥ç¿»è­¯æ˜¯å¦æœ‰æ•ˆ (ä¸ç‚ºç©º ä¸” ä¸ç­‰æ–¼åŸæ–‡)
            if trans_title and trans_title != raw_title:
                display_title = f"{raw_title}\n   [{trans_title}]"
            else:
                display_title = raw_title
            
            if raw_section:
                trans_section = translate_text(raw_section, target_lang)
                if trans_section and trans_section != raw_section:
                    display_section = f"{raw_section} / {trans_section}"
                else:
                    display_section = raw_section
            else:
                display_section = ""
        else:
            display_title = raw_title
            display_section = raw_section

        score = r.get("_score", 0.0)
        is_article = bool(r.get("is_article"))
        youtube_url = r.get("youtube_url")

        entry = {
            "section_title": display_section, 
            "title": display_title, 
            "score": score,
            "is_article": is_article, 
            "type": "article" if is_article else "video",
        }

        if is_article:
            content_text = (r.get("content_text") or "").replace("\n", " ")
            snippet_raw = content_text[:100] + "..."
            entry["article_url"] = r.get("article_url") or r.get("url")
            
            if target_lang != "zh-TW":
                trans_snippet = translate_text(snippet_raw, target_lang)
                entry["snippet"] = trans_snippet
            else:
                entry["snippet"] = snippet_raw     
        else:
            # è™•ç†å½±ç‰‡æç¤ºèª
            seg = r.get("_best_segment")
            if seg:
                start_str = format_time(seg.get("start_sec", 0.0))
                seg_text = seg.get('text', '')[:30]
                
                # è‹¥æ˜¯è‹±æ—¥æ–‡ï¼Œä½¿ç”¨ç¿»è­¯ä¸¦ Hardcode çµæ§‹
                if target_lang == "en":
                    trans_seg = translate_text(seg_text, target_lang)
                    hint_body = f"Mentioned at {start_str}: \"{trans_seg}...\""
                elif target_lang == "ja":
                    trans_seg = translate_text(seg_text, target_lang)
                    hint_body = f"{start_str} ã«ã¦è¨€åŠ: ã€Œ{trans_seg}...ã€"
                elif target_lang != "zh-TW":
                    trans_seg = translate_text(seg_text, target_lang)
                    hint_body = f"{start_str}: \"{trans_seg}...\""
                else:
                    hint_body = f"è©²å–®å…ƒåœ¨ {start_str} æœ‰æåˆ°ï¼šã€Œ{seg_text}...ã€"
            else:
                # é€™è£¡æœƒç”¨åˆ°ä¸Šæ–¹å®šç¾©å¥½çš„ ui["hint_default"] (å·²ç¶“æ˜¯æ—¥æ–‡äº†)
                hint_body = ui["hint_default"]
            
            entry["hint"] = f"{ui['hint_prefix']} {hint_body}"
            entry["youtube_url"] = youtube_url
            entry["link_label"] = ui["video_link"] 

        items.append(entry)
    
    return {
        "type": "course_recommendation", 
        "query": query, 
        "total": total,
        "video_count": video_count, 
        "article_count": article_count,
        "offset": offset, 
        "limit": limit, 
        "has_more": offset + limit < total,
        "results": items,
        "header_text": header_msg, 
        "message": ui["more_btn"] if (offset + limit < total) else "" 
    }

def build_nearby_points_response(address: str, results):
    if not results:
        return {
            "type": "xin_points",
            "address": address,
            "points": [],
            "message": f"åœ¨ã€Œ{address}ã€5 å…¬é‡Œå…§æ²’æœ‰æ‰¾åˆ°å¿ƒæ“šé»"
        }

    points = []
    origin_encoded = urllib.parse.quote(address)

    for p, d in results:
        dest_address = p.get("address", "")
        dest_encoded = urllib.parse.quote(dest_address)
        
        map_url = f"https://www.google.com/maps/dir/?api=1&origin={origin_encoded}&destination={dest_encoded}&hl=zh-TW"

        points.append({
            "title": p.get("title"),
            "address": dest_address,
            "tel": p.get("tel"),
            "distance_km": round(d, 2),
            "map_url": map_url
        })

    return {
        "type": "xin_points",
        "address": address,
        "points": points
    }

def execute_hybrid_search(search_query: str) -> List[Dict[str, Any]]:
    print(f"[hybrid] é–‹å§‹æœå°‹: {search_query}")
    kw_results = search_units(UNITS_CACHE, search_query, top_k=9999)
    vec_results = search_units_semantic(search_query, top_k=50)
    
    combined_map = {}
    for r in kw_results:
        key = get_base_key(r.get("section_title"), r.get("title"))
        combined_map[key] = r

    for r in vec_results:
        key = get_base_key(r.get("section_title"), r.get("title"))
        VECTOR_WEIGHT_BOOST = 20.0 
        VECTOR_WEIGHT_BASE = 10.0
        if key in combined_map:
            combined_map[key]["_score"] += (r["_score"] * VECTOR_WEIGHT_BOOST)
        else:
            if r["_score"] > 0.25: 
                r["_score"] = r["_score"] * VECTOR_WEIGHT_BASE
                combined_map[key] = r
    
    final_results = list(combined_map.values())
    final_results.sort(key=lambda x: x["_score"], reverse=True)
    return final_results

app = FastAPI(title="å¿ƒå¿«æ´»èª²ç¨‹æ¨è–¦ API")

app.add_middleware(
    CORSMiddleware, allow_origins=["*"], allow_credentials=True, allow_methods=["*"], allow_headers=["*"],
)
app.mount("/static", StaticFiles(directory="static"), name="static")

@app.get("/", include_in_schema=False)
def serve_index():
    return FileResponse("static/index.html")

UNITS_CACHE = load_all_units()
init_vector_model()

HISTORY: Dict[str, List[Dict[str, Any]] ] = {}

class ChatRequest(BaseModel):
    query: str
    session_id: Optional[str] = None

class NearbyRequest(BaseModel):
    address: str

class RecommendRequest(BaseModel):
    query: str

@app.get("/ping")
def ping(): return {"status": "ok"}

@app.post("/chat")
def chat(req: ChatRequest):
    q_origin = req.query.strip()
    session_id = req.session_id or "anonymous"
    
    user_lang = detect_language(q_origin)
    
    print(f">>> [/chat] session_id: {session_id} | User Lang: {user_lang} | Origin: {q_origin}")

    if user_lang != "zh-TW":
        q_search = translate_text(q_origin, "zh-TW")
        print(f"    Translated for search: {q_search}")
    else:
        q_search = q_origin

    def detect_media_preference(text: str) -> Optional[str]:
        if any(w in text for w in ["æƒ³çœ‹æ–‡ç« ", "çµ¦æˆ‘æ–‡ç« ", "åªæœ‰æ–‡ç« ", "æ–‡ç« æ¨è–¦", "æ‰¾æ–‡ç« ", "åªæƒ³çœ‹æ–‡ç« "]): return "article"
        if any(w in text for w in ["æƒ³çœ‹å½±ç‰‡", "çµ¦æˆ‘å½±ç‰‡", "æ’­æ”¾å½±ç‰‡", "å½±éŸ³", "çœ‹å½±ç‰‡", "youtube", "åªæƒ³çœ‹å½±ç‰‡"]): return "video"
        return None

    media_pref_check = detect_media_preference(q_search)
    q_cleaned = q_search

    if media_pref_check == "article":
        for w in ["æƒ³çœ‹æ–‡ç« ", "çµ¦æˆ‘æ–‡ç« ", "åªæœ‰æ–‡ç« ", "æ–‡ç« æ¨è–¦", "æ‰¾æ–‡ç« ", "åªæƒ³çœ‹æ–‡ç« ", "æ–‡ç« "]: q_cleaned = q_cleaned.replace(w, "")
    elif media_pref_check == "video":
        for w in ["æƒ³çœ‹å½±ç‰‡", "çµ¦æˆ‘å½±ç‰‡", "æ’­æ”¾å½±ç‰‡", "å½±éŸ³", "çœ‹å½±ç‰‡", "youtube", "åªæƒ³çœ‹å½±ç‰‡", "å½±ç‰‡"]: q_cleaned = q_cleaned.replace(w, "")
    q_cleaned = q_cleaned.strip()
    
    user_core, _, _ = normalize_query(q_cleaned)
    if not user_core and len(q_cleaned) >= 2: user_core = [q_cleaned]

    resp = {}

    if ("é™„è¿‘" in q_search) and ("å¿ƒæ“šé»" in q_search or "çœ‹è¨º" in q_search or "é–€è¨º" in q_search):
        addr = extract_address_from_query(q_search)
        if not addr: 
            msg = "æˆ‘æœ‰é»æŠ“ä¸åˆ°åœ°å€ï¼Œè«‹å˜—è©¦è¼¸å…¥å®Œæ•´åœ°å€"
            if user_lang != "zh-TW": msg = translate_text(msg, user_lang)
            resp = {"type": "xin_points", "address": None, "points": [], "message": msg}
        else:
            geo = geocode_address(addr)
            if not geo: 
                msg = f"æŸ¥ä¸åˆ°ã€Œ{addr}ã€é€™å€‹åœ°å€"
                if user_lang != "zh-TW": msg = translate_text(msg, user_lang)
                resp = {"type": "xin_points", "address": addr, "points": [], "message": msg}
            else:
                lat, lon = geo
                results = find_nearby_points(lat, lon, max_km=5, top_k=TOP_K)
                resp = build_nearby_points_response(addr, results)

    elif ADDR_HEAD_RE.match(q_search):
        geo = geocode_address(q_search)
        if not geo: 
            msg = f"æŸ¥ä¸åˆ°ã€Œ{q_search}ã€é€™å€‹åœ°å€"
            if user_lang != "zh-TW": msg = translate_text(msg, user_lang)
            resp = {"type": "xin_points", "address": q_search, "points": [], "message": msg}
        else:
            lat, lon = geo
            results = find_nearby_points(lat, lon, max_km=5, top_k=TOP_K)
            resp = build_nearby_points_response(q_search, results)

    elif detect_pagination_intent(q_search):
        history = HISTORY.get(session_id, [])
        last = next((h for h in reversed(history) if h["response"].get("type") == "course_recommendation"), None)
        
        if not last: 
            msg = "ç›®å‰æ²’æœ‰ä¸Šä¸€ç­†æ¨è–¦çµæœï¼Œå¯ä»¥å…ˆå•ä¸€å€‹å•é¡Œ ğŸ˜Š"
            if user_lang != "zh-TW": msg = translate_text(msg, user_lang)
            resp = {"type": "text", "message": msg}
        else:
            prev_resp = last["response"]
            prev_query = prev_resp.get("query_raw") or prev_resp.get("query")
            prev_filter = prev_resp.get("filter_type", None)
            new_offset = prev_resp["offset"] + prev_resp["limit"]
            
            full_results = execute_hybrid_search(prev_query)
            if prev_filter == "article": full_results = [r for r in full_results if r.get("is_article")]
            elif prev_filter == "video": full_results = [r for r in full_results if not r.get("is_article")]
            
            resp = build_recommendations_response(
                prev_query, full_results, offset=new_offset, limit=TOP_K, 
                target_lang=user_lang
            )
            resp["filter_type"] = prev_filter
            resp["query_raw"] = prev_query

    elif media_pref_check and not q_cleaned:
        history = HISTORY.get(session_id, [])
        last = next((h for h in reversed(history) if isinstance(h.get("response"), dict) and h["response"].get("type") == "course_recommendation"), None)
        
        if not last:
            msg = "è«‹å…ˆè¼¸å…¥ä¸€å€‹ä¸»é¡Œï¼Œä¾‹å¦‚ã€Œç„¦æ…®ã€æˆ–ã€Œå¤±çœ ã€ã€‚"
            if user_lang != "zh-TW": msg = translate_text(msg, user_lang)
            resp = {"type": "course_recommendation", "query": q_search, "total": 0, "video_count": 0, "article_count": 0, "offset": 0, "limit": TOP_K, "has_more": False, "results": [], "message": msg}
        else:
            prev_resp = last["response"]
            original_topic = prev_resp.get("query_raw") or prev_resp.get("query")
            
            full_results = execute_hybrid_search(original_topic)
            if media_pref_check == "article": full_results = [r for r in full_results if r.get("is_article")]
            elif media_pref_check == "video": full_results = [r for r in full_results if not r.get("is_article")]
            
            resp = build_recommendations_response(
                original_topic, full_results, offset=0, limit=TOP_K, 
                target_lang=user_lang
            )
            resp["filter_type"] = media_pref_check
            resp["query_raw"] = original_topic
            
            if not resp["results"]: 
                msg = f"é—œæ–¼ã€Œ{original_topic}ã€ç›®å‰æ²’æœ‰ç›¸é—œçš„å…§å®¹ã€‚" 
                if user_lang != "zh-TW": msg = translate_text(msg, user_lang)
                resp["message"] = msg

    else:
        search_q = q_cleaned if q_cleaned else q_search
        full_results = execute_hybrid_search(search_q)
        
        final_filter = None
        if media_pref_check == "article":
            full_results = [r for r in full_results if r.get("is_article")]
            final_filter = "article"
        elif media_pref_check == "video":
            full_results = [r for r in full_results if not r.get("is_article")]
            final_filter = "video"
        
        resp = build_recommendations_response(
            q_origin, 
            full_results, 
            offset=0, 
            limit=TOP_K, 
            target_lang=user_lang
        )
        resp["filter_type"] = final_filter
        
        resp["query_raw"] = search_q 
        
        resp["detected_lang"] = user_lang
        resp["query_search_zh"] = search_q

        if media_pref_check and not resp["results"]: 
            msg = f"é—œæ–¼ã€Œ{search_q}ã€ç›®å‰æ²’æœ‰ç›¸é—œçš„å…§å®¹ã€‚"
            if user_lang != "zh-TW": msg = translate_text(msg, user_lang)
            resp["message"] = msg

    history_list = HISTORY.setdefault(session_id, [])
    history_list.append({"query": q_origin, "response": resp})
    if len(history_list) > 50: history_list.pop(0)
    
    return resp

@app.get("/history")
def get_history(session_id: str):
    return { "items": HISTORY.get(session_id, []) }

@app.post("/nearby")
def nearby(req: NearbyRequest):
    addr = req.address.strip()
    if not addr: return {"type": "xin_points", "address": None, "points": [], "message": "è«‹æä¾›å®Œæ•´åœ°å€"}
    geo = geocode_address(addr)
    if not geo: return {"type": "xin_points", "address": addr, "points": [], "message": f"æŸ¥ä¸åˆ°ã€Œ{addr}ã€é€™å€‹åœ°å€"}
    results = find_nearby_points(geo[0], geo[1], max_km=5, top_k=TOP_K)
    return build_nearby_points_response(addr, results)

@app.post("/recommend")
def recommend(req: RecommendRequest):
    q = req.query.strip()
    sid = "anonymous" 
    pref = None
    if any(w in q for w in ["æ–‡ç« "]): pref = "article"
    elif any(w in q for w in ["å½±ç‰‡"]): pref = "video"
    
    full_results = execute_hybrid_search(q)
    
    if pref == "article": full_results = [r for r in full_results if r.get("is_article")]
    elif pref == "video": full_results = [r for r in full_results if not r.get("is_article")]

    resp = build_recommendations_response(q, full_results, offset=0, limit=TOP_K)
    history_list = HISTORY.setdefault(sid, [])
    history_list.append({"query": q, "response": resp})
    return resp

if __name__ == "__main__":
    import uvicorn
    uvicorn.run("xin_api:app", host="0.0.0.0", port=8000, reload=True)